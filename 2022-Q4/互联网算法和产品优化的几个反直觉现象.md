互联网算法和产品优化的几个反直觉现象
==================

[mp.weixin.qq.com](http://mp.weixin.qq.com/s?__biz=MzIzMTAxNDQyNA==&mid=2650935621&idx=1&sn=ce0578faf9c94b98248dc533d7780c76&chksm=f35c1832c42b91241f72cc7c96069a5e9a0815ebd1a783f68bbc3e6dce71c3a98d7645b09e07&mpshare=1&scene=1&srcid=1207DMLIEYNNB3dvO1FQueOY&sharer_sharetime=1670345369902&sharer_shareid=c58007142b3c8dd4da3163f5c61d6b7b#rd)沙漠君 沙漠之鹰

本文不涉及任何具体的业务和形态，没有公开任何数据和需要保护的技术。

互联网产品和算法的优化，是广大程序员和产品经理的主要工作。但想准确衡量线上实验效果，从来都不简单。笔者将这些反直觉现象，总结成三个典型案例予以讨论。然而它们的隐秘性一个比一个更大。  

太长不看的，可以直接看精简版：

左手倒右手：多个入口挨个优化，此消彼长，一年到头大盘没变

只看点击率：用户完成功能需要更多的操作，页面越来越复杂，却不自知

系统耦合：模型变差，但点击率效率是涨的。最后发现是系统间耦合破坏实验分层假设，流量分布发生了改变， 曝光PV比点击PV跌得更多

大部分人只对delta（增量）有感知。第一版从0开始，很难做AB实验，第一版做的太好了反而给后续的迭代拦路了，一定要做的足够快(cha)，然后快速迭代，这样才能符合"互联网思维"。

下面是正文。﻿

### **1. 左手倒右手**

通常，一个功能有多个入口。流量如同沙漏，一层层地从上往下漏：

![图片](https://image.cubox.pro/article/2022121409343877813/64086.jpg?imageMogr2/quality/90/ignore-error/1)
例如有四个入口, 当D入口的样式或算法改变，而用户对新样式很感兴趣，就会到新入口D尝试。于是A，B， C 的流量转到了D，D点击率置信提升。但是实验挂载的指标常常只有D入口，总量无置信变化，却又很难检测出其他入口流量的下降。

随着用户对新入口的新鲜感逐渐降低，实验变得不置信了，不过没关系，实验报告早就写好了，推全了。那下一次就是优化A了...最后结果是，忙活了一整年，发现整个大盘原地不动。此消彼长，彼消此涨。左手倒右手，乐此不疲。就像不会游泳的人在游泳池里来回扑腾。

指标下钻也是一样，全局无变化，那么下钻到某个特定口径，就置信提升了。但原因可能只是流量从其他地方，流到了这个特定口径中。

其根本原因是，用户需求量是相对稳定的。比如买鞋，只要经济形势没有明显变化，总量就是一亿双。每个用户不论点了多少下，最终也只会成交比较确定的数目。  

悲观地来说，可能80%的实验都是左手倒右手；乐观地说，对全局大盘没有影响的实验也不能一棒子打死，如果原先的体验短板被补齐了，这也是贡献。但从实验测量的角度，怎么来判断用户是因为新鲜，还是真正的用户体验的提升呢？以笔者拙见：

* 增加流量系统的完整监控：即使是单入口实验，也要去分析其他入口的流量和用户流转，并观察足够长的时间以确定用户的对新样式的接受度。

* 全局留白和长期实验：以年的单位，对所有实验进行汇总，整体统筹的来看全年效果。

以上都提出要长期观察，但是长期实验是很难实施的，它极大地增加了实验门槛和观察时长，违背了互联网"快即是好"的教义，你让大家等三个月再给老板汇报吗？

另外，长期实验可能从根本上不可行：物理实验假设了系统的参数是确定和稳态的，但用户实验不是，用户有记忆性，显著地会受整体大盘，前期实验和用户习惯的影响：今天喜欢，明天就疲劳；不仅如此，实验分层的流量很可能会在收集和训练时泄露，进而使得所有实验效果在长期看会趋于平均。这就导致了实验在分桶和时间上耦合，短期看有效果的实验，长期看就未必有效果。

### **2. 点击越多，产品越好？**

如果说在老板那里获得的最大收获是什么，那应该是对指标口径的严格要求了。

一切优化都是需要指标的，而指标的定义和计算并不简单，极大地影响了优化的实施和最终的效果。指标是一切问题的开始。定义好的指标就解决了一半的问题，否则方案可以不用看了。

实际情况中，实验测量天生受到了度量手段的限制，除了上面的长期性，用户需求不一定非要通过点击来实现。可是大部分情况下，只有点击和转化才有埋点。这就造成了实验指标度量不准的问题。我们举三个例子：

一个音乐播放器，如果其团队的KPI是应用打开次数。那么在锁屏界面的小banner播放器上，按钮越少越好。如果把"上一首"按钮去掉，那么用户就不得不进入APP再去点击上一首，应用打开次数直线上涨。可是这样的优化，除了增加用户的操作步骤，有什么意义呢？

![图片](https://image.cubox.pro/article/2022121409343883241/40317.jpg?imageMogr2/quality/90/ignore-error/1)
一个列表展示页，如果因为某些标签展示，导致关键信息如标题展示不全，用户为了看清标题，就不得不点击进入详情页，进而导致点击率飙升，可是这是反用户的。产品连最关键的信息都没展示全，还谈什么体验？反过来说，很多产品优化，越是把丰富信息在列表页展示出来，用户在列表页已经获得了足够信息，点击就越少，最后线上点击率爆降，那该实验该推全吗？

再一个大家熟知且可怕的例子，一个APP的广告安装引流：直接最大化装机量和展示量，只要能提升装机量，那么其他问题就不是问题。那么各种五迷三道的事情就出现了：部分厂商的牛皮癣广告，弹窗，自动安装等，无所不用其极。

这本质是指标的设定和评价问题。以笔者拙见，商业和战略目标，业务目标，产品目标，技术目标，四者应该是有不同的推导逻辑的，甚至是完全不同的。但现实情况是，技术背产品目标，产品背业务目标，最终全部坍缩到了某个特定的指标如点击率上。而一旦这个指标并不能完整地刻画整个系统时，奇怪的事情就发生了。

因为要最大化点击率，所以页面自然而然地进化地越来越复杂，入口越来越多。点击是手段而不是目的，最后的转化成交和用户口碑和留存才是，我们却常常搞混了目的和手段。用户迷失在信息的迷宫中，用大量无效的点击换得了团队的KPI。反之，你要花大量的功夫给老板解释，为什么点击率降了还要推全？

以笔者拙见， 有以下可能解法：

1. 更完整和清晰的指标体系：区分上述四个目标（商业/业务/产品/技术），完善推导思考，并引入其他体验指标。搜索有满意度评测，推荐有不感兴趣的负反馈按钮，以及用户体验评测和原声反馈。当然期间遇到不少问题：虽然大部分体验指标很难作为算法优化的目标；算法导致的个性化也使得评测变成了难题。

2. 提升测量的准确性：例如尽可能地简化反馈，将反馈行为和用户必然使用路径绑定在一起：如抖音的上下滑，婚恋匹配程序的左右滑。喜欢即点击，不喜欢即滑动，反馈越简单，收集越准确。不过很多产品的设计未必能如此极简。另外，似乎也没有团队或模型，专门研究用户在"干什么"或"怎么想"。排除隐私考虑，如果眼动甚至脑电能引入，优化便能上一个新的台阶。

3. 引入带约束的目标。如最小化用户成交所需的点击和停留时长，可能真没人这么使用。工具属性的产品，最大的价值就是让用户在尽可能方便简单地使用，"用完即走"。问题是，没人甘心只做工具，所有的APP都想做平台，总想让用户留下来，先看文章再点广告。负责工具的和负责商业化的，在企业里通常是两个并行部门。商业化部门看了这个指标直摇头：你这老同志坏得很呢！用户不点击了，我们的饭碗怎么办？

### **3. 指标耦合逆优化**

这个问题相比前面两个更难以察觉。实验系统通过分层实验和参数正交保证各系统的解耦。**但当两个算法模块之间有内生的耦合性时，分层实验的假设就被破坏了。**

介绍一种典型情况：移动端搜索的分卡片feeds流。由于单个query会触发多种类型的结果。因此会设计不同的卡片，卡片间通过效率预估进行"组间排序"，例如搜美团，对外卖感兴趣的用户美团外卖在前面，而对股票感兴趣的则股票卡片在前面。卡片在内部进行商品的"组内排序"。它们通常由不同团队负责。

当上线新模型时，如果组内新模型效果发生变化，如将冷门商品排到前面，组间排序模型会预估卡片效率下降，进而使得整个卡片排名相对靠后，导致卡片曝光下降，但本卡片点击PV的下降程度反而比曝光量要小。分子分母都变小，而分母小的更多。

这就是耦合导致"指标逆优化"：组内模型效果变差，反而本卡片的点击率提升！

这种现象也很难观察，你说卡片的曝光PV降了呀？但因为搜索是分query的，以上现象可能只在一部分query上出现。卡片间的效率越相似的query，组间排序的震荡反而会更大。不下钻根本发现不了。

算法优化，还得去摸清上下游系统的脾气。系统间耦合使得问题变得扑朔迷离。实验效果真的能反映真实的模型性能吗？对此恐怕难有通用的解决方案。笔者拙见，先解耦，再优化。参考EM算法，先固定住一个优化另外一个。当然，首先是要发现问题，并说服上下游一起优化。

### **4. 算法的未来**

现在，回到文章最前面的那张图，应该就一目了然了。三个维度的耦合都可能存在，而现实中，多个维度的耦合甚至会同时发生，进而大大增加问题的复杂性。

![图片](https://image.cubox.pro/article/2022121409343991785/47856.jpg?imageMogr2/quality/90/ignore-error/1)
产品和算法迭代还有其他问题，如：

* 大部分人只对delta（增量）有感知。第一版从0开始，很难做AB实验，那么不论基线做得多好，都很难量化地作为迭代贡献。第一版做的太好了反而给后续的迭代拦路了，一定要做的足够快(cha)，然后快速迭代，这样才能符合"互联网思维"。

* 找软柿子捏：很多活动类的基线都是随机，可是跟随机比获得100%提升，有意思吗？这也是对第一点的延伸。

* 拿着锤子找钉子：搞出一个技术，然后往所有的业务上硬套。配合上多个优化点，最后起作用的很可能不是这个技术，而是特征或样本优化

* 产品和算法脱节：产品看绝对，算法看相对。不论绝对指标跌得如何洪水滔天，算法依然可以讲自己的故事，分享transformer可以堆叠多少层。反过来，产品也不理解算法逻辑，认为算法就是没有业务sense的"配置生成器"。

* 实验次数多了，指标关注多了，实验系统总会发生第一类错误，总能找到置信上涨的实验。只要实验够勤奋，就能通过反复的震荡，来获得N次实验推全，而对大盘的影响可能并不显著。

这涉及到非常复杂的问题，一篇文章必然无法讲完。但是，我们能做哪些事情呢？首先是花时间去思考和揣摩更加科学、严谨和长期的指标体系。长期有耐心。对复杂系统心存敏感和敬畏。其次，做产品和做算法不能纯靠AB实验；由于测量能力和准确性的限制，过分依赖指标优化，会导致恶劣的产品和算法。深度学习和实验系统的复杂性虽高，但不能成为懒惰的借口。只看指标，不做深入分析，就是鸵鸟心态。

我们可以明显的看到，现有的算法还不够好，在与亿万用户的交互过程当中，我们的算法还处在非常原始的状态。小红书的种草心智，大概率也不是由算法来获得的。算法只能follow和猜测，却不能拿捏用户的想法，更不能智能地影响它。算法流量机制依然需要专家花大量的时间去设计。算法优化的公平性也逐渐提上日程，其他例如算法歧视，和信息茧房，这些问题到现在也没有明确的解法。

推而广之，绩效考核也是如此。人都是很聪明的，只要给他制定一个规则，他就能通过各种方式来破解这套规则，实现自身的利益最大化。**虽然数据是金标准，可是数据也是人定出来和测算出来的**。我还是比较认可"过程和结果一样重要"的理念。目标要做牵引，而不是绝对要求：太过绝对的只关注指标的好坏，会让动作变形。一个不好的过程获得的好的指标，比好的过程获得一般的指标，带来的副作用更大。

![图片](https://image.cubox.pro/article/2022071607551737403/19531.jpg)

[跳转到 Cubox 查看](https://cubox.pro/my/card?id=6999847949949732071)
