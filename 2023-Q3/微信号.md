微信号
===

[mp.weixin.qq.com](https://mp.weixin.qq.com/s/X5l-xT6TdNbxu0dxxQJfwQ)AI协同创新智库

![图片](https://image.cubox.pro/cardImg/2023091319114223989/45735.jpg?imageMogr2/quality/90/ignore-error/1)

自主智能体一直是学术界的一个突出研究课题。该领域先前的研究通常集中在孤立环境中训练知识有限的智能体，这与人类的学习过程有很大差异，从而使智能体难以实现类似人类的决策。

最近，通过获取大量的网络知识，大型语言模型（LLM）在实现人类水平的智能方面表现出了巨大的潜力。这引发了研究基于LLM的自主Agent的热潮。为了充分利用LLM的潜力，研究人员设计了适合不同应用的不同Agent架构。

在本文中，我们对这些研究进行了全面的调查，从整体的角度对自主智能体领域进行了系统的回顾。更具体地说，我们的重点在于构建基于LLM的Agent，为此我们提出了一个统一的框架，该框架包含了以前的大部分工作。此外，我们还总结了基于LLM的AI Agent在社会科学、自然科学和工程领域的各种应用。最后，我们讨论了基于LLM的AI Agent常用的评估策略。

在前人研究的基础上，我们还提出了该领域的一些挑战和未来方向。为了跟踪该领域并不断更新我们的调查，我们在https://github.com/Paitesanshi/LLM-Agent-Survey.


1 背景
----


长期以来，自主智能体一直被视为实现人工通用智能（AGI）的一条很有前途的道路，能够通过自主规划和指令完成任务。

在早期的范式中，决定Agent行动的策略函数是通过启发式方法构思的，随后通过环境参与进行细化\[101，86，120，55，9，116\]。但存在一个明显的差距，这些策略函数往往无法复制人类水平的熟练程度，特别是在不受约束的开放领域环境中。这种差异可以追溯到启发式设计和训练环境提供的限定知识中固有的潜在不准确。

近年来，大型语言模型（LLM）取得了显著的成功，表明了它们在实现类人智能方面的潜力\[108，116，9，4，130，131\]。这种能力来自于对综合训练数据集的利用，再加上大量的模型参数。在这种能力的推动下，近年来出现了一种蓬勃发展的趋势（该领域的增长趋势见图1），其中LLM被用作创建自主智能体的核心协调器\[19，125，123，115，119，161\]。  
这些方法旨在模仿类似人类的决策过程，从而为实现更复杂、更具适应性的人工智能系统提供途径。

沿着基于LLM的自主智能体的方向，人们设计了许多有前景的模型，重点是增强LLM的基本能力，如记忆和规划，使其能够刺激人类行动并熟练地承担一系列任务。

然而，这些模型是独立提出的，在全面总结和比较它们方面所做的努力有限。对现有的基于LLM的自主智能体工作进行全面总结分析至关重要，这对全面理解该领域具有重要意义，对未来的研究具有启示意义。

在本文中，我们对基于LLM的自主智能体领域进行了全面的调查。具体来说，我们从基于LLM的自主智能体的构建、应用和评估三个方面组织了我们的调查。

对于agent构建，我们提出了一个由四个组件组成的统一框架：

*
  **表示agent属性的profile模块**
*
  **存储历史信息的内存模块**
*
  **制定未来行动战略的计划模块**
*
  **执行计划决策的行动模块**


通过禁用一个或多个模块，以前的大多数研究都可以被视为该框架的具体例子。

在介绍了典型的Agent模块后，我们还总结了常用的微调策略，以增强Agent对不同应用场景的适应性。除了构建智能体，我们还概述了自主智能体的潜在应用，探索这些智能体如何增强社会科学、自然科学和工程领域。最后，我们讨论了评估自主智能体的方法，重点是主观和客观策略。

总之，这项调查提供了一个系统的综述，并为基于LLM的自主智能体领域的现有研究建立了明确的分类法。主要从**Agent构建、Agent应用、Agent评价**三个方面进行论述。

在先前研究的基础上，我们确定了该领域的几个挑战，并讨论了未来的潜在方向。我们相信该领域仍处于早期阶段，因此，我们维护了一个Github存储库，以持续跟踪该领域的研究https://github.com/Paitesanshi/LLM-Agent-Survey.

![图片](https://image.cubox.pro/cardImg/2023091319114272924/46843.jpg?imageMogr2/quality/90/ignore-error/1)

2 基于LLM的自主智能体构建
---------------


基于LLM的自主智能体有望基于LLM类似人类的能力有效地完成不同的任务。为了实现这一目标，有两个重要方面，即：  
（1）应该设计哪种架构来更好地使用LLM；  
（2）如何学习架构的参数。

在架构设计方面，我们对现有研究进行了系统的综合，最终形成了一个全面统一的框架。

至于第二个方面，我们总结了三种常用的策略，包括：  
（1）从例子中学习，其中模型是基于精心策划的数据集进行微调的；  
（2）从环境反馈中学习，利用实时交互和观察；  
（3）从人类反馈中学习，利用人类专业知识和干预进行改进。

### 2.1 Agent架构设计


语言模型（LLM）的最新进展已经证明了它们完成广泛任务的潜力。然而，仅基于LLM，由于其架构的限制，很难有效地实现自治Agent。为了弥补这一差距，之前的工作开发了许多模块，以启发和增强LLM构建自主智能体的能力。

在本节中，我们提出了一个统一的框架来总结之前工作中提出的架构。具体来说，我们的框架的总体结构如图2所示，它由一个profile模块、一个内存模块、一个计划模块和一个执行模块组成。

*
  profiling模块的目的是识别Agent的角色。
*
  记忆和计划模块将Agent置于动态环境中，使其能够回忆过去的行为并计划未来的行动。
*
  行动模块负责将Agent的决策转化为特定的输出。


在这些模块中，profiling模块影响记忆和计划模块，这三个模块共同影响执行模块。

![图片](https://image.cubox.pro/cardImg/2023091319114279911/89012.jpg?imageMogr2/quality/90/ignore-error/1)

#### 2.1.1 Profiling模块


自主智能体通常通过承担特定角色来执行任务，如代码开发人员、教师和领域专家\[113,35\]。Profiling模块旨在定义Agent的角色配置文件，这些角色配置文件通常被写入提示中以影响LLM行为。在现有的工作中，有三种常用的策略来生成Agent配置文件。

*
  手工制作方法
*
  基于LLM生成方法
*
  数据集对齐方法


**手工制作方法**：在该方法中，Agent的配置文件是手动指定的。例如，如果一个人想设计不同性格的Agent，他可以用"你是一个外向的人"或"你是个内向的人"来描述Agent。

手工制作方法在以前的许多工作中都被用来指示Agent的Profiling文件。具体而言，Generative Agent\[156\]通过名称、目标以及与其他Agent的关系等信息来描述Agent。MetaGPT\[58\]、ChatDev\[113\]和Self collaboration\[29\]预先定义了软件开发中的各种角色及其相应职责，手动为每个Agent分配不同的配置文件以促进协作。最近的一项工作\[27\]表明，手动分配不同的人物角色会显著影响LLM的生成，包括毒性。通过指定特定的人物角色，与默认的人物角色相比，它们显示出更高的毒性。

一般来说，手工制作的方法非常灵活。然而，它可能是劳动密集型的，尤其是在与大量Agent商打交道时。

**基于LLM生成的方法**：在该方法中，Agent配置文件是基于LLM自动生成的。

通常，它首先提供手动提示，概述特定的生成规则，并阐明目标群体中Agent配置文件的组成和属性。此外，它可以指定初始Agent profile配置文件作为few-shot的例子。然后，这些配置文件作为基于LLM生成其他Agent信息的基础。例如，RecAgent\[134\]首先通过手动制作年龄、性别、个人特征和电影偏好等细节，为少数Agent创建种子档案。然后，它利用ChatGPT基于种子信息生成更多的Agent配置文件。当Agent数量很大时，LLM生成方法可以节省大量时间，但它可能缺乏对生成的配置文件的精确控制。

**数据集对齐方法**：在该方法中，基于真实世界的数据集来创建Agent配置文件。真实人类的基本信息被充分或选择性地用来描述Agent。

例如，\[5\]中的Agent基于真实世界调查数据集中的参与者人口统计背景进行初始化。数据集对齐方法可以准确地捕捉真实人群的属性，有效地弥合了虚拟世界和现实世界之间的差距。

除了profile文件生成策略之外，另一个重要问题是如何指定用于描述(profiling)Agent的信息。信息的例子包括人口统计信息，它介绍了人口的特征（例如，年龄、性别和收入），心理信息，它表明了Agent人的个性，以及社会信息，它描述了Agent之间的关系。

对Agent进行配置的信息的选择在很大程度上取决于具体的应用场景。例如，如果研究的重点是用户的社交行为，那么社交档案信息就变得至关重要。然而，建立简档信息和下游任务之间的关系并不总是简单的。一个潜在的解决方案是最初输入所有可能的配置文件信息，然后开发自动方法（例如，基于LLM）来选择最合适的方法。

#### 2.1.2 记忆模块


记忆模块在AI Agent的构建中起着非常重要的作用。它存储从环境中感知到的信息，并利用记录的记忆来促进未来的行动。记忆模块可以帮助Agent积累经验，自我进化，并以更一致、合理和有效的方式行事。

本节提供内存模块的全面概述，重点介绍其结构、格式和操作。

**记忆结构**

基于LLM的自主智能体通常结合了认知科学研究人类记忆过程的原理和机制。人类记忆遵循一个总体的过程，从记录感知输入的感觉记忆，到短暂维持信息的短期记忆，再到长期巩固信息的长期记忆。

在为AI Agent设计记忆架构时，研究人员从人类记忆的这些方面获得灵感，同时也认识到能力的关键差异。

AI Agent中的短期记忆类似于Transformer架构的上下文窗口约束中支持的学习能力。长期内存类似于外部向量存储，Agent可以根据需要快速查询和检索。

因此，当人类通过强化逐渐将感知信息从短期存储转移到长期存储时，AI Agent可以在其算法实现的记忆系统之间设计更优化的写和读过程。

通过模拟人类记忆的各个方面，设计者可以创建利用记忆过程来提高推理和自主性的Agent。在下文中，我们将介绍两种常用的记忆结构。

•**统一记忆**。在这种结构中，记忆被组织成一个单一的框架，短期记忆和长期记忆之间没有区别。该框架具有用于记忆读取、写入和反思的统一接口。例如：

*
  Atlas\[65\]存储基于通用密集向量的文档存储器，这些向量是从双编码器模型生成的。
*
  Augmented-LLM\[121\]为其记忆采用了统一的外部存储，可以通过提示进行访问。
*
  Voyager\[133\]还利用了统一的记忆体系结构，将不同复杂性的技能集中在一个中央库中。在代码生成过程中，可以根据技能的匹配和检索相关性对其进行索引。
*
  ChatLog\[132\]维护统一的记忆流，这允许模型保留重要的历史信息，并针对不同的环境自适应地调整Agent本身。


•**混合记忆**。混合记忆清楚地区分了短期和长期功能。短期成分暂时缓冲了最近的感知，而长期记忆则随着时间的推移巩固了重要信息。例如：

*
  \[109\]采用双层记忆结构来存储Agent的经验和知识，包括长期记忆和短期记忆。长期记忆被用来保持主体对整个世界的理解和概括，而短期记忆被用来保留主体对个别事件的理解和注释。
*
  AgentSims\[89\]还实现了一种混合记忆体系结构。长期记忆利用向量数据库来有效地存储和检索每个Agent的情景记忆。LLM用于实现短期记忆，并执行抽象、验证、校正和模拟任务。
*
  在GITM\[161\]中，短期记忆存储当前轨迹，而长期记忆存储从成功的先前轨迹中总结的参考计划。  
  长期记忆提供稳定的知识，而短期记忆允许灵活的计划。
*
  Reflexion\[125\]利用短期滑动窗口来捕捉最近的反馈，并结合持久的长期存储来保留浓缩的见解。这种组合允许利用详细的即时体验和高级抽象。
*
  SCM\[84\]选择性地激活最相关的长期知识，与短期记忆相结合，从而能够在复杂的上下文对话中进行推理。
*
  SWIFTSAGE\[87\]使用小型LM来管理短期记忆以产生直觉和联想思维，同时使用LLM来处理长期记忆以产生深思熟虑的决策。


**记忆格式**

信息可以使用各种格式存储在内存中，每种格式都具有独特的优势。例如，自然语言可以保留全面的语义信息，而嵌入可以提高记忆阅读的效率。在下文中，我们介绍了四种常用的内存格式。

•**自然语言**。使用自然语言进行任务推理/编程可以实现灵活、语义丰富的存储/访问。例如，Reflexion\[125\]将自然语言的经验反馈存储在滑动窗口中。Voyager\[133\]使用自然语言描述来表示《我的世界》游戏中的技能，这些技能直接存储在内存中。

•**嵌入**。使用嵌入来存储信息可以提高记忆检索和阅读效率。例如，MemoryBank\[158\]将每个内存段编码为嵌入向量，构建索引语料库以供检索。GITM\[161\]将参考计划表示为嵌入，以便于匹配和重用。ChatDev\[113\]将对话历史编码为用于检索的向量。

•**数据库**。外部数据库提供结构化存储，并且可以通过高效和全面的操作来操作存储器。例如，ChatDB\[61\]利用数据库作为符号长期存储器。LLM控制器生成的SQL语句可以准确地对数据库进行操作。

•**结构化列表**。另一种类型的存储器格式是结构化列表，在此基础上可以以更简洁高效的方式传递信息。例如，GITM\[161\]将子目标的行动列表存储在层次树结构中。层次结构明确地捕捉了目标和相应计划之间的关系。RET-LLM\[102\]最初将自然语言句子转换为三元组短语，然后将其存储在内存中。

**记忆操作**

有三种关键的记忆操作，包括记忆读取、记忆写入和自我反思，用于与外部环境交互。

•**记忆读取**。记忆阅读的关键在于从记忆中提取信息。通常，信息提取有三个常用的标准，即最近性、相关性和重要性\[109\]。最近的、相关的和重要的记忆更有可能被提取出来。形式上，我们得出以下方程来提取信息：

![图片](https://image.cubox.pro/cardImg/2023091319114379048/20267.jpg?imageMogr2/quality/90/ignore-error/1)

其中q是查询，例如，Agent应该处理的任务或Agent所在的上下文。M是所有记忆的集合。s_rec（·）、s_rel（·）和s_imp（·）是衡量记忆m的最近性、相关性和重要性的评分函数。需要注意的是，s_imp只反映了记忆本身的特征，因此与查询无关。α、β和γ是平衡参数。通过给它们分配不同的值，人们可以获得各种各样的记忆读取策略。例如，通过设置α=γ=0，许多研究\[102，161，133，49\]只考虑记忆读取的相关性得分。通过指定α=β=γ=1.0，\[109\]对上述三个度量进行同等加权，以从记忆中提取信息。

•**记忆写入**。Agent可以通过在记忆中存储重要信息来获得知识和经验。在写入过程中，有两个潜在的问题需要认真解决。一方面，解决如何存储与现有存储器相似的信息（即记忆复制）至关重要。另一方面，当记忆达到其存储极限（即记忆溢出）时，考虑如何删除信息是很重要的。这些问题可以通过以下策略来解决。

（1） 内存重复。为了整合类似的信息，人们开发了各种方法来整合新的和以前的记录。  
例如，在\[108\]中，与同一子目标相关的成功动作序列存储在列表中。一旦列表的大小达到N（=5），则使用LLM将其中的所有序列浓缩为统一的计划解决方案。记忆中的原始序列被新生成的序列所替换。Augmented-LLM\[121\]通过计数累加聚合重复信息，避免冗余存储。Reflexion\[125\]将相关反馈整合为高级见解，取代原始经验。

（2） 内存溢出。为了在信息充满时将其写入记忆，人们设计了不同的方法来删除现有信息以继续记忆过程。例如，在ChatDB\[61\]中，可以根据用户的命令明确删除记忆。RET-LLM\[102\]使用固定大小的循环缓冲区作为记忆，基于先进先出（FIFO）方案覆盖最旧的条目。

•**记忆反思**。这项行动旨在赋予Agent浓缩和推断更先进信息的能力，或自主验证和纠正自己的行为。它帮助Agent理解自己和他人的属性、偏好、目标和联系，从而指导他们的行为。以往的研究对各种形式的记忆反思进行了研究，即：

（1）自我总结。反思可以用来将Agent的记忆浓缩为更高级的概念。在\[109\]中，Agent能够将存储在记忆中的过去经历总结为更广泛、更抽象的见解。具体来说，Agent首先根据其最近的记忆生成三个关键问题。然后，这些问题被用来查询记忆，以获得相关信息。基于所获得的信息，Agent产生五个见解，这些见解反映了Agent的高级思想。此外，反思可以分层发生，这意味着可以根据现有的见解产生见解。

（2）自我验证。另一种形式的反思涉及评估Agent行为的有效性。在\[133\]中，Agent旨在完成Minecraft中的任务。在每一轮执行期间，Agent使用GPT-4作为评论家来评估当前操作是否足以实现所需任务。如果任务失败，评论家会通过提出完成任务的方法来提供反馈。Replug\[124\]采用训练方案来进一步使检索模型适应目标语言模型。具体来说，它利用语言模型作为评分函数来评估每个文档对减少语言模型困惑的贡献。通过最小化检索概率和语言模型得分之间的KL偏差来更新检索模型参数。这种方法有效地评估检索结果的相关性，并根据语言模型的反馈进行调整。

（3）自我纠正。在这种类型的反思中，Agent可以通过结合来自环境的反馈来纠正其行为。在MemPrompt\[96\]中，模型可以根据用户反馈调整对任务的理解，以生成更准确的答案。在\[137\]中，Agent被设计为玩《我的世界》，它根据预定义的计划采取行动。当计划失败时，Agent会重新思考其计划并对其进行更改以继续探索过程。

（4） 同理心。记忆反射也可以用来增强Agent的移情能力。在\[49\]中，Agent是一个聊天机器人，但它通过考虑人类的认知过程来生成话语。在每一轮谈话之后，Agent都会评估他的话对听众的影响，并更新他对听众状态的看法。

#### 2.1.3 规划模块


当人类面对复杂的任务时，他们首先将其分解为简单的子任务，然后逐一解决每个子任务。规划模块使基于LLM的Agent能够思考和规划解决复杂任务，使Agent更加全面、强大和可靠。

在下文中将介绍两种类型的计划模块。

**无反馈规划**

在这种方法中，Agent在规划过程中不会收到反馈。这些计划是以整体的方式产生的。以下是许多具有代表性的规划策略。

•**子目标分解**。一些研究人员打算让LLM一步一步地思考，以解决复杂的任务。

*
  思想链\[138\]已成为允许大型模型解决复杂任务的标准技术。它提出了一种简单而有效的提示方法，即通过提示中的少量语言示例，逐步解决复杂的推理问题。
*
  Zero-shot-CoT\[72\]允许LLM通过提示模型"一步一步地思考"来自主生成复杂问题的推理过程，并通过实验证明LLM是不错的Zero-shot推理器。
*
  在\[63\]中，LLM充当Zero-shot规划器，在交互式模拟环境中做出目标驱动的决策。
*
  \[53\]进一步使用环境对象和对象关系作为LLM行动计划生成的额外输入，为系统提供对周围环境的感知以生成计划。
*
  ReWOO\[147\]引入了一种将规划与外部观察分离的范式，使LLM能够充当一个规划者，直接生成一系列独立的计划，而不需要外部反馈。


总之，通过将复杂任务分解为可执行的子任务，大大提高了大型语言模型制定计划和决策的能力。

•**多路径思维**。基于CoT，一些研究人员认为，人类思考和推理的过程是一个树状结构，有多条通向最终结果的路径。

*
  自洽CoT（CoT-SC）\[135\]假设每个复杂问题都有多种思维方式来推导最终答案。具体而言，CoT用于生成推理的几个路径和答案，其中出现次数最多的答案将被选择为最终答案输出。
*
  思维树（ToT）\[150\]假设人类在为规划目的对复杂问题做出决策时，倾向于以树状方式思考，其中每个树节点都是一种思维状态。它使用LLM生成思维的评估或投票，可以使用BFS或DFS进行搜索。这些方法提高了LLM在复杂推理任务中的性能。
*
  \[153\]讨论了受约束的语言规划问题。它生成额外的脚本并对其进行过滤，以提高脚本生成的质量。在生成的几个脚本中，脚本选择由（1）脚本和目标之间的余弦相似性，（2）脚本是否包含目标约束关键字来确定。
*
  DEPS\[137\]使用视觉语言模型作为选择器来选择可选子任务中的最佳路径。
*
  SayCan\[2\]将来自语言模型的概率（动作对高级指令有用的概率）与来自值函数的概率（成功执行所述动作的概率）相结合，并选择要采取的动作。然后，它附加到robot响应并再次查询模型以重复该过程，直到输出步骤结束。


总之，多路径思维进一步使agent能够解决更复杂的规划任务，但也带来了额外的计算负担。

•**外部规划师**。LLM，即使具有显著的Zero-shot规划能力，在许多情况下也不如传统规划者可靠，尤其是在面临特定领域的长期规划问题时。

*
  LLM+P\[90\]将自然语言描述转换为正式的规划领域定义语言（PDDL）。然后，使用外部规划器计算结果，并最终由LLM转换为自然语言。同样，
*
  LLM-DP\[24\]利用LLM将观测、当前世界状态和目标目标转换为PDDL格式。然后将该信息传递给外部符号规划器，该规划器有效地确定从当前状态到目标状态的最佳动作序列。
*
  MRKL\[71\]是一种模块化的神经符号AI架构，LLM处理输入文本，将其路由到每个专家，然后将其通过LLM的输出。
*
  CO-LLM\[156\]认为LLM擅长生成高级计划，但不擅长低级控制。他们使用启发式设计的低级计划器，根据高级计划稳健地执行基本操作。有了子任务领域的专家规划师，LLM就有可能在特定领域中导航复杂任务的规划。


基于LLM的Agent的广义知识很难在所有领域中都能最好地执行任务，但将其与外部规划者的专家知识相结合可以有效地提高性能。

**有反馈的规划**

当人类处理任务时，成功或失败的经历会引导他们反思自己，提高他们的计划能力。这些经验往往是在外部反馈的基础上获得和积累的。为了模拟这种人类能力，许多研究人员设计了规划模块，可以接收来自环境、人类和模型的反馈，显著提高了智能体的规划能力。

•**环境反馈**。在许多研究中，Agent根据环境反馈制定计划。例如：

*
  ReAct\[151\]将Agent的动作空间扩展为动作和语言空间的集合。显式推理和动作是按顺序执行的，当来自动作的反馈没有正确答案时，将再次执行推理，直到获得正确答案。
*
  Voyager\[133\]通过对三种类型的反馈进行操作来自我完善Agent生成脚本，直到它通过自我验证并存储在技能库中。
*
  Ghost\[161\]、DEPS\[137\]可以接收来自环境的反馈，包括关于Agent在环境中的当前状态的信息，以及关于所执行的每个动作的成功或失败的信息。通过整合这些反馈，Agent可以更新他们对环境的理解，改进他们的策略并调整他们的行为。
*
  基于Zero-shot规划器\[63\]，Re-prompting\[117\]使用预先条件错误信息来检测Agent是否能够完成当前规划。它还使用先决条件信息来重新提示LLM完成闭环控制。
*
  Inner Monologue\[64\]在指令中添加了三种类型的环境反馈：子任务的成功执行、被动场景描述和主动场景描述，从而实现了基于LLM的Agent的闭环规划。
*
  Introspective Tips\[17\]允许LLM通过环境反馈的历史进行内省。
*
  LLM Planner\[127\]引入了一种基于基础的重新规划算法，当在任务完成过程中遇到对象不匹配和无法实现的计划时，该算法会动态更新LLM生成的计划。
*
  在Progprompt\[126\]中，断言被合并到生成的脚本中，以提供环境状态反馈，从而在不满足操作的前提条件的情况下允许错误恢复。


总之，环境反馈是规划成败的直接指标，从而提高了闭环规划的效率。

•**人为反馈**。Agent可以在真实的人类反馈的帮助下制定计划。这样的信号可以帮助Agent更好地与实际设置保持一致，也可以缓解幻觉问题。

*
  Voyager\[133\]中提到，人类可以充当评论家，通过多模型反馈要求Voyager更改上一轮代码。
*
  OpenAGI\[51\]提出了一种带有任务反馈的强化学习（RLTF）机制，该机制利用手动或基准评估来提高基于LLM的Agent的能力。


•**模型反馈**。语言模型可以作为批评者来批评和改进生成的计划。

*
  Self-Refine\[97\]引入了Self-Refine机制，通过迭代反馈和改进来提高LLM的输出。具体而言，LLM被用作生成器、反馈提供者和refiner。首先，生成器用于生成初始输出，然后反馈提供者为输出提供特定且可操作的反馈，最后，refiner用于使用反馈改进输出。LLM的推理能力通过生成器和评论家之间的迭代反馈回路得到了提高。
*
  Reflexion\[125\]是一种通过言语反馈增强Agent的框架，它引入了记忆机制。参与者首先生成动作，然后评估者生成评估，最后通过自我反思模型生成对过去经历的总结。摘要将存储在内存中，以通过过去的经验进一步提高Agent的生成。世界模型通常是指Agent对环境的内部表示，用于环境的内部模拟和抽象。它帮助Agent推理、规划和预测不同行动对环境的影响。
*
  RAP\[57\]涉及将LLM同时用作世界模型和Agent。在推理过程中，Agent构建了一个推理树，而世界模型则提供奖励作为反馈。agent对推理树进行MCTS（Monte Carlo Tree Search，蒙特卡洛树搜索）以获得最优计划。
*
  REX\[103\]引入了一种加速MCTS方法，其中奖励反馈由环境或LLM提供。
*
  Introspective Tips\[17\]可以从其他专家模型的演示中学习。
*
  在MAD（多主体辩论）\[83\]框架中，多个主体以"以眼还眼"的方式表达他们的论点，法官管理辩论过程以达成最终解决方案。MAD框架鼓励LLM中的发散思维，这有助于完成需要深入思考的任务。


总之，规划模块对于Agent解决复杂任务非常重要。虽然外部反馈总是有助于制定明智的计划，但它并不总是存在的。有反馈和无反馈的规划对于构建基于LLM的Agent都很重要。

#### 2.1.4 行动模块


行动模块旨在将Agent的决定转化为具体结果。它直接与环境交互，决定Agent完成任务的有效性。

本节概述了行动模块，主要考察行动目标、策略、空间和影响力。

**行动目标**

行动目标是指期望通过动作执行达到的目标，通常由真人或Agent自己指定。三个主要行动目标包括任务完成、对话互动、环境探索和互动。

•**任务完成**。动作模块的基本目标是以合乎逻辑的方式完成特定任务。不同场景中的任务类型各不相同，因此需要对动作模块进行必要的设计。例如：

*
  Voyager\[133\]利用LLM作为行动模块，指导Agent探索和收集资源，以完成Minecraft中的复杂任务。
*
  GITM\[161\]将整个任务分解为可执行的动作，使Agent能够逐步完成日常活动。
*
  Generative Agents\[109\]类似地通过分层分解高级任务规划来执行可执行的动作序列。


•**对话互动**。基于LLM的自主Agent与人类进行自然语言对话的能力至关重要，因为人类用户通常需要获得Agent状态或完成与Agent的协作任务。先前的工作已经提高了Agent在不同领域的对话交互能力。例如：

*
  ChatDev\[113\]在软件开发公司的员工之间进行相关对话。
*
  DERA\[104\]以迭代的方式增强了对话交互。
*
  \[31，139\]利用不同主体之间的互动对话，从而鼓励他们对某个主题有相似的意见。


•**环境探索与互动**。Agent能够通过与环境互动获得新知识，并通过总结最近的经验来增强自己。通过这种方式，智能体可以产生新的行为，这些行为越来越适应环境并符合常识。例如：

*
  Voyager\[133\]通过允许Agent在开放式环境中进行探索来进行持续学习。
*
  SayCan\[2\]中的记忆增强强化学习（MERL）框架不断积累文本知识，然后基于外部反馈调整主体行动方案。
*
  GITM\[161\]允许Agent不断收集文本知识，从而根据环境反馈调整其行为。


**行动策略**

行动策略是指Agent产生行动的方法。

在现有的工作中，这些策略可能是记忆回忆、多轮互动、反馈调整和融入外部工具。

•**记忆回忆**。记忆回忆技术有助于Agent根据存储在记忆模块中的经验做出明智的决定\[109，78，161\]。

*
  生成Agent\[109\]保持对话和经验的记忆流。在执行操作时，会检索相关的内存片段作为LLM的条件输入，以确保操作的一致性。
*
  GITM\[161\]使用记忆来指导行动，比如向先前发现的位置移动。
*
  CAMEL\[78\]构建了历史经验的记忆流，使LLM能够基于这些记忆生成知情的行动。


•**多轮互动**。该方法试图利用多轮对话的背景，让Agent将适当的反应确定为行动\[113，104，31\]。

*
  ChatDev\[113\]鼓励Agent根据他们与他人的对话历史采取行动。
*
  DERA\[104\]提出了一种新的对话Agent，在通信过程中，研究者Agent可以提供有用的反馈来指导决策者Agent的行动。
*
  \[31\]构建了一个多智能体辩论（MAD）系统，每个基于LLM的智能体参与迭代交互，交换挑战和见解，最终目的是达成共识。
*
  ChatCot\[20\]采用多轮对话框架对思维链推理过程进行建模，通过对话互动将推理和工具使用无缝集成。


•**反馈调整**。人类反馈或参与外部环境的有效性已被证明有助于Agent适应和加强其行动策略\[133，99，2\]。例如：

*
  Voyager\[133\]使Agent能够在经历行动失败后改进其策略，或使用反馈机制验证成功的策略。
*
  交互式构建学习Agent（ICLA）\[99\]利用用户对初始行动的反馈来迭代增强计划，从而制定更精确的策略。
*
  SayCan\[2\]采用了强化学习框架，在该框架中，智能体仅基于环境反馈不断调整动作，从而实现基于试错的自动化增强。


•**整合外部工具**。可以通过引入外部工具和扩展知识源来增强基于LLM的自主Agent。

一方面，Agent可以具备在训练或推理阶段访问和使用各种API、数据库、web应用程序和其他外部资源的能力。例如：

*
  Toolformer\[119\]被训练来确定要调用的适当API、这些调用的时间以及将返回的结果集成到未来令牌预测中的最佳方法。
*
  ChemCrow\[8\]设计了一种基于化学的LLM试剂，该试剂包含17种专家设计的工具，用于执行包括有机合成、药物发现和材料设计在内的任务。
*
  ViperGPT\[128\]提出了一个代码生成框架，它将视觉和语言模型组装成能够返回任何给定查询结果的子例程。
*
  HuggingGPT\[123\]使用LLM连接机器学习社区中的各种人工智能模型（例如，Hugging Face），以解决人工智能任务。具体而言，HuggingGPT提出了一种元学习方法来训练LLM生成代码片段，然后使用这些片段从外部社区中心调用所需的人工智能模型。


另一方面，Agent直接获得的知识的范围和质量可以在外部知识来源的帮助下扩大。在之前的工作中，外部知识源包括数据库、知识图、网页等。例如：

*
  Gorilla\[111\]能够有效地提供适当的API调用，因为它是在三个额外的机器学习中心数据集上训练的：Torch hub、TensorFlow hub和HuggingFace。
*
  WebGPT\[105\]提出了一种扩展，可以在使用ChatGPT时将从网站检索到的相关结果合并到提示中，从而实现更准确、更及时的对话。
*
  ChatDB\[61\]是一种人工智能数据库助手，它利用LLM控制器生成的SQL语句来准确地操作外部数据库。
*
  GITM\[161\]使用LLM生成文本挖掘任务的可解释结果，该任务采用了一种新颖的文本挖掘管道，该管道集成了LLM、知识提取和主题建模模块。


**动作空间**

基于LLM的Agent的动作空间是指Agent可以执行的一组可能的动作。这源于两个主要来源：

*
  扩展行动能力的外部工具
*
  Agent自己的知识和技能，如语言生成和基于记忆的决策。


具体而言，外部工具包括搜索引擎、知识库、计算工具、其他语言模型和视觉模型。通过与这些工具对接，Agent可以执行各种现实的操作，如信息检索、数据查询、数学计算、复杂的语言生成和图像分析。基于语言模型的智能体自我获取的知识可以使智能体能够规划、生成语言和做出决策，从而进一步扩大其行动潜力。

•**外部工具**。各种外部工具或知识源为Agent提供了更丰富的操作能力，包括API、知识库、视觉模型、语言模型等。

（1）API。利用外部API来补充和扩展操作空间是近年来流行的模式。例如：

*
  HuggingGPT\[123\]使用搜索引擎，将查询转换为搜索请求以获取相关代码。
*
  \[105,118\]提出在响应用户请求时自动生成查询以从外部网页提取相关内容。
*
  TPTU\[118\]与Python解释器和LaTeX编译器接口，以执行复杂的计算，如平方根、阶乘和矩阵运算。


另一种类型的API是LLM可以基于自然语言或代码输入直接调用的API。例如:

*
  ToolFormer\[119\]是一个基于LLM的工具转换系统，可以根据自然语言指令将给定的工具自动转换为具有不同功能或格式的另一个工具。
*
  API-Bank\[80\]是一种基于LLM的API推荐Agent，可以自动搜索并生成各种编程语言和领域的适当API调用。API-Bank还为用户提供了一个交互式界面，方便用户修改和执行生成的API调用。
*
  类似地，ToolBench\[115\]是一个基于LLM的工具生成系统，可以根据自然语言需求自动设计和实现各种实用工具。ToolBench生成的工具包括计算器、单位转换器、日历、地图、图表等。所有这些Agent都使用外部API作为其外部工具，并为用户提供交互式界面，以便轻松修改和执行生成或转换的工具。


（2） 知识库。连接到外部知识库可以帮助Agent获得特定的领域信息，以生成更现实的操作。例如:

*
  ChatDB\[61\]使用SQL语句查询数据库，以逻辑方式促进Agent的操作。
*
  ChemCrow\[8\]提出了一种基于LLM的化学试剂，旨在借助17种专家设计的工具完成有机合成、药物发现和材料设计领域的任务。
*
  MRKL系统\[71\]，OpenAGI\[51\]结合了各种专家系统，如知识库和规划者，调用它们以系统的方式访问特定领域的信息。


（3） 语言模型。语言模型也可以作为丰富动作空间的工具。例如:

*
  MemoryBank\[158\]采用了两种语言模型，一种旨在对输入文本进行编码，而另一种负责匹配到达的查询语句，以提供辅助的文本检索。
*
  ViperGPT\[128\]首先使用基于语言模型的Codex从文本描述中生成Python代码，然后执行该代码来完成给定的任务。
*
  TPTU\[118\]结合了各种LLM来完成广泛的语言生成任务，如生成代码、生成歌词等。


（4） 视觉模型。将视觉模型与Agent集成可以将动作空间扩展到多模态领域。

*
  ViperGPT\[128\]利用GLIP等模型来提取视觉内容相关操作的图像特征。
*
  HuggingGPT\[123\]提出使用视觉模型进行图像处理和生成。


•**Agent的自我认识**。Agent自获取的知识也提供了多种行为，例如:利用LLM的生成能力进行规划和语言生成，根据记忆做出决策等。Agent自获得的知识，如记忆、经验和语言能力，实现了各种无工具的行动。例如：

*
  Generative Agents\[109\]主要包含所有过去对话的综合记忆日志。在采取行动时，它检索相关的内存片段作为条件输入，以指导LLM自回归生成逻辑和一致的语言计划。
*
  GITM\[161\]构建了一个经验的记忆库，比如发现的村庄或收集的资源。当采取行动时，它会在内存库中查询相关条目，例如回忆以前的村庄方向以再次向该位置移动。
*
  SayCan\[2\]开发了一个强化学习框架，在该框架中，智能体完全基于环境反馈重复调整动作，以实现自动试错改进，而无需任何人工演示或干预。
*
  Voyager\[133\]利用LLM广泛的语言生成功能来合成自由形式的文本解决方案，如Python代码片段或根据当前需求定制的会话响应。同样，
*
  LATM\[10\]使LLM能够利用Python代码来制作自己的可重用工具，从而培养灵活的解决问题的方法。
*
  CAMEL\[78\]将所有历史经历记录在一个记忆流中。LLM然后从相关记忆中提取信息，自回归生成高级文本计划，概述预期的未来行动方案。
*
  ChatDev\[113\]为LLMAgent配备了对话历史记忆，以根据上下文确定适当的沟通反应和行动。


总之，Agent的内部知识通过记忆回忆、反馈调整和开放式语言生成等方法，实现了多样化的无工具行动。

**行动影响**

行动影响是指一项行动的后果，包括环境的变化、主体内部状态的改变、新行动的触发以及对人类感知的影响。

•**不断变化的环境**。动作可以直接改变环境状态，例如移动Agent位置、收集物品、建造建筑物等。例如，GITM\[161\]和Voyager\[133\]通过执行完成任务的动作序列来改变环境状态。

•**改变内部状态**。Agent所采取的行动也可以改变Agent本身，包括更新记忆、形成新计划、获取新知识等等。例如，在Generative Agents\[109\]中，在系统内执行操作后更新记忆流。SayCan\[2\]使Agent能够采取行动更新对环境的理解，从而适应后续行为。

•**触发新动作**。对于大多数基于LLM的自治Agent，动作通常是以顺序的方式进行的，即前一个动作可以触发下一个新动作。例如，Voyager\[133\]试图在收集《我的世界》场景中的环境资源后建造建筑。Generative Agents\[109\]首先将计划分解为子目标，然后进行一系列相关行动来完成每个子目标。

•**影响人类感知和体验**。行动的语言、图像和其他形式直接影响用户的感知和体验。例如，CAMEL\[78\]生成连贯、信息丰富、吸引会话主体的话语。ViperGPT\[128\]产生逼真、多样化的视觉效果，并与图像生成任务相关。HuggingGPT\[123\]可以生成视觉输出，如图像，将人类感知扩展到视觉体验领域。此外，HuggingGPT还可以生成多模式输出，如代码、音乐和视频，以丰富人类与不同媒体形式的互动。


### 2.2 学习策略


学习是人类获得知识和技能的重要机制，有助于增强他们的能力------这一意义深入到基于LLM的Agent领域。在学习过程中，这些Agent有能力在遵守指令、熟练处理复杂任务以及无缝适应前所未有的多样化环境方面表现出更高的熟练度。这一变革过程使这些Agent能够超越最初的编程，从而能够更精细、更灵活地执行任务。

在本章中，我们将深入研究基于LLM的Agent所采用的各种学习策略，并探讨它们的深远影响。

**榜样学习**

榜样学习是人类和人工智能学习的基础过程。在基于LLM的Agent领域，这一原则体现在微调中，这些Agent通过接触真实世界的数据来完善他们的技能。

•**从人类标注的数据中学习**。在追求与人类价值观对齐的过程中，整合人类生成的反馈数据成为微调LLM的基石。这种做法在塑造旨在补充甚至取代人类参与特定任务的智能Agent方面尤为重要。

*
  刘等人\[91\]提出的CoH方法涉及一个多步骤的过程，LLM生成响应，由人类评审员进行评估，以区分有利和不利的结果。这种反应和评估的融合有助于微调过程，使LLM对错误有全面的了解，并有能力纠正错误，同时与人类偏好保持一致。尽管这种方法简单直接，但它受到大量注释成本和时间的阻碍，在快速适应不同场景方面带来了挑战。
*
  MIND2WEB\[26\]使用来自不同领域的人工标注的真实世界网站任务数据进行微调，从而产生在实际网站上有效执行的通用Agent。


•**从LLM标注的数据中学习**。在预训练期间，LLM从广泛的预训数据中获得了丰富的世界知识。在与人类进行微调和调整后，它们表现出类似于人类判断的能力，例如ChatGPT和GPT-4等模型。因此，我们可以将LLM用于标注任务，与人工标注相比，这可以显著降低成本，为广泛的数据采集提供了潜力。

*
  刘等人\[92\]提出了一种基于社会互动的LLM微调的稳定对齐方法。他们设计了一个包含多个Agent的沙箱环境，每个Agent都对一个试探性的问题做出响应。然后由附近的Agent和ChatGPT对这些反应进行评估和评分。随后，响应Agent根据这些评估refine他们的答案，然后由ChatGPT重新评分。这种迭代过程产生了大量的交互式数据语料库，随后使用对比监督学习对LLM进行微调。
*
  在Refiner\[112\]中，要求生成器生成中间步骤，并引入评论家模型来生成结构化反馈。然后，利用反馈记录对生成器模型进行微调，以提高推理能力。
*
  在ToolFormer\[119\]中，预训练语料库标记有使用LLM的潜在API调用。然后，LLM在此注释数据上进行微调，以了解如何以及何时使用API，并将API结果集成到其文本生成中。同样，
*
  ToolBench\[115\]也是一个完全使用ChatGPT生成的数据集，旨在微调和提高LLM使用工具的熟练程度。ToolBench包含大量API描述，以及概述使用特定API要完成的任务的指令，以及实现这些指令的相应操作序列。使用ToolBench的微调过程产生了一个名为ToolLLaMA的模型，该模型的性能与ChatGPT相当。值得注意的是，即使面对以前看不见的API，ToolLLaMA也表现出强大的泛化能力。


**从环境反馈中学习**

在许多情况下，智能Agent需要主动探索周围环境并与环境互动。因此，他们需要适应环境的能力，并从环境反馈中增强自己的能力。在强化学习领域，智能体通过不断探索环境并基于环境反馈进行适应来进行学习\[68，82，98152\]。这一原理也适用于基于LLM的智能Agent。

*
  Voyager\[133\]遵循迭代提示方法，Agent执行操作、收集环境反馈，并不断迭代，直到新获得的技能通过自我验证、得到验证并添加到技能库中。
*
  类似地，LMA3\[22\]在交互式环境中自主设定目标和执行动作，LLM将其性能作为奖励函数进行评分。通过反复这个过程，LMA3独立学习了广泛的技能。
*
  GITM\[161\]和Inner Monologue\[64\]将环境反馈集成到基于大规模语言模型的规划闭环过程中。
*
  此外，创建一个紧密反映现实的环境也大大有助于提高Agent的性能。WebShop\[149\]开发了一个模拟的电子商务环境，在该环境中，Agent可以参与搜索和购买等活动，并获得相应的奖励和反馈。
*
  在\[145\]中，embodiment simulator用于使Agent能够在模拟的真实世界环境中进行交互，促进物理参与，从而获得具体体验。随后，利用这些经验对模型进行微调，从而提高其在下游任务中的性能。


与从注释中学习相比，从环境反馈中学习明显地包裹了基于LLM的Agent的自主性和独立性特征。这种差异体现了环境反应性和自主学习之间的深刻相互作用，促进了对主体行为和适应的微妙理解。

**从交互式人类反馈中学习**

交互式人类反馈为主体提供了在人类指导下以动态方式适应、进化和完善其行为的机会。与一次性反馈相比，交互式反馈更符合真实世界的场景。由于智能体是在动态过程中学习的，它们不仅仅处理静态数据，还参与了对其理解、适应和与人类结盟的不断完善。例如：

*
  \[156\]结合了一个通信模块，该模块通过基于聊天的交互和来自人类的反馈实现协作任务完成。正如\[122\]所强调的，交互式反馈促进了关键方面，如可靠性、透明度、即时性、任务特征以及在学习Agent时信任随时间的演变。


在上面的章节中，我们总结了之前基于Agent构建策略的工作，重点从架构设计和参数优化两个方面进行了介绍。我们在表1中展示了之前的工作和我们的分类法之间的对应关系。

![图片](https://image.cubox.pro/cardImg/2023091319114330800/17586.jpg?imageMogr2/quality/90/ignore-error/1)

3 基于LLM的自主Agent应用
-----------------


基于LLM自主智能体在各个领域的应用代表了我们解决问题、决策和创新方式的范式转变。这些Agent被赋予了语言理解、推理和适应的能力，通过提供前所未有的见解、帮助和解决方案，正在颠覆行业和学科。

在本节中，我们探讨了基于LLM的自主智能体在三个不同领域的变革影响：社会科学、自然科学和工程（见图3左侧的全局概述）。

![图片](https://image.cubox.pro/cardImg/2023091319114369435/95810.jpg?imageMogr2/quality/90/ignore-error/1)

### 3.1 社会科学


计算社会科学涉及计算方法的开发和应用，以分析复杂的人类行为数据，通常是大规模的，包括模拟场景的数据\[74\]。

最近，LLM显示出了令人印象深刻的类人能力，这为社会计算科学的研究带来了希望\[54\]。在下文中，我们介绍了基于LLM的Agent已经应用到的许多代表性领域。

**心理学**：基于LLM的Agent可以在心理学中用于进行心理实验\[1,3，95，163\]。

*
  在\[1\]中，基于LLM的Agent被用来模拟心理实验，包括ultimatum game, garden path sentence, Milgram shock experimen和群体智能能力。在前三个实验中，基于LLM的Agent可以重现当前的心理学发现，而最后一个实验揭示了一些语言模型（包括ChatGPT和GPT-4）中的"超精确失真"(hyperaccuracy distortions)，这可能会影响下游应用。
*
  在\[3\]中，作者使用基于LLM的Agent来模拟博弈论领域中两个典型的重复游戏：囚犯困境和两性之战。他们发现，基于LLM的Agent表现出一种将自身利益置于协调之上的心理倾向。
*
  关于在心理健康方面的应用，\[95\]讨论了利用基于LLM的Agent提供心理健康支持的优缺点。


**政治学和经济学**：最近的研究在政治学和经济领域使用了基于LLM的Agent\[5，59,163\]。

*
  这些Agent被用来分析党派印象，探索政治行为者如何修改议程，以及其他应用程序。此外，基于LLM的Agent可以用于意识形态检测和预测投票模式\[5\]。
*
  此外，最近的研究工作集中在通过基于LLM的Agent的帮助来理解政治演讲的话语结构和说服因素\[163\]。
*
  在Horton等人\[59\]进行的研究中，基于LLM的Agent具有特定的特征，如天赋、偏好和个性。这使研究人员能够在模拟场景中探索经济行为，并对经济学领域获得新的见解。


**社会模拟**：在人类社会中进行实验往往是昂贵的、反伦理的、反道德的，甚至是无法实现的。相比之下，基于Agent的模拟使研究人员能够在特定规则下构建假设场景，以模拟一系列社会现象，例如有害信息的传播。研究人员在宏观和微观层面参与观察和干预系统，这使他们能够研究反事实事件\[110，81，76，109，89，73，50，140\]。这一过程允许决策者制定更多的规则或政策。例如：

*
  Social Simulacara\[110\]模拟了一个在线社交社区，并探索了利用基于LLM的Agent模拟来帮助决策者改进社区法规的潜力。
*
  \[81，76\]调查了社交网络中基于LLM的Agent的行为特征及其对社交网络的潜在影响。
*
  Generative Agents\[109\]和AgentSims\[89\]构建了包括多个Agent的城镇。
*
  SocialAI School\[73\]采用模拟来研究儿童发展过程中表现出的基本社会认知技能。
*
  S3\[50\]专注于信息、情绪和态度的传播，而\[140\]专注于传染病的传播。


**法学**：基于LLM的Agent可以在法律决策过程中发挥辅助作用，帮助法官做出更明智的判决\[23，56\]。

*
  Blind Judgement\[56\]采用几种语言模型来模拟多个法官的决策过程。它通过投票机制收集不同的意见并巩固结果。ChatLaw\[23\]是一个中国法律领域微调的LLM。为了解决模型幻觉的问题，ChatLaw融合了数据库搜索和关键词搜索技术，以提高准确性。同时，采用自注意机制来增强LLM在减轻参考数据不准确影响方面的能力。


**社会科学研究助理**：除了在社会计算的不同领域进行专门研究外，基于LLM的Agent还可以扮演研究助理的角色\[6，163\]。它们有可能帮助研究人员完成生成文章摘要、提取关键词和生成脚本等任务\[163\]。此外，基于LLM的Agent可以作为写作辅助工具，它们甚至能够为社会科学家识别新的研究查询\[6\]。

基于LLM的Agent的发展为计算社会科学研究领域带来了新的研究方法。然而，基于LLM的Agent在社交计算中的应用仍然存在一些挑战和局限\[163，6\]。两个主要问题是偏见和毒性，因为LLM是从真实世界的数据集中训练的，这使它们容易受到固有偏见、歧视内容和不公平的影响。当引入LLM时，它可能会产生有偏差的信息，这些信息被进一步用于训练LLM，导致偏差的放大。

因果关系和可解释性提出了另一个挑战，特别是在社会科学的背景下，通常需要强有力的因果关系。基于概率的LLM往往缺乏明确的可解释性。

### 3.2自然科学


由于大型语言模型的快速发展，基于LLM的Agent在自然科学领域的应用正在兴起。这些Agent为自然科学的科学研究带来了新的机会。在下文中，我们介绍了许多具有代表性的领域，其中基于LLM的Agent可以发挥重要作用。

**文献和数据管理**：在自然科学研究领域，大量的文献和数据往往需要细致的收集、组织和提取，这需要大量的时间和人力资源。基于LLM的Agent表现出强大的自然语言处理能力，使他们能够有效地访问各种工具来浏览互联网、文档、数据库和其他信息源。这种能力使他们能够获得大量数据，无缝集成和管理数据，从而为科学研究提供宝贵的帮助\[7，70，8\]。

*
  通过利用API访问互联网，\[7\]中的Agent可以有效地查询和检索实时相关信息，帮助完成问答和实验计划等任务。
*
  ChatMOF\[70\]利用LLM从人类书面文本描述中提取关键点，并制定调用必要工具包的计划，以预测金属有机框架的性质和结构。
*
  数据库的利用进一步提高了Agent在特定领域的性能，因为它们包含丰富的定制数据。例如，在访问化学相关数据库时，ChemCrow\[8\]可以验证化合物表征的准确性或识别危险物质，从而有助于更准确和知情的科学调查。


**自然科学实验助理**：基于LLM的Agent可以自主操作，独立进行实验，并作为支持科学家研究项目的宝贵工具\[7，8\]。例如：

*
  \[7\]引入了一种创新的Agent系统，该系统利用LLM来自动化科学实验的设计、规划和执行。当提供实验目标作为输入时，系统访问互联网并检索相关文件以获取必要的信息。随后，它使用Python代码执行基本计算，并最终执行实验的顺序步骤。
*
  ChemCrow\[8\]包含17种精心制作的工具，专门用于帮助化学研究人员。收到输入目标后，ChemCrow为实验程序提供了富有洞察力的建议，同时仔细强调了与拟议实验相关的潜在安全风险。


**自然科学教育**：得益于自然语言能力，LLM通过自然语言互动促进了与人类的无缝沟通，使其成为令人兴奋的教育工具，提供实时问答和知识传播\[7，129，30，18\]。例如：

*
  \[7\]提出了Agent系统，作为学生和研究人员学习实验设计、方法和分析的宝贵教育工具。它们有助于培养批判性思维和解决问题的技能，同时鼓励人们更深入地理解科学原理。
*
  Math Agents\[129\]是使用人工智能技术来探索、发现、解决和证明数学问题的实体。数学Agent还可以与人类交流，帮助他们理解和使用数学。
*
  \[30\]利用CodeX\[18\]的功能，通过少量学习实现大学级数学问题的人类级自动求解、解释和生成。这一成就对高等教育具有重要意义，提供了课程设计和分析工具以及自动化内容生成等优势。


利用基于LLM的Agent支持自然科学研究也带来了一定的风险和挑战。

*
  一方面，LLM本身可能容易受到幻觉和其他问题的影响，偶尔会提供错误的答案，导致错误的结论、实验失败，甚至在危险的实验中对人类安全构成风险。因此，在实验过程中，用户必须具备必要的专业知识和知识，才能适当谨慎。
*
  另一方面，基于LLM的Agent可能被用于恶意目的，例如开发化学武器，这就需要实施安全措施，例如人员协调，以确保负责任和合乎道德的使用。

### 3.3 工程


基于LLM的自主智能体在协助和加强工程研究和应用方面显示出巨大的潜力。在本节中，我们回顾并总结了基于LLM的智能体在几个主要工程领域中的应用。

**土木工程**：在土木工程中，基于LLM的Agent可用于设计和优化复杂结构，如建筑物、桥梁、大坝、道路等。\[99\]提出了一种交互式框架，人类建筑师和AI Agent在3D模拟环境中协作构建结构。交互式Agent可以理解自然语言指令、放置块、检测混淆、寻求澄清并结合人类反馈，显示了人类人工智能在工程设计中协作的潜力。

**计算机科学与软件工程**：在计算机科学和软件工程领域，基于LLM的Agent为自动化编码、测试、调试和文档生成提供了潜力\[115，113,58，29，33，44，41\]。

*
  ChatDev\[113\]提出了一种端到端的框架，其中多个Agent角色通过自然语言对话进行通信和协作，以完成软件开发生命周期。该框架展示了可执行软件系统的高效且经济高效的生成。
*
  ToolBench\[115\]可用于代码自动完成和代码推荐等任务。例如，ToolBench可以自动完成代码中的函数名和变量名，以及推荐代码片段。
*
  MetaGPT\[58\]抽象了多个角色，如产品经理、架构师、项目经理和工程师，以在内部监督代码生成并提高最终输出代码的质量。这使得能够进行低成本的软件开发。
*
  \[29\]提出了一个使用LLM生成代码的自协作框架，以ChatGPT为例。在这个框架中，多个LLM为复杂任务中的特定子任务承担不同的"专家"角色。他们根据指定的指示进行协作和互动，形成一个虚拟团队，为彼此的工作提供便利。最终，虚拟团队协作处理代码生成任务，而不需要人工干预。
*
  GPT Engineer\[33\]、SmolModels\[44\]和DemoGPT\[41\]是开源项目，专注于通过提示自动生成代码以完成开发任务。
*
  LLM还可以应用于代码错误测试和更正。LLIFT\[79\]利用LLM来帮助进行静态分析，以检测代码漏洞，从而在精度和可扩展性之间取得平衡。


**航空航天工程**：在航空航天工程中，早期的工作探索使用基于LLM的Agent来建模物理、求解复杂微分方程和优化设计。\[107\]在解决空气动力学、飞机设计、轨迹优化等相关问题方面显示出了有希望的结果。随着进一步的发展，基于LLM的Agent可以通过生成与工程系统集成的可执行代码来创新地设计航天器、模拟流体流动、进行结构分析，甚至控制自动驾驶汽车。

**工业自动化**：在工业自动化领域，基于LLM的Agent可以用于实现生产过程的智能规划和控制。\[144\]提出了一种新的框架，将大型语言模型（LLM）与数字孪生系统集成在一起，以满足灵活的生产需求。该框架利用即时工程技术创建LLMAgent，该Agent可以根据数字孪生提供的信息适应特定任务。这些Agent可以协调一系列原子功能和技能，以完成自动化金字塔中不同级别的生产任务。这项研究展示了将LLM集成到工业自动化系统中的潜力，为更敏捷、灵活和适应性更强的生产流程提供创新解决方案。

**机器人与嵌入式人工智能**：最近的工作为机器人和嵌入式人工智能开发了更有效的强化学习Agent\[25，160，106，143，133，161，60，142，154，28，2\]。重点是增强自主主体在具体环境中的规划、推理和协作能力。

*
  一些方法，如\[25\]，将互补的优势结合到统一的系统中，用于具体推理和任务规划。高级命令可以改进计划，而低级控制器可以将命令转换为操作。  
  \[160\]中的信息收集对话可以加速培训。其他著作如\[106，143\]采用自主主体，在内部世界模型的指导下进行具体决策和探索。
*
  考虑到物理约束，Agent可以生成可执行的计划，并完成需要多种技能的长期任务。在控制策略方面，SayCan\[2\]专注于研究利用移动机械手机器人的各种操纵和导航技能。它从厨房环境中遇到的典型任务中获得灵感，展示了一套包含551项技能（涵盖7个技能家族、17件物品）的集合。这些技能包括各种动作，如拾取、放置、倾倒、抓取和操纵物体等。
*
  VOYAGAR\[133\]和GITM\[161\]等其他框架提出了能够进行通信、协作和完成复杂任务的自主Agent。这证明了自然语言理解、动作规划和人机交互在现实世界机器人中的前景。


随着能力的发展，自适应自治Agent可以完成越来越复杂的具体任务。总之，通过\[60，142，154，28\]中的推理和规划能力来补充传统方法，可以显著提高嵌入式环境中的自主Agent性能。重点是提高样本效率、泛化能力和完成长期任务的整体系统。

**通用自主AI Agent**：许多基于LLM开发的开源项目对人工通用智能（AGI）进行了初步探索，致力于自主通用AI Agent框架\[45，43，38，40，35，36，42，15，32，39，34，114，47，41，37，46，141\]，使开发人员能够快速可靠地构建、管理和运行有用的自主Agent。例如：

*
  LangChain\[15\]是一个开源框架，可以自动执行编码、测试、调试和文档生成任务。通过将语言模型与数据源集成并促进与环境的交互，LangChain通过多个Agent角色之间的自然语言通信和协作，实现了高效且经济高效的软件开发。
*
  基于LangChain，XLang\[36\]提供了一套全面的工具、完整的用户界面，并支持三种不同的Agent场景，即数据处理、插件使用和webAgent。
*
  AutoGPT\[45\]是一种完全自动化、可联网的Agent，它只需设置一个或多个目标，并自动将其分解为相应的任务，并在其中循环，直到达到目标。
*
  WorkGPT\[32\]是一个类似于AutoGPT和LangChain的Agent框架。通过向它提供一条指令和一组API，它可以与人工智能进行来回对话，直到指令完成。
*
  AGiXT\[40\]是一个动态人工智能自动化平台，旨在协调许多供应商的高效人工智能命令管理和任务执行。
*
  AgentVerse\[35\]是一个通用的框架，可以帮助研究人员快速创建自定义的基于多LLM的Agent模拟。
*
  GPT Researcher\[34\]是一个实验应用程序，它利用大型语言模型来高效地开发研究问题，触发网络爬网来收集信息、汇总来源和汇总摘要。
*
  BMTools\[114\]是一个开源存储库，它用工具扩展LLM，并为社区驱动的工具构建和共享提供了一个平台。它支持各种类型的工具，允许使用多个工具同时执行任务，并提供了一个通过URL加载插件的简单界面，促进了对BMTools生态系统的轻松开发和贡献。


总之，基于LLM的自主Agent正在不同的工程领域开辟新的可能性，以提高人类的创造力和生产力。随着LLM在推理和泛化能力方面的不断进步，我们预计共生的人类人工智能团队将在工程创新和发现方面开辟新的视野和能力。

然而，在安全关键工程系统中部署基于LLM的Agent时，围绕信任、透明度和控制的问题仍然存在。在确保稳健性的同时，在人类和人工智能能力之间找到正确的平衡，将使这项技术能够充分发挥潜力。

在上面的部分中，我们根据基于LLM的自治Agent的应用介绍了以前的工作。为了更清楚地理解，我们在表3中总结了这些应用程序。

![图片](https://image.cubox.pro/cardImg/2023091319114443948/68113.jpg?imageMogr2/quality/90/ignore-error/1)


4 基于LLM的自主智能体评估
---------------


本节介绍了评估基于LLM自主智能体有效性的评估方法。与LLM本身类似，AI agent的评估也不是一个容易的问题。在这里，我们提出了两种常用的评估AI Agent的评估策略：主观评估和客观评估。（请参阅图3右侧的概述。）

![图片](https://image.cubox.pro/cardImg/2023091319114477173/63960.jpg?imageMogr2/quality/90/ignore-error/1)

### 4.1 基于主观评估


LLM的Agent具有广泛的应用。然而，在许多情况下，缺乏评估Agent性能的通用指标。一些潜在的特性，如智能体的智能性和用户友好性，也不能用定量指标来衡量。因此，主观评价在当前的研究中是必不可少的。

主观评价是指人类通过互动、评分等多种方式对基于LLM的Agent的能力进行测试。在这种情况下，参与测试的测试人员通常是通过众包平台招募的\[75，110，109，5，156\]；而一些研究人员认为，由于个体差异，众包人员不稳定，并使用专家注释进行测试\[163\]。在下文中，我们介绍了两种常用的杠杆策略。

**人类标注**：在一些研究中，人类评估者基于一些特定的视角，直接对基于LLM的Agent生成的结果进行排名或评分\[163，5，156\]；另一种评估类型是以用户为中心的，它要求人类评估者回答基于LLM的Agent系统是否对他们有帮助\[110\]，以及它是否用户友好\[75\]等等。具体来说，一种可能的评估是社会模拟系统是否能有效地促进在线社区的规则设计\[110\]。

**图灵测试**：在这种方法中，人类评估者总是被要求区分Agent和人类行为。在Generative Agents\[109\]中，第一批人类评估者被要求通过访谈来评估Agent在五个领域的关键能力。在两天的游戏时间后，另一组人类评估者将被要求在相同条件下区分Agent和人类反应。在Free-form Partisan Text 实验\[5\]中，人类评估者被要求猜测反应是来自人类还是基于LLM的Agent。

由于基于LLM的Agent系统最终为人类服务，因此在现阶段，人工评估发挥着不可替代的作用，但它也存在成本高昂、效率低下和群体偏见的问题。随着LLM的进步，它可以在一定程度上扮演评估任务的人的角色。

在目前的一些研究中，可以使用额外的LLM Agent作为结果的主观评估者。在ChemCrow\[8\]中，EvaluatorGPT通过评分来评估实验结果，评分既考虑了任务的成功完成，也考虑了潜在思维过程的准确性。ChatEval\[12\]基于LLM组建了一个由多个Agent裁判组成的小组，通过辩论评估模型产生的结果。我们相信，随着LLM的进展，模型评估的结果将更加可信，应用将更加广泛。

### 4.2 客观评价


客观评价比人类评价有几个好处。量化指标可以在不同方法之间进行清晰的比较，并跟踪一段时间内的进展情况。大规模自动化测试是可行的，允许对数千项任务进行评估，而不是对少数任务进行评估\[113，5\]。结果也更加客观和可重复。

然而，人类评估可以评估难以客观量化的互补性能力，如自然性、细微差别和社会智力。因此，这两种方法可以结合使用。

客观评估是指使用定量指标来评估基于LLM的自主Agent的能力，这些定量指标可以随着时间的推移进行计算、比较和跟踪。与主观或人为评估相比，客观指标旨在提供对Agent性能的具体、可衡量的见解。在本节中，我们将从指标、战略和基准的角度回顾和综合客观评估方法。

指标：为了客观评价Agent的有效性，设计适当的指标具有重要意义，这可能会影响评价的准确性和全面性。理想的评估指标应该准确地反映Agent的质量，并在现实世界场景中使用时与人类的感受保持一致。在现有的工作中，我们可以看到以下具有代表性的评估指标。

（1） 任务成功度量：这些度量衡量Agent完成任务和实现目标的能力。常见指标包括成功率\[156，151，125，90\]、奖励/得分\[156，151，99\]、覆盖率\[161\]和准确性\[113，1，61\]。值越高，表示任务完成能力越强。

（2） 人类相似性度量：这些度量量化了Agent行为与人类行为非常相似的程度。典型的例子包括轨迹/位置准确性\[163，133\]、对话相似性\[110，1\]和模仿人类反应\[1，5\]。相似性越高，推理就越像人。

（3） 效率指标：与上述用于评估Agent有效性的指标不同，这些指标从不同的角度评估Agent效率。典型指标包括规划长度\[90\]、开发成本\[113\]、推理速度\[161，133\]和澄清对话的数量\[99\]。

策略：基于用于评估的方法，我们可以确定几种常见的策略：

（1）环境模拟：在这种方法中，使用任务成功和人类相似性的指标，在游戏和互动小说等沉浸式3D环境中评估Agent，这些指标包括轨迹、语言使用、，并完成了目标\[16，156，161，151，133，99，137，85，149，155\]。这展示了Agent在现实世界场景中的实践能力。

（2） 独立推理：在这种方法中，研究人员通过使用有限的任务，如准确性、通道完成率和消融测量，专注于基本认知能力\[113，51，125，90，61，21，149，155\]。这种方法简化了对个人技能的分析。

（3） 社会评价：\[110，1，21，89，94\]使用人类研究和模仿指标直接探究社会智力。这评估了更高层次的社会认知。

（4） 多任务：\[5，21，114，93，94，149，155\]使用来自不同领域的各种任务套件，进行Zero-shot/Few-shot评估。这衡量了可推广性。

（5） 软件测试：\[66，69，48，94\]探索LLM在各种软件测试任务中的使用，如生成测试用例、再现错误、调试代码以及与开发人员和外部工具交互。他们使用测试覆盖率、错误检测率、代码质量和推理能力等指标来衡量基于LLM的Agent的有效性。

基准：除了指标外，客观评估还依赖于基准、受控实验和统计显著性测试。许多论文使用任务和环境的数据集构建基准，以系统地测试Agent，如ALFWorld\[151\]、IGLU\[99\]和Minecraft\[161133137\]。

*
  Clembench\[11\]是一种基于游戏的方法，用于评估作为会话Agent的聊天优化语言模型，该方法通过将LLM暴露在旨在挑战特定能力的受限游戏式设置中，探索有意义地评估LLM的可能性。
*
  Tachikuma\[85\]是一个基准，它利用TRPG游戏日志来评估LLM理解和推断与多个角色和新颖对象的复杂交互的能力。
*
  AgentBench\[93\]为评估LLM作为不同环境中的自主Agent提供了一个全面的框架，通过采用F1作为主要指标，实现了LLMAgent的标准化基准测试。它代表了首次对预训练的LLM作为跨不同领域的现实世界挑战的Agent进行系统评估。
*
  SocKET\[21\]是一个全面的基准，用于评估大型语言模型（LLM）在58项任务中的社会知识能力，涵盖幽默和讽刺、情绪和感受、可信度等五类社会信息。
*
  AgentSims\[89\]是一种多功能的基础设施，用于为大规模语言模型构建测试沙盒，促进数据生成和社会科学重新搜索中的各种评估任务和应用。
*
  ToolBench\[114\]是一个开源项目，旨在通过提供一个用于培训、服务和评估工具学习的强大大型语言模型的开放平台，促进构建具有通用工具使用能力的大型语言模型。
*
  Dialop\[88\]设计有三个任务：优化、规划和调解，以评估基于LLM的Agent的决策能力。
*
  WebShop\[149\]Benchmark通过搜索查询和点击，使用基于属性重叠和召回性能的奖励，评估LLMAgent对118万个真实世界项目的产品搜索和检索。
*
  Mobile Env\[155\]是一个易于扩展的交互平台，它为评估基于LLM的Agent在与信息用户界面（InfoUI）交互时的多步骤交互能力提供了基础。
*
  WebArena\[159\]建立了一个全面的网站环境，包括常见的域。该环境是一个以端到端方式评估Agent的平台，用于评估已完成任务的功能正确性。
*
  GentBench\[146\]是一个基准，旨在评估Agent的各种能力，包括推理、安全性、效率等。此外，它还支持评估Agent利用工具处理复杂任务的能力。


总之，客观评估能够通过任务成功率、人类相似性、效率和消融研究等指标对基于LLM的Agent能力进行定量评估。从环境模拟到社会评价，针对不同的能力，出现了一套多样化的客观技术工具箱。

虽然目前的技术在衡量一般能力方面存在局限性，但客观评估提供了补充人类评估的关键见解。客观评估基准和方法的持续进展将进一步推动基于LLM的自主Agent的开发和理解。

在上面的部分中，我们介绍了基于LLM的自治Agent的主观和客观评估策略。Agent的评估在这一领域发挥着重要作用。然而，主观评价和客观评价各有优缺点。也许，在实践中，它们应该结合在一起，对Agent进行全面评估。我们在表3中总结了之前的工作与这些评估策略之间的对应关系。

5 相关综述
------


随着大型语言模型的蓬勃发展，出现了许多全面的调查，对各个方面提供了详细的见解。

*
  \[157\]广泛介绍了LLM的背景、主要发现和主流技术，包括大量现有工作。
*
  \[148\]主要关注LLM在各种下游任务中的应用以及与部署相关的挑战。将LLM与人类智力相结合是解决偏见和幻觉等问题的一个活跃研究领域。
*
  \[136\]汇编了现有的人类协调技术，包括数据收集和模型训练方法。
*
  推理是智力的一个重要方面，影响决策、解决问题和其他认知能力。\[62\]介绍了LLM推理能力的研究现状，探索了提高和评估其推理技能的方法。\[100\]提出，语言模型可以通过推理能力和利用工具的能力来增强，称为增强语言模型（ALM）。他们对ALM的最新进展进行了全面回顾。
*
  随着大规模模型的使用越来越普遍，评估其性能变得越来越重要。\[14\] 阐明了LLM的评估、评估内容、评估地点以及如何评估其在下游任务中的表现和社会影响。\[13\] 还讨论了LLM在各种下游任务中的能力和局限性。


上述研究涵盖了大型模型的各个方面，包括培训、应用和评估。然而，在本文发表之前，没有任何工作专门关注基于LLM的Agent这一快速出现且极具前景的领域。在这项研究中，我们汇编了100篇关于基于LLM的Agent的相关著作，涵盖了它们的构建、应用和评估过程。


6 挑战
----


尽管之前基于LLM的自主AI Agent的工作已经显示出许多有前景的方向，但该领域仍处于初级阶段，其发展道路上存在许多挑战。在下文中，我们提出了几个重要挑战。

### 6.1 角色扮演能力


与传统的LLM不同，AI agent通常必须扮演特定的角色（如程序编码员、研究员和化学家）来完成不同的任务。因此，Agent的角色扮演能力是非常重要的。虽然对于许多常见的角色（例如影评人），LLM可以很好地模拟它们，但LLM仍然很难捕捉到许多角色和方面。

首先，LLM通常是基于网络语料库进行训练的，因此对于网络上很少讨论的角色或新出现的角色，LLM可能无法很好地模拟它们。此外，先前的研究\[49\]表明，现有的LLM可能无法很好地模拟人类的认知心理特征，导致在对话场景中缺乏自我意识。这些问题的潜在解决方案可能会微调LLM或仔细设计Agent提示/架构\[77\]。例如，人们可以首先收集不常见角色或心理特征的真实人类数据，然后利用这些数据来微调LLM。然而，如何确保微调后的模型仍然能很好地执行常见角色可能会带来进一步的挑战。除了微调之外，还可以设计定制的Agent提示/架构，以增强LLM在角色扮演方面的能力。然而，找到最佳提示/架构并不容易，因为它们的设计空间太大。

### 6.2 广义人类价值观对齐


在自主AI Agent领域，特别是当Agent用于模拟时，我们认为应该更深入地讨论这个概念。为了更好地为人类服务，传统的LLM通常会经过微调，以符合正确的人类价值观，例如，Agent不应该计划制造一枚为社会复仇的炸弹。

然而，当Agent被用于真实世界的模拟时，理想的模拟器应该能够诚实地描述不同的人类特征，包括具有错误值的特征。事实上，模拟人类的消极方面可能更重要，因为模拟的一个重要目标是发现和解决问题，没有消极方面就意味着没有问题可以解决。例如，为了模拟现实世界的社会，我们可能必须允许Agent计划制造炸弹，并观察它将如何执行计划以及其行为的影响。基于这些观察，人们可以采取更好的行动来阻止现实社会中的类似行为。

受上述案例的启发，基于agent的模拟可能面临的一个重要问题是如何进行广义的人类对齐，即对于不同的目的和应用，agent应该能够与不同的人类价值观进行对齐。然而，包括ChatGPT和GPT-4在内的现有强大LLM大多与统一的人类价值观保持一致。因此，一个有趣的方向是如何通过设计适当的提示策略来"重新调整"这些模型。

### 6.3 提示鲁棒性


为了确保Agent的合理行为，设计师通常会将额外的模块（如记忆和计划模块）纳入LLM中。然而，纳入这些模块需要开发更多的提示，以促进一致的操作和有效的沟通。

先前的研究\[162，52\]强调了LLM提示缺乏稳健性，因为即使是微小的改变也会产生显著不同的结果。当构建自治Agent时，这个问题变得更加明显，因为它们包含的不是单个提示，而是考虑所有模块的提示框架，其中一个模块的提示有可能影响其他模块。

此外，提示框架在不同的LLM之间可能存在显著差异。开发一个可应用于各种LLM的统一而强大的即时框架是一个重要但尚未解决的问题。对于上述问题，有两种潜在的解决方案：

*
  （1）通过试错手动制作基本提示元素。
*
  （2）使用GPT自动生成提示。

### 6.4 幻觉


幻觉对LLM提出了根本性挑战，其中模型错误地自信地输出虚假信息。这个问题在自主Agent中也很普遍。例如，在\[67\]中，观察到当在代码生成任务中遇到简单化的指令时，Agent可能会表现出幻觉行为。幻觉可能导致严重后果，如错误或误导性代码、安全风险和道德问题\[67\]。为了解决这个问题，一种可能的方法是将人类校正反馈纳入人类-主体相互作用的循环\[58\]。关于幻觉问题的更多讨论可以在\[157\]中看到。

### 6.5 知识边界


自主AI Agent的一个重要应用是模拟不同的真实世界人类行为\[109\]。人类模拟的研究有着悠久的历史，最近人们的兴趣激增可归因于LLM取得的显著进步，LLM在模拟人类行为方面表现出了显著的能力。

然而，重要的是要认识到LLM的力量可能并不总是有利的。具体来说，理想的模拟应该准确地复制人类的知识。在这方面，LLM可能会表现出过度的权力，因为它们是在超出普通人范围的广泛网络知识库上进行训练的。LLM的巨大能力可以显著影响模拟的有效性。

例如，当试图模拟各种电影的用户选择行为时，确保LLM处于对这些电影一无所知的位置是至关重要的。然而，LLM有可能已经获得了有关这些电影的信息。如果不实施适当的策略，LLM可能会根据他们广泛的知识做出决定，即使现实世界的用户事先无法访问这些电影的内容。基于以上例子，我们可以得出结论，对于构建可信Agent仿真环境，一个重要的问题是如何约束LLM的用户未知知识的使用。

### 6.6 效率


由于LLM的自回归架构，其推理速度通常较慢。然而，Agent可能需要多次查询每个动作的LLM，例如从记忆模块中提取信息、在采取行动之前制定计划等。因此，LLM推理的速度在很大程度上影响了Agent动作的效率。使用相同的API密钥部署多个Agent可能会进一步显著增加时间成本。


[跳转到 Cubox 查看](https://cubox.pro/my/card?id=7101526745844548407)
