OpenAI"打死都不说"的GPT-4训练细节被传出，这是我的解读
=================================

[www.36kr.com](https://www.36kr.com/p/2347761478475396)关注

那是前几天一个普通的上午。我正在日常搬砖，突然各路信息席卷而来：「赶紧的，GPT-4 模型构架泄露啦，国产大模型要再次超越啦！」

打开社媒一看，好么，都不用会英语，国内的人翻机翻都已经上线了，这速度，我是真的服气。但是，等我去追根溯源，看看信息到底有几分靠谱的时候，我突然就有把科技圈逛出了娱乐圈的感觉。

鉴于目前「Fake News」满天飞的互联网现状，我看到这个消息后，干的第一件事就是追本溯源。

**01 来龙去脉**
-----------

我信息挖掘的起点是 Hacker News 上 分享 的、通过 Thread Reader 提取的 推文串 （ 存档 于 7 月 11 日）。点开一看，上来就是两句：
> GPT-4's details are leaked.
>
> It is over.

这标题党水平完全不逊色于国内。

众所周知，OpenAI 在发布 GPT-4 的同时打破了自己对 open 的承诺，没有透露任何权重或技术细节，被业界广泛批评。这大概就是博主为什么要用 It is over 这个梗 来渲染「情节反转」的戏剧效果。

再看内容，正是 OpenAI 守口如瓶的 GPT-4 训练细节。这些信息前面有过很多猜测，但是官方一直都没有披露，提到的时候都说的很模糊（原文比较晦涩，用了很多缩写和行话，一些会在后文解释）：

**模型参数量**：1.8 万亿，比 GPT-3.5（1750 亿）大 10 倍左右。

**模型层深**：120 层。

**模型构架**：混合专家模型（MoE，解释见后文），一共 16 个「专家」，每个专家 1110 亿参数量。每次向前传递推理（生成一个 token 的输出）选择两个专家。

**训练数据**：共 13T（13 万亿个）token 的数据。文本数据被重复训练了 2 次，代码数据被重复训练了 4 次。这个数据其实挺重要的，后续具体分析。

**并行策略**：8 路张量并行 + 16 路管道并行。有多个位于不同数据中心的多个 GPU 集群同时训练，每个集群有 128 个 GPU。

**预训练上下文**：8K。32K 版本是在 8K 基础上微调的。

**训练成本** ：在约 25000 张 A100 上，以大约 2.15e25 flops 的速率，持续训练 90 至 100 天。按照每张 A100 小时 1 美元，大约需要 6300 万美元。（如今可在约 55 天内使用约 8192 张 H100 完成，费用估算为 2150 万美元。）

问题是，这些信息是怎么弄来的，是否靠谱呢？

顺藤摸「瓜」，我又找到了这串推文的发布者------ Yam Peleg 。

![](https://image.cubox.pro/article/2023071715305554057/89555.jpg?imageMogr2/quality/90/ignore-error/1)

这老哥的账号虽然我没关注，但还真看过他以前的文章。他是以色列一个「创业公司」的 CEO（但成立有 15 年，可能再叫创业公司不太合适了）；本人工程经历丰富，很懂大语言模型，曾经尝试反向破解过 GPT-4 和 ChatGPT 代码解释器。今年六月，OpenAI 成员访问以色列的时候，Peleg 还去参加座谈沟通了，并且还和 CEO Sam Altman 合影。

读这老哥的文章，我就禁不住想起来在以色列时候见过的一位学生联络员 Tom，随便说点啥都能给你搞得热血沸腾的。

![](https://image.cubox.pro/article/2023071715305573227/81088.jpg?imageMogr2/quality/90/ignore-error/1)

左起：Sam Altman、Yam Peleg（来源：@Yampeleg）

考虑到这老哥一直在研究 OpenAI，也认识 OpenAI 内部很多人，所以他如果得到了点啥内部消息，我觉得可信度其实还挺高的。

但等我晚上准备去仔细研读下他发的东西的时候，突然发现他把前面发的都删了。本来我以为是被 OpenAI 捂嘴了，还庆幸自己留了档。后面仔细一看，发现不是因为 OpenAI 要求删除，而是因为他也是从一个付费专栏转述的，被人投诉侵犯了版权。

![](https://image.cubox.pro/article/2023071715305575117/22765.jpg?imageMogr2/quality/90/ignore-error/1)

这篇原始来源是一个叫做 SemiAnalysis 的 Substack 专栏，他们稍早之前发了一篇题为 GPT-4 Architecture, Infrastructure, [Traini](https://pitchhub.36kr.com/project/1981899352519945)ng Dataset, Costs, Vision, MoE 的文章，放在付费墙后。

查了一下得知：
> SemiAnalysis 是一个精品半导体研究和咨询公司，专注于从化学原料到晶圆厂到设计 IP 和战略的半导体供应链。该公司由 Dylan Patel 创办，一位有着多年半导体行业经验的分析师和工程师。Patel 曾在英特尔、AMD、高通等公司担任过从设计工程师到市场营销经理等的角色。
>
> SemiAnalysis 的团队还包括多位专业的半导体分析师和咨询顾问。他们各自有着不同的专长领域，如 AI、云计算、网络、存储、电动汽车、射频、物联网等。他们为客户提供了从化学原料到晶圆厂到设计 IP 和战略的全方位的半导体供应链分析和咨询服务。

早些时候，SemiAnalysis 还发过一篇文章披露谷歌工程师在内部通信中说「 我们没有护城河，但 OpenAI 也没有 」（We Have No Moat, And Neither Does OpenAI），引起不小议论。这篇文章后面被证实为真。

这样看来，Dylan Patel 老哥可能确实有些内线，他们给出的信息可信度应该还是可以的。

至于他们为啥这么急着让 Yam 老哥删推------因为这些「内部信息」确实价值不菲，订阅 SemiAnalysis 的付费文章，一年要 500 美元。Yam 老哥订阅的精英版更是要 1000 美元。

**梳理分析**

根据这个来龙去脉，我的看法是，这个传闻还是有一定的可信度的。以下则是我基于这些信息的一些分析，提出来供大家讨论。

**02 私有模型的竞争将集中在并行能力上**
-----------------------

> 根据此次传闻，目前如果要训练一个 GPT-4 竞品，按照使用约 8,192 个 H100 芯片来估算，以每小时 2 美元的价格，在约 55 天内可以完成预训练，成本约为 2150 万美元（1.5 亿人民币）。

这个成本对于目前波涛汹涌的 LLM 市场来说，真的不算大。国内目前的主要玩家都可以比较轻松地承担数次训练。所以，这次说真的，再过半年模型能力（起码参数规模）对标 GPT-4 可能真的不是吹牛。

如果训练成本不是问题，那么训练数据会不会成为问题呢？我认为也不会。传闻称，GPT-4 的训练数据共 13T（13 万亿个）token。作为对比，CommonCrawl 和 RefinedWeb 两个公开数据集都是 5T 个 token，传闻称，余下部分来源 Twitter、Reddit 和 YouTube；一些诉讼还主张 OpenAI 使用了来自 LibGen、SciHub 等「影子图书馆」的盗版数据。

因此，我认为这个数据规模并不是不可企及，再加上国内本身也积累了很多中文资源，所以训练数据也应该问题不大。

其他的类似预训练、微调以及中文编解码等问题，其实也不存在太多的技术秘密，方法还是 比较公开的。给足够的资源，半年时间应该都可以解决。

所以，最后剩下的门槛就是并行能力了。其实这次传闻里面用了极大的篇幅去介绍相关的内容，专业程度还是比较高的，这里我只能做些粗浅解释。

粗略地说，所谓并行问题，就是你有了大模型，如何以最低的成本让最多的人同时使用。这里面涉及到很多专业的设计问题，在运算资源固定的情况下，应该如何分配不同环节的运算资源？如何处理并发？如何管理内存？

并行处理的能力直接决定了用户体验。目前基于 GPT-3.5 的 ChatGPT 和 API 都比较流畅了，这是非常厉害的。这里大家可能会说，我体验的其他国产 LLM 或者 Claude 都比 GPT-3.5 还快啊。但是，大家没有考虑使用的量级问题，GPT-3.5 在这么高的并发下有这样的性能，其他的厂商如果匹配不了 OpenAI 的这个能力，也就没能力来抢 OpenAI 的市场。

**所以，并行能力可能会成为各路 OpenAI 竞争对手的角逐重点之一。**

**03 GPT-5 的重点在于多模态**
---------------------

前面提到，传闻称 GPT-4 是由 16 个专家模型组成的「专家混合」（mixture of experts, MoE）模型。这里简单解释一下什么是「专家混合」，这是指将用户的「问题」划分成若干子问题，每个子问题交给一个较小的模型（也就是一个「专家」）去解决，然后通过一个「路由模型」进行选择和组合，再输出给用户。

传闻进一步称，GPT-4 的每个「专家」有 1110 亿参数量------相当于 GPT-3（这个和 Sam Altman 前期说的 GPT-4 参数甚至小于 GPT-3.5 相符），其中有 550 亿个参数是共享的。每次向前传递推理（生成一个 token 的输出）使用两个「专家」，事实上耗费的参数量约为 2800 亿。这个数字显著小于不用 MoE 所需的数量，也和前期很多学者预测的类似。

值得注意的是，传闻指出 GPT-4 训练所用的文本和代码数据都是被重复使用过的。再结合使用 MoE 构架的选择，我个人猜测：要么目前可以比较方便获取的高质量文本数据已经接近枯竭，要么无限制增大数据量对 LLM 性能的提升已经非常有限了。

但是，无论是哪一种情况，GPT-5 想要有大的性能突破，就必须能够充分利用现存的大量视频、图片以及音频数据，换言之是一个「多模态」的模型。

问题在于，根据这次的传闻，OpenAI 目前的视觉多模态并没有太多的过人之处。它是一个独立的视觉编码器，使用文本做输入进行预训练，然后使用约 2 万亿个 Token 进行微调。这种训练方式，明显无法充分利用已有的视频、图片以及音频数据。

所以，OpenAI 一直强调还没有训练 GPT-5，大概率是真话。在训练 GPT-5 之前，他们还得找到一个更好的多模态模型构架，让模型可以充分利用音视频数据。而只有能够利用这些优质的训练数据，GPT-5 才有可能获得足够的能力提升。（同时，如果 GPT-5 真的能够充分利用这些音视频数据的话，那不管是 AGI 还是 OpenAI 最近提出的「超智体」，似乎确实也没那么遥远了。）

**04 OpenAI 可能是有意放出的本次传闻**
--------------------------

这个推论就完全是个人的瞎猜了。事实根据不足，大家看看就好。

我的理解是，OpenAI 很清楚 GPT-4 的护城河并不深；在如今这种热潮中，竞争对手迎头赶上也并不困难。并且如上分析，他们现在的多模态大模型构架应该还没有搞定，这个时候如果有新的玩家上来就从多模态开始突破，OpenAI 被弯道超车的概率也是很大的。

所以，这也许是 OpenAI 的缓兵之计------我就给你们透露一些 GPT-4 的信息，让头部的玩家都先去做 GPT-4 的复刻工作，把 OpenA I 已经走过的路也再走一遍。

如果在这个过程中，OpenAI 给 GPT-5 的训练打好了基础，完成了多模态大模型的前期攻关，即使 GPT-4 已经被其他的大语言模型超越，OpenAI 也不慌了。个人认为，多模态很可能就是人卷人的最后一代了，再未来的模型开发和演进说不定就以 AGI 为主力了。也就是说，这次赢了，可能就赢到最后了。

本文来自微信公众号["少数派"（ID：sspaime）](http://mp.weixin.qq.com/s?__biz=MzU4Mjg3MDAyMQ==&mid=2247557457&idx=2&sn=4fe65e967fa4b599ce99b9338167ea24&chksm=fdb2143bcac59d2d624cf7b49e487eb477d7909f2d26a643daa1ecad3d9895beae682bfbc091#rd)，作者：博而不士，36氪经授权发布。

[跳转到 Cubox 查看](https://cubox.pro/my/card?id=7080518973409724392)
