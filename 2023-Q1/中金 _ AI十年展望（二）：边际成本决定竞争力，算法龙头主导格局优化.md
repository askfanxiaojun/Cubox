中金 \| AI十年展望（二）：边际成本决定竞争力，算法龙头主导格局优化
====================================

[mp.weixin.qq.com](http://mp.weixin.qq.com/s?__biz=MzI3MDMzMjg0MA==&mid=2247547307&idx=1&sn=00d20841f86a365cce43e0619fb77181&chksm=ead0df6cdda7567aa3e12edb24cf339d283fe0fc484f4c88dd5cafb8047a2914b00ff3c05a4c&mpshare=1&scene=1&srcid=0321RKRSOmGXyK18yEYsrm0H&sharer_sharetime=1679361674341&sharer_shareid=c58007142b3c8dd4da3163f5c61d6b7b#rd)陈星宇 于钟海等 中金点睛


![图片](https://image.cubox.pro/article/2021081608551348670/42624.jpg)


**摘要**


AI行业处在商业落地初期，目前呈现出场景多、需求碎片化的行业痛点。本文指出边际成本是未来AI竞争的核心要素，我们认为，龙头有望率先实现AI行业的"工业革命"，并受益于数据-模型的反馈优化机制带来的先发优势，未来将占据较高份额。我们预计AI核心的平台、算法层将朝着一超多强的格局演变，持续底层投入的龙头企业能够建立成本及全栈技术的护城河，并通过大模型的技术路线解决碎片化的问题，在未来竞争中占领高地。  

**边际成本是未来十年AI行业竞争核心要素，AI平台及算法层有望演进为类似基础软件的分化格局，龙头规模效应优势明显。**各行业的数字化转型意味着大量AI模型需求，AI模型通用性低导致项目碎片化、交付效率低是行业目前痛点，未来高生产效率、低边际成本是AI竞争焦点。龙头厂商通过全栈底层能力提升实现AI模型的工业化生产，从根本上提升效率。另一方面，用于AI模型的算力增长、算法效率优化呈现新"摩尔定律"，且基于数据-模型反馈机制，龙头算力、算法正向循环强化巩固先发优势，我们预计AI在关键的平台层、算法层有望形成龙头遥遥领先的分化格局。

**持续底层投入才能铸就技术成本、效率的护城河，国内目前底层技术自研能力较为稀缺。**在AI技术体系内，软件框架是核心，目前主要由海外巨头引领，国内仅少数巨头具备底层自研能力。算力层方面，AIDC能力国内稀缺，我们认为，AI公司自建算力中心或将成为未来趋势，同时自建AIDC实现AISC算力云化是突破英伟达CUDA生态壁垒的关键；平台层方面，基于软件训练框架、生产平台、部署框架等，AI算法行业有望迎来"工业革命"，实现效率的大幅提升。算法层则借力大算力下通用模型带来的海量场景模型发挥规模效应，降低边际成本。

**大模型技术路线是解决碎片化问题的突破点，技术、资本壁垒较高，是AI巨头之间的战场。**传统的精妙模型在长尾需求中暴露短板，近年大模型发展加速，是实现通用AI的重要方向。大模型+小模型是降低算法边际成本的关键架构，但技术和算力壁垒高，常见开源框架对于大模型场景的性能支撑不足，通过自主框架进行底层定制优化是实现大模型的关键。

**风险**

国内技术进步、生态建设不及预期，行业竞争加剧，海外出口管制风险。


**正文**


**图表：全文章节结构框架**


![图片](https://image.cubox.pro/article/2022050622112665510/10390.jpg)


资料来源：中金公司研究部


**一、算法边际成本是未来十年AI市场竞争焦点**


**目前AI行业痛点在于碎片化、边际成本高**


AI行业碎片化的本质在于模型生产仍停留在"手工作坊"时代

**目前AI项目落地还停留在"手工作坊"阶段，存在重复造轮子情况，边际成本高。**目前国内大部分AI项目的落地是以项目制的形式，主要包括需求阶段、打光阶段、数据阶段、算法设计阶段、训练评估阶段、部署阶段和运维阶段。其中，数据阶段和训练评估阶段往往需要多次循环，专家驻场收集数据并训练模型，发现指标无法满足需求后再重新进入数据阶段，一个项目往往需要专家团队驻场数月完成。


**图表：目前AI项目开发的流程繁琐，数据阶段和训练评估阶段的循环占用了大量时间**


![图片](https://image.cubox.pro/article/2022050622112660580/28380.jpg)


资料来源：极市平台公众号，中金公司研究部


**碎片化的本质原因在于现阶段AI模型的通用性低，模型生产还停留在"手工作坊"的时代，单个模型只适用特定任务。**即使同样的算法在不同场景落地，也会演化出非常不同的版本，一旦场景和任务发生变化，就需要重新收集、标注数据、训练模型。由于客户需求多样，以至于几乎每个项目都要重复进行这一流程，研发流程难以复用。

需求端：数字化意味着大量模型需求，高生产效率，低边际成本是关键

**全社会的数字化是人工智能的重要目标，同时也意味着巨量的建模需求。**随着数字信息世界、物理世界融合，产生的数据量是以前的成千上万倍，以无人车为例，一辆无人车每天产生的数据量大约5-10T。数字化社会海量的数据只能交给机器处理，因此需要更多的算法来识别和检测，在机器处理完成后再把数据传递给人员。

**巨量的模型需求需要较高的AI模型生产效率、较低的算法边际成本。**如果要完成设备数字化的建设，必然需要涵盖社会的方方面面，像人脸识别这类流量大、数据多、投入产出比高的场景占少数，还存在大量的范围广、差异大、频次低的长尾场景，这样广泛的场景凭借定制化开发的路线难以实现全面的数字化覆盖。

供给端：AI人才稀缺、基础设施不完善导致项目落地成本高

**技术层面，深度学习作为一门新的学科人才稀缺。**上世纪90年代的互联网潮流吸引了大部分计算机科研和工作人员，也导致了深度学习的沉寂，直到2012年Hinton带领的团队在ImageNet挑战赛中大幅降低图片识别错误率，深度学习才开始被业界和资本关注，全球范围内极少数人专注于此领域研究，导致人才供给非常稀缺。

**深度学习人才稀缺导致整体项目实现难度、成本高，未来解决方案落地推广的核心方向在于降低边际成本。**早期AI落地多以项目定制为主要形式，这样重人力的落地方式和AI人才稀缺性造成AI项目的人力成本很高，AI的落地仅限于人效高的场景，长尾场景解决方案的投入产出比让甲方和乙方望而却步。我们认为，未来解决方案落地的方向在于降低边际成本。


**大模型时代强者愈强，数据-模型反馈优化机制决定先发优势**


大模型时代下，AI模型进入工业化生产模式，规模效应导致马太效应

**大模型预训练的难题被攻克，预训练大模型成为降低边际成本的关键。**GPT-3的出现让业界充分认识到，大模型是产业未来趋势。即先使用海量数据预训练大模型，得到一套模型参数，然后用这套参数对模型进行初始化，再进行训练，预训练大模型能大幅提高模型的生产效率。

**基于预训练大模型的时代到来，模型生产的效率提高、边际成本降低。**包括商汤科技、百度、华为等国内AI龙头企业在GPT-3面世之前就前瞻认识到，基于预训练大模型的AI模型工业化方式是未来趋势，并在训练框架、算力平台等底层技术设施上大力投入，目前已经在工业化AI生产的方式上拥有较强的先发优势。

数据-模型反馈机制为龙头企业带来较强的先发优势

**数据-模型反馈优化机制是深度学习技术的重要特点。**深度学习模型的准确性很大程度是数据驱动的，数据量大、质量高、多样性强很大程度上解决了训练过拟合的问题，是好模型的基础。因此，人工智能的数据反馈机制是由深度学习底层技术原理所决定的。


**图表：数据量不足容易带来过拟合问题，更多的数据有利于更好的模型**


![图片](https://image.cubox.pro/article/2022050622112618172/44411.jpg)


资料来源：fast.ai，中金公司研究部


**数据反馈机制意味着先发优势，抢占标杆客户是目前AI行业的重要竞争点。**由于AI行业独特的数据反馈机制造就了行业的先发优势，当下处于AI大规模商业化落地的早期阶段，跑马圈地、抢占优质的大客户是AI行业关键竞争点。


**AI新"摩尔定律"下，龙头凭借边际成本优势有望主导一超多强格局**


AI模型正经历从"手工作坊"到工业化生产的转变

**AI的工业化时代需要新的底层基础设施，高强度的底层投入才能解决生产效率问题。**只有在底层硬件、训练框架、算法模型、通用技术上大力投入，才能解决生产效率问题，如果不投入底层技术，AI项目难以摆脱以重人力方式落地的限制，AI公司不得不在应用层投入大量人力。看起来，与应用层AI公司类似，大力投入底层技术设施的AI公司同样保持着数千人的研发团队，但其建立起了远远更强的竞争优势。

**由于AI底层投入资金量大、风险大，全球仅个别巨头在AI底层投入重金，国内企业大多布局风险更低的应用层，仅少数企业在AI技术底层大力投入。**底层投入资金需求大、时间长、风险高，实力雄厚的美国互联网巨头对AI底层技术战略性投入力度较大，但中国的AI产业主要受需求拉动，大多数AI公司布局应用层。

►深度学习平台方面：国内大部分公司的AI研发都选择使用外资巨头开源的TensorFlow、PyTorch等深度学习底层框架。


**图表：企业使用AI底层框架企业占比调研情况**


![图片](https://image.cubox.pro/article/2022050622112617956/19508.jpg)


资料来源：《德勤中国成长型AI企业研究报告：迈向巅峰之路》，中金公司研究部


►算法方面：算法研发是一个高风险的事情，算法的领先需要庞大的队伍投入，需要研究人员不断突破算法的极限，持续在顶级期刊中发表论文。

►硬件方面，为了支持框架和算法，需要的计算力大，但大量GPU并联计算会导致严重内存墙和算力墙等问题，大幅增加训练的成本，因此需要对硬件和框架做深度优化。


**图表：我国AI公司技术层次分布（2020年）**


![图片](https://image.cubox.pro/article/2022050622112686078/90909.jpg)


资料来源：中国新一代人工智能发展战略研究院，中金公司研究部


**通过底层投入，AI模型生产和部署正在经历从"小作坊"到工业化生产的转变。**以商汤科技AI大装置为例，与 "手工作坊"式AI模型的打造方式不同，传统AI大装置是更像是流水线工厂，可以从底层上对各种场景的算法模型进行抽象，使用各种算法工具模块，通过组合算法套件模块化的进行新场景的定制，以很低的边际成本规模化生产、部署AI模型。

**龙头厂商通过全栈底层能力的提升，在效率上实现千倍提升，率先实现从项目制向标准化服务转变，AI行业开始分化。**以商汤为例，受益于多年来在底层技术的持续投入，其算法的生产效率显著提高。根据公开资料，商汤部分算法的生产效率在过往5年中提升了约1000 倍，同样精度的一个算法模型，5年前可能需要10个研究员花半年时间来研发，现在只需要一个研究员花三天就可以完成研发。

大模型+大装置解决边际成本问题，碎片化痛点有望解决

**AI行业不会向碎片化的方向发展，而是强者愈强。**在大模型路线下，除了需要的训练数据量更少，模型精度更高外，AI模型的边际成本还会受益于三个因素的影响大幅降低：底层基础设施可复用、模型研发流程可复用、研发流程自动化。因此AI行业将出现强者愈强的马太效应，我们认为，AI行业不会向着碎片化的方式发展。

**►底层基础设施可复用，降低边际成本。**以商汤科技的AI大装置为例，公司将多年累积的硬件、框架和AI算法和落地经验结合起来，一起融合到AI大装置，尽可能的减少重复研发。

**►模型研发流程可复用，大模型提升AI通用性。**在大模型压缩制造小模型的工业化生产方式下，AI公司可以生产大量的、覆盖不同场景的模型，遇到新场景时，可以通过将原有的模型模块化组装，快速制造新模型。

**►研发流程自动化（AutoML），开发门槛降低，人员成本降低。**AutoML能自动执行部分工程性任务，减少了AI模型生产过程中对专家的需求量，使机器学习的使用门槛降低，不再需要招聘大量深度理解AI工程的专家。

具备规模效应的AI企业将在算法生产建立强大的先发优势

依靠底层技术投入，AI龙头的先发优势和规模效应主要体现在如下方面：

**►底层投入技术、人才、资金壁垒高，时间窗口短：**底层技术投入属于高风险、回报周期长的项目，仅有国内外少数龙头企业能大力投入。由于底层基础设施中算力和算法的新"摩尔定律"，率先投入基础设施企业能在技术迭代中与竞争对手迅速形成代差，因此底层基础设施投资的时间窗口期短。

**►通用大模型+小模型的方式减少定制化：**通过不断在算法、算力和数据上的突破极限，生产智能程度尽可能高的通用模型，减少进入新场景时定制化带来的数据收集、算法生产等边际成本。

**►算法生产上建立数量级的领先，模块化生产大幅降低进入新场景的边际成本：**通过大量的模型积累，AI企业可以沉淀可复用的算法底层模块，再模块化组装的方式生产AI模型。

**由于数据-模型反馈优化机制的存在，规模效应会带来强大的先发优势，龙头公司进一步掌握定价权。** 由于边际成本低，诸如商汤等龙头公司能够率先低成本地解决长尾场景，满足客户的商业场景的闭环，并取得在AI行业的定价权。抢占各细分领域的关键客户后，龙头能得到更多的数据，进一步优化自身的模型，积累更丰富的算法库，从而有了更低的边际成本。**数据反馈机制为AI龙头带来强大的、难以复制的先发优势。**


**图表：AI龙头有望率先解决长尾场景，完成商业模式的闭环，解决碎片化问题**


![图片](https://image.cubox.pro/article/2022050622112631374/35936.jpg)


资料来源：WAIC2021，中金公司研究部


**AI基础设施的算力、算法呈现新"摩尔定律"，AI行业或将出现类似半导体行业的龙头独大格局。**龙头本身算法效率领先，相同算力下能训练生产更优质的模型，同时最先进的AI模型约每几个月算力需求就会扩大一倍，构建高并行效率的AIDC的难度更高，成本更大。我们认为，随算法和算力的持续迭代，行业有望形成类似半导体行业龙头独大的格局，边际成本领先的龙头如商汤等将持续领跑。

**►**算力方面，根据斯坦福大学和麦肯锡联合发布的《2019人工智能指数报告》，2012年之前最先进AI模型计算量每两年翻一倍；2012年之后计算量每3.4个月翻一番，从2012年到2020年3月已增长30万倍。

**►**算法方面，OpenAI发现最高效的AI模型，所需的算力每16个月就会减少1/2。


**图表：算力增长和算法的效率增长均呈现出新"摩尔定律"**


![图片](https://image.cubox.pro/article/2022050622112673134/20692.jpg)


资料来源：OpenAI，中金公司研究部


**远期来看，物理世界的数字化有望带来全新的商业模式，AI工业化生产的龙头企业凭借先发优势有望率先推动新场景、新商业模式。**人工智能技术有望重塑社会的工作流程，带来成倍的效率增长，并在B端和C端建立新的商业模式。但这样的愿景背后需要强大的算力、强大的空间感知能力、海量信息的实时的提取和更新能力等等，率先实现AI的工业化生产的企业有望率先实现远期商业模式的愿景：

**►B端：**龙头企业可以构建基于城市物理空间的数字化搜索的引擎和推荐系统，将虚拟世界的搜索转化到物理世界的搜索，实现物理世界的搜索引擎。

**►C端：**通过元宇宙，将类似现实世界的虚拟世界投射进去，所有现实世界的商业模式都可以在元宇宙中再造。


**二、AI大装置：持续底层投入铸就技术成本技术护城河，国内底层自研能力稀缺**


**AI技术体系架构：软件框架是核心，全栈能力铸就护城河**

AI技术体系分为算力、平台软件、算法、应用软件及解决方案

深度学习分为训练(training)和推断(inference)两个环节：训练需要海量数据输入，训练出一个复杂的深度神经网络模型；推断指利用训练好的模型，使用待判断的数据去"推断"得出各种结论。

AI产业技术体系架构可划分如下：

**►算力层：**包括AI芯片和AIDC。AI芯片需要具备极强的并行计算能力。AIDC中的智算中心需要根据算法进行深度优化，提高并行计算效率。

**►平台软件层：** 平台层是支撑AI大规模训练生产、部署的技术体系，包括训练框架、模型生产平台、推理部署框架、数据平台。其中**训练、部署软件框架是整个技术体系的核心，软件框架作为深度学习算法的工程实现，是巨头竞争的核心点。**

**►算法模型层：**算法模型本质上是特殊的软件，是AI企业公司在科研和产业经验中的先进算法在平台上的沉淀。

**►应用软件及解决方案层：**应用软件指基于AI算法模型的赋能之后，以解决方案的形式实现场景落地。


**图表：AI行业整体技术体系架构**


![图片](https://image.cubox.pro/article/2022050622112652131/20025.jpg)


资料来源：中国信通院，中金公司研究部


**软件框架是整个AI技术体系的核心，巨头以开源软件框架为核心打造生态：**

►主流训练软件框架：TensorFlow（谷歌）、pyTorch（脸书），Caffe/2（脸书，图像处理领域生态积累深厚）、MXNet（亚马逊）、CNTK（微软）、SenseParrots（商汤）、PaddlePaddle（百度）

►主流推断软件框架：推断计算量相对小很多，往往限定性能及功耗，典型的有TensorRT（英伟达）、TensorFlow Lite（谷歌）、SensePPL（商汤）等

►现有格局：谷歌早期在软件框架领域领先，美国其他巨头探索模型互换、模型迁移以联合对抗谷歌；近年来pyTorch为代表的技术框架在工业界发展势头较好，有赶超谷歌之势。国内仅商汤、百度等极少数企业具有自研底层软件框架能力。


**图表：国内外典型深度学习软件框架情况**

![图片](https://image.cubox.pro/article/2022050622112677228/80509.jpg)

资料来源：CSDN，中金公司研究部


软件框架分析：以TensorFlow为例

TensorFlow由Google Brain团队主要支撑，与Facebook推出的pyTorch一道成为全球主要活跃社区，以功能全面，兼容性好、生态完备著称。

**开源框架具有强大的规模效应。**人工智能开源软件框架生态的核心，是通过使用者和贡献者之间的良好互动和规模化效应，形成现实意义的标准体系和产业生态。绝大多数的深度学习框架的组件都有共通之处，由类似的技术堆栈组成。


**图表：不同深度学习软件框架技术栈对比**


![图片](https://image.cubox.pro/article/2022050622112689744/84338.jpg)


资料来源：CSDN，中金公司研究部


基于开源框架的模型训练具体是做什么？------调节超参数，生成参数

参数是由数据驱动调整：神经网络训练的最终目标是找到一套合适的模型参数（简单理解为神经网络权重W和bias b），但参数是在训练过程中自动更新生成的。

超参数由训练者人工输入并调整优化，超参数是训练过程中的"调节旋转"，用于控制模型的结构、效率，常见超参数有：Learning rate（学习率）、Num of iterations（迭代次数）、Num of hidden layers（隐层数目）


**图表：深度学习模型优化流程**


![图片](https://image.cubox.pro/article/2022050622112687115/25196.jpg)


资料来源：CSDN，中金公司研究部


**图表：深度学习模型典型超参数**

![图片](https://image.cubox.pro/article/2022050622112695668/33357.jpg)

资料来源：CSDN，中金公司研究部


**算力层：自研芯片+自建AIDC是破局英伟达CUDA生态极佳路径**


**AI作为一门新学科，其所需要的算力不同于传统信息科学，需要从芯片、AIDC、传感器到算法的深度改造和适配。**AI大模型训练成本非常高，以GTP-3为例，训练一次的成本达到约1200万美元。AI超算中心训练大模型时采用超大规模、并行计算的异构算力，技术难度非常高，往往需要自研AI芯片、AI传感器、训练框架和算法，并进行深度的适配和优化以减小能耗。

芯片层面：英伟达凭借CUDA生态占据产业链核心位置

**AI训练环节，GPU是目前主流；推断环节，ASIC是主流。**

►CPU：擅长统领全局等复杂操作，但不擅长处理并行计算场景。

►GPU：GPU提供多核并行计算能力，核心数量非常多，拥有更高的浮点运算能力和访存速度，相比CPU等硬件能大幅提升训练速度。

►ASIC：特定用途集成电路，在控制功耗的同时不断提升专用计算能力，对AI芯片进行定制，在特定场景下实现芯片的高性能和低功耗。

►FPGA：现场可编程门阵列，集成大量基本门电路及存储器，主要特点是可编程。灵活性高，性能/功耗适中。


**图表：人工智能芯片应用情况**


![图片](https://image.cubox.pro/article/2022050622112665451/77320.jpg)


资料来源：商汤科技公众号，中金公司研究部


**英伟达在人工智能产业生态中提供关键底层软硬件能力。**以cuDNN为例，它是NVIDIA深度神经网络软件开发包中的一种加速库，为深度神经网络中的卷积、池化、归一化和激活函数层等标准流程提供高度优化的实现方式。大部分的深度学习框架都支持cuDNN加速工具，包括Caffe、TensorFlow、PyTorch等。

**CUDA生态是英伟达主要壁垒。**英伟达从2007年开始前瞻转型HPC（高性能计算），在10年时间内大力投入统一计算设备架构CUDA生态。目前CUDA生态已形成丰富、完善的软件工具库，成为了人工智能开发者的首选，其生态是英伟达在AI领域主要的护城河。

超算层面：重资本构建厚城墙，ASIC云化是突破英伟达生态壁垒的极佳策略

**简单的硬件积木式堆叠效率低，AIDC建设技术难度高。**计算性能受制于高速互联网络，而数据拥堵会大大降低系统效率。此外，硬件的积木式堆叠的加速效果不会线性增长，会导致更低的系统效率，甚至出现多个GPU还不如单个GPU的情况，同时导致过大功耗，增加AIDC超算中心计算成本。

**AI超算中心是AI时代的新型数据中心，需要大量落地经验。**由于AI算法属于计算密集型算法，海量的数据处理任务和人工智能的训练和推理更适合GPU、FPGA、ASIC等并行计算能力强的芯片。超算中心中硬件的架构设计至关重要，需要考虑到网络、存储、电源、散热、管理和软件等多方面因素，以实现超低延迟和高带宽访问，建设过程需要大量行业实践经验。

**AI模型对算力需求的指数级增长是建造AI超算中心的重要原因。**2018年业界充分认识到大模型的威力后，模型的参数规模越来越大，对算力的需求也越来越大。2012年到2018年间，芯片的计算性能仅提升了30多倍，但从算法对AI模型所需的算力来看，AlexNeT到AlphaGo Zero，算法对算力的需求却提升了30万倍。因此，建造超大规模的人工智能超算中心是AI下一阶段发展的方向。


**图表：大规模预训练模型算力需求增长**

![图片](https://image.cubox.pro/article/2022050622112617732/99165.jpg)

资料来源：MEET 2021智能未来大会，中金公司研究部


**图表：全球AI超算进展梳理**

![图片](https://image.cubox.pro/article/2022050622112616561/20318.jpg)

资料来源：MEET 2021智能未来大会，中金公司研究部


**自建AIDC与算法有协同效应，AI公司自建算力中心或将成为趋势。**超算中心巨额的计算成本和技术难度一直是阻止AIDC建设的重要因素，而自建算力的情形下，可以提高算法和算力的适配程度，降低计算成本。

**由于超算中心对于训练大模型、降低算法成本至关重要，国内外巨头纷纷超前布局AI超算中心。**由于超算中心是计算大模型的关键，国内外巨头均在近几年大力投入AIDC基础设施。

**国内AI公司近年来大力投入AIDC，目前进展领先。**国内AI厂商重视AI基础设施，近年来大力投入AIDC建设，以商汤科技上海临港在建的AIDC为例，其包含两万块 GPU，能提供 3740 PFlops算力，是目前亚洲最大的AI超算中心。商汤AIDC完整训练一次GPT-3仅需一天，其目标是未来进行更大规模参数模型的训练。

**超算中心需配合自研训练框架才能充分释放效能，国内龙头框架技术全球领先。**深度学习框架需要基于底层异构芯片，支持超大规模模型的训练。目前的开源训练框架难以解决内存墙和显存墙的问题，并不适用超大模型的训练，因此需要自研训练框架。国内龙头企业自研框架如SenseParrots、MindSpore已经能完成高性能的分布式大型网络的并行训练，训练框架技术全球领先。

**自研芯片+自建超算，是国内ASIC突破英伟达CUDA生态壁垒的极佳策略。**寒武纪等国内ASIC厂商芯片相对于英伟达GPU在性能上并无劣势，英伟达的核心壁垒在于CUDA生态，但ASIC更高的定制化造成生态更难以形成。但通过自建超算、基于私有的软硬件极致优化ASIC，再以云服务的方式提供封装好的算力，则可以解决AISC通用性差的问题，是突破英伟达GPU生态壁垒的极佳战略。


**平台层：工业级的算法生产流程掀起AI算法的工业革命**


**平台层是支撑AI大规模训练生产、部署的技术体系，连接算力层和算法层。**机器学习平台指的是将机器学习从理论变成实际应用的一整套工程，最底层是训练框架，决定了AI模型的上限；其次是模型生产平台，影响AI模型生产的效率；第三层和第四层分别是推理部署框架和数据平台。如下图所示，深度学习模型平台一般分成四层：


**图表：深度学习模型平台**


![图片](https://image.cubox.pro/article/2022050622112697487/68726.jpg)


资料来源：华为官网，商汤科技官网，中金公司研究部


训练框架

**目前美国巨头在训练框架领域遥遥领先，国内仅有少数企业自研框架。** 由于AI人才储备远多于中国，且积累时间较早，美国在基础层与技术层的布局领先中国，目前美国谷歌与脸书两大互联网巨头分别拥有TensorFlow与PyTorch等行业主流框架，而国内仅有商汤、百度、华为、旷视等少数企业自研训练框架。  

**图表：GitHub收藏数和贡献率排名TOP16的框架（大部分属于外资巨头开发）**


![图片](https://image.cubox.pro/article/2022050622112656396/80789.jpg)


资料来源：GitHub，中金公司研究部

**相比于开源框架，自研底层框架能够实现更高的性能上限。**随着AI进入工业化时代，实现大规模的产业落地，海外的开源框架开始在很多方面出现局限性，包括在网络结构、设备兼容、性能与功耗均衡和各种自动化设计等。国内商汤、百度、华为等龙头公司自研的框架在设计之初就考虑到大模型训练的内存墙和显存墙等问题，能实现更高的性能上限。

**自主框架能适应AI的产业落地，具备自主性和协同效应。**虽然主流的开源框架利用先发优势，生态建设方面遥遥领先，但这类框架所提供的能力无法契合AI大规模落地的产业需求，因为这类开源框架更多是面向普及应用的。但产业界需要工业级的模型生产平台，自主框架能为AI技术赋能百业提供有力的支撑。

**在算法框架的自主性和先进性支撑下，自研框架能为大量企业提效赋能。**以商汤和百度为例，商汤SenseParrots框架的 GPU集群的并行处理效率损失仅为10%左右，计算速度提升接近900倍，远强于开源框架的方案。


**图表：主流开源框架对比（大部分开源框架设计时未充分考虑多GPU支持能力）**

![图片](https://image.cubox.pro/article/2022050622112698787/13960.jpg)

资料来源：DEVOPEDIA，中金公司研究部


模型生产平台

**基于训练框架，模型平台可大幅优化数据流程和模型生产流程的耗时，实现AI算法大规模生产。**在AI落地工程中，通常数据流程和模型生产流程占工程耗时占比最长。而模型生产平台能通过标准数据接口、数据增强技术、数据前处理技术等技术提高AI落地效率。

**国内龙头模型生产平台推出时间晚，但架构更完善，起点高，在部分技术领域做到全球领先。**国内的开源框架由于开源时间晚，虽然生态上有劣势，但是在搭建时考虑未来需求，架构更完善，起点更高。

推理部署框架

**推理部署指把已经训练好的模型放在云端或设备端并运行。**当AI模型经过复杂的训练之后，往往需要较大的计算性能，只能部署在云端的计算中心。也可以将模型压缩成很小的模型，直接部署在终端设备进行推理。硬件方面，目前云端推断主要使用云服务器+FPGA芯片代替GPU和CPU，谷歌使用自研的ASIC芯片为AI训练和推理同时提供服务。设备端推断则一般通过芯片的高度定制化以实现低功耗。

**如同训练框架，推理部署框架的生态同样是核心竞争要素之一。**为开发者提供足够友好、易用的工具环境，让其迅速获取深度学习加速算力，从而降低深度学习模型研发和训练加速的成本和研发周期，是推理部署框架的核心竞争要素，国外开源框架如英伟达TensorRT和英特尔OpenVINO形成较强生态，竞争较为激烈。


**图表：目前推理部署框架主要部署在设备端和云端**


![图片](https://image.cubox.pro/article/2022050622112688996/74875.jpg)


资料来源：雷锋网，中金公司研究部


**深度学习推理部署框架需求分散，研发门槛相对较低，自研厂商相对较多。**不同于深度学习模型训练具备相对明确的需求，推理框架的需求相对分散，主要有三个，精度需求、易用需求、性能需求。由于几乎不可能出现一个框架同时在三个需求上做到极致，如TensorRT 和 OpenVINO 虽然性能好，但是支持的平台单一，进行模型转换存在精度和掉点的问题。因此我们认为，推理部署框架的竞争格局会相对较为分散。

**国内部分厂商自研推理框架性能卓越。**国内目前的云端推理框架基本都依赖对TensorRT和OpenVINO的集成，芯片厂商在AI业务落地过程中会提供计算库，使用芯片厂商计算库导致AI业务落地的全链条端到端的效率低，在计算性能、内存占用和功耗等方面离业界差距较大。而国内AI龙头自研推理框架推出时间点晚，充分考虑推理框架的各种需求，且不受海外芯片厂商计算库限制，还具备明显性能优势，如商汤SensePPL在多种AI视觉模型的推理当中，SensePPL 的端到端性能超过了TensorRT和OpenVINO，在各类终端上加速比都大幅超越CAFFE推理框架。


**图表：SensePPL相对于CAFFE的比较加速比**


![图片](https://image.cubox.pro/article/2022050622112639113/34844.jpg)


资料来源：CSDN，中金公司研究部


**未来大模型时代，AI进入工业化生产，自动化是提高部署环节的核心重点。**工业化生产AI模型的时代，头部AI公司如商汤每天能产出数百个AI模型，不可能全部靠人来手工调试进行模型部署，背后必须要有一套自动化系统。

**AI算法的工业化生产需要一套工具，提供工业级的模型量化效率。**为满足工业级的量化需求，国内AI龙头商汤提出一套量化生产工具，将不同类型的网络模型输入这套生产工具，就可以得到适应AI部署的模型。


**图表：量化流水线提供高效率、低成本的系统层面优化部署方式**


![图片](https://image.cubox.pro/article/2022050622112627722/29834.jpg)


资料来源：GTIC 2021，商汤科技，中金公司研究部


**通过软硬件的协同，龙头AI企业铸就效率护城河。**率先进入工业化AI生产模式的龙头企业，能率先进行大量网络结构、大量推理库和各类芯片的优化和适配，结合丰富的AI落地场景，能建立起一个规模庞大的数据库来优化三者的匹配，并且持续迭代和完善。我们认为龙头AI企业如商汤、华为、百度能领先于同类企业建立起这种壁垒，反过来优化模型的实际生产，铸就效率护城河。

**经验沉淀建立的效率护城河带来模型的高效率开发。**以商汤科技为例，目前该公司已沉淀训练过的超过20万个模型和10万个不同的网络结构，同时支持11种硬件，当面对新的网络结构时，可以通过查询该数据库高效地进行硬件和框架的适配，大大加速了进入新场景时模型的开发速度。

数据平台

**数据平台主要目的是将数据处理过程中与AI关键算法相关性较弱的步骤流程化，大大提升开发者的效率。**数据平台主要任务主要包括数据存储、数据标注、数据加密等，其与算法训练框架、模型工厂形成一个闭环的体系，让数据发挥出更大价值。

**数据平台提供数据的标注和自动训练，打造模型工业化生产的闭环。**数据流程包括数据采集、数据标注、模型训练、模型评估，是AI项目中最为耗时的流程，而在很多智能化场景中，新场景和新应用的需求不断变化，因此必须将数据流程自动化从而提高模型生产效率。


**图表：数据处理相关工作是AI落地的难点和痛点**


![图片](https://image.cubox.pro/article/2022050622112631631/40579.jpg)


资料来源：第四范式公众号，中金公司研究部


**算法层：海量模型解决项目标准化问题**


**算法层基于平台层之上，包括丰富的算法模型工具箱。**算法层是AI企业公司在科研和产业经验中的先进算法模型在平台上的沉淀，AI企业使用丰富的算法工具为各行各业赋能，通过模块化的方式提高部署效率。

**算法层模块化组装解决解决长尾场景，大大提升客户付费意愿。**除了高频应用场景的算法外，算法工具箱通常也包含低频和长尾场景的算法，足够大的算法工具链积累能大幅降低扶梯逆行、城市火灾和垃圾检测等长尾场景的解决难度，提升客户的付费意愿。

**算法模型规模效应很强，大模型压缩出海量小模型，结合小模型的模块化组装，大大降低算法的边际成本。**更强的通用性算法可以快速压缩出海量的模型，快速帮助企业在各个下游场景将AI落地，更多的模型对场景的覆盖也会更广，企业受益于更多的经验，能锻造出更好的模型。


**三、大模型是解决碎片化问题的突破点**


**大模型是AI行业重要的产业趋势**


**生物智能发展是量变到质变的过程，意味着AI的发展也是模型参数、连接数的不断增加的过程，即具备通用性的人工智能必然需要大模型、大算力的支持。** 我们在上篇系列报告《[人工智能十年展望(一)∶底层模拟人脑，算力决定上限](http://mp.weixin.qq.com/s?__biz=MzI3MDMzMjg0MA==&mid=2247542372&idx=3&sn=ec3f080571005b59ce145da82aed6a11&chksm=ead0c0a3dda749b5857879f27f8018c877c823d31c7a4fae1759142836c452dbb95b3c1f2e69&scene=21#wechat_redirect)》提到，低级的动物智能进化到人的智能的过程中，并没有明显的分界线，从具备初级智能的寒武纪时代生物线虫（302个神经元，7000个突触）到人脑（10\^11个神经元、10\^15个突触），智能本质上是神经元个数、连接数的量变到质变的过程。

**早年业界一度认为小模型、小算力是方向，但行业落地实践证明其短板明显。**2012年AI商业以来，业内外对于大规模参数的通用模型一直存在质疑，因为此前的共识是精妙的算法是未来方向，即通过更优质的算法和更高的模型精度，减少对算力的需求。精妙模型路线无法低成本的解决大量的长尾场景，以AI市场中份额占比接近一半的智慧城市为例，长尾场景是客户商业价值闭环的关键部分，小模型的生产方式无法赋能千行百业。


**图表：智慧城市占人工智能市场行业近一半份额（2020年）**


![图片](https://image.cubox.pro/article/2022050622112728129/22020.jpg)


资料来源：艾瑞咨询，中金公司研究部


**受限于开源训练框架和底层基础设施，早年业界对超大规模参数预训练模型的实现质疑较多。**2020年参数量达到1750亿的GPT-3被推出之前，业界都对大模型路线抱有质疑，因为训练超大规模参数模型的技术难度和成本让绝大部分人望而却步，只有如OpenAI这样一直对超大模型抱有坚定信仰和情怀，背后又有微软、特斯拉等巨头的财力支持的机构才敢于探索超大模型训练的可能性。

**复盘产业发展，深度学习模型所需算力的指数级增加，背后原因是通用性带来的参数指数级增加。**以2012年到2018年最先进的算法所需算力为标准，从AlexNeT到AlphaGo Zero，算法对算力的需求提升了30万倍。这种指数级增长的背后原因是大模型对算力的需求指数级上升。

**2018年后，超大规模参数模型的趋势明显加速。**观察当前国内外AI巨头在超大规模预训练模型的探索，可以看出人工智能行业未来的发展趋势，是在输出方向大致确定的情况下，通过更大的模型和更多的数据，"大力出奇迹"地撞出结果。


**图表：大规模预训练模型对于算力需求呈指数增长**


![图片](https://image.cubox.pro/article/2022050622112650706/59932.jpg)


资料来源：MEET 2021智能未来大会，中金公司研究部


**从技术演进看大模型是实现通用AI的重要方向。**过去的人工智能公司倾向于选择人效高的场景先落地人工智能。但近几年在长尾场景等问题导致了对更通用的人工智能的刚需，在国内外巨头纷纷投入大量资源攻克通用人工智能难题的推动下，一些通用的语言模型、视觉模型甚至多模态模型也开始逐渐取得突破。


**大模型技术、资本壁垒高，是AI巨头之间的博弈**


大模型时代，应用层公司无力进行底层技术的探索

**超大规模参数模型技术探索所需技术、资本壁垒高，是巨头之间的战场。**在AI模型参数量疯狂增长的背后，人工智能的竞争已经成为大公司之间的角逐，GPU集群的使用成本极高，GPT-3训练一次的成本达到上千万美金，且GPU集群并行计算的工程难度非常高，大部分公司没有财力支持这样的训练和研发。

**除了资本壁垒，GPU集群计算的技术难度较高，存在内存墙等典型技术挑战。**由于模型参数规模很大，往往只能采用分布式的策略将训练扩展到多个GPU，但芯片间的内存墙严重影响GPU集群的训练效果。根据伯克利RISELab统计，过去20年硬件峰值计算能力增加了9万倍，但内存/硬件互连带宽只提高了30倍。

**训练框架是大模型时代的另一难点，现有开源框架无法直接训练超大模型。**为了训练GPT-3，OpenAI实验室在对开源框架做了深度定制和优化，但自研分布式训练框架的难度高，基于开源框架的深度优化和定制也存在较高门槛，大部分AI公司较难实现突破。

自主底层框架能力、算法效率及成本是实现大模型关键

**大模型训练过程面临大量的技术挑战。**在大模型的分布式训练过程中，大模型和大数据来带的显存墙和内存墙会影响训练，因此必须使用大规模训练技术。但大规模训练技术同样面临诸多困难，包括：

**►显存问题：**模型训练的显存占用峰值很高，可能会触碰显存墙而导致训练停止；

**►通信问题：**深度学习迭代式训练需要大量的局部数据交换，但网络的传输速率远不能匹配 GPU 或 TPU 等专用加速芯片的运算速率，将导致训练速度变低。

**►计算挑战：**各项技术的应用降低了计算效率，要采用合适的分布式架构，才能保证多节点数量下的加速比。


**图表：ResNeSt269训练时的显存墙问题**


![图片](https://image.cubox.pro/article/2022050622112668201/24144.jpg)


资料来源：商汤科技公众号，中金公司研究部


**图表：扩大带宽将导致带宽利用率将持续降低**


![图片](https://image.cubox.pro/article/2022050622112724307/10352.jpg)


资料来源：商汤科技公众号，中金公司研究部


**底层自研框架是实现大模型关键，开源框架所提供支撑不足。**训练框架是实现大模型的关键，因为训练框架的空间决定了深度学习发展的天花板，而训练框架的能力决定了深度学习发展的速度。目前海外开源框架在网络结构、设备兼容、性能与功耗均衡和各种自动化设计等均出现局限性，无法契合行业需求。由于大部分国内AI厂商做应用层面技术，而较少涉足基础AI技术层面，选择自研训练框架的厂商很少，拥有开源框架的就更少，因此国内AI厂商自研的开源框架非常稀缺。

**大模型训练需要极强算力支持、成本高昂，仅少数头部机构布局，且需要底层技术投入降低成本来支持。**大模型的训练需要极强的算力支撑，以微软与OpenAI合作建造的Azure人工智能算力平台为例，该算力平台投资约10亿美元，使用该超算中心训练一次超大模型GPT-3大约花费1200万美元。伴随大模型和大数据而来的，是对新一波以算力为核心的AI基础设施的需求，需要从AI全栈技术的各个层面去优化性能。


**基于底层大装置+大模型，AI有望向基础软件分化格局演变**


大模型+小模型是降低算法边际成本、解决碎片化痛点的关键架构

**基于具备一定通用性的大模型，通过少量的增量训练蒸馏出小模型，是解决长尾问题的关键技术架构。**从模型训练到部署，需要通过剪枝、量化、蒸馏等模型压缩技术实现更高的经济性及快速推理。以蒸馏为例，可以将结构复杂、参数规模庞大的大模型，压缩成结构简单、易于部署的小模型，相比于直接生产的小模型，大模型蒸馏出的小模型具有更强的泛化能力。


**图表：蒸馏技术是类似于老师-学生传递知识的过程**


![图片](https://image.cubox.pro/article/2022050622112773898/41065.jpg)


资料来源：ICCV2019，华为云，中金公司研究部


**大模型+小模型的方式能有效降低AI落地边际成本。**通过对大模型进行量化、剪枝、知识蒸馏等模型压缩方法把大模型变小，高效的进行模型生产，能够覆盖各种长尾场景，且大大降低了复制成本。

**从满足客户需求的角度看，大模型+小模型的应用是行业必然趋势。**只有解决了长尾场景的各种痛点需求，客户对解决方案买单的意愿才会显著提高。大模型是解决长尾场景有效办法，因此我们认为，大模型是未来AI行业的必然趋势。

AI底层基础软件或将呈现分化格局，AI赋能百业指日可待

**通过大模型+小模型模式解决边际成本和碎片化问题，实现AI赋能百业。**以一个亿分之一级别精度的行人识别模型为例，2015年时，商汤需要10个研究员花6个月的时间开发，而现在只需要1个研究员用3天时间就可以达到同样的效果，GPU资源使用量降到原来的一半，边际成本大幅下降。


**图表：商汤科技AI赋能百业实现方式**


![图片](https://image.cubox.pro/article/2022050622112762758/47853.jpg)


资料来源：艾瑞咨询，英特尔，商汤科技，中金公司研究部


**相关链接**


[中金 \| AI十年展望（一）：底层模拟人脑，算力决定上限](http://mp.weixin.qq.com/s?__biz=MzI3MDMzMjg0MA==&mid=2247542372&idx=3&sn=ec3f080571005b59ce145da82aed6a11&chksm=ead0c0a3dda749b5857879f27f8018c877c823d31c7a4fae1759142836c452dbb95b3c1f2e69&scene=21#wechat_redirect)
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


**文章来源**


本文摘自：2021年8月25日已经发布的《人工智能十年展望（二）：边际成本决定竞争力，算法龙头主导格局优化》

陈星宇 SAC 执业证书编号：S0080121020020

于钟海 SAC 执业证书编号：S0080518070011 SFC CE Ref：BOP246

魏鹳霏 SAC 执业证书编号：S0080121070252


**法律声明**


向上滑动参见完整法律声明及二维码


![图片](https://image.cubox.pro/article/2022050622112756530/92513.jpg)


[跳转到 Cubox 查看](https://cubox.pro/my/card?id=7037665074323915182)
