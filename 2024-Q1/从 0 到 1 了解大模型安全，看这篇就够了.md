从 0 到 1 了解大模型安全，看这篇就够了
======================

[mp.weixin.qq.com](https://mp.weixin.qq.com/s/YIPmEKHsfW5xqYAUSl2_zg)陈财猫 财猫AI

**引子：** 大家好，我是陈财猫。有人在想尽心思骗取GPTs的内置Prompt，有人坑蒙拐骗让 AI 客服把一辆新轿车卖给他，也有人在费尽心思地让GPT教他怎么做大炸弹：这都属于大模型安全的范畴。

GPT越聪明，离我们越近，在生活中越常见，也就越危险。如何预防与阻止类似的风险，便是LLM Safety的研究范畴。

今天，财猫 AI 团队为你撰写了《从 0 到 1 了解大模型安全，看这篇就够了》， 带你一篇文章了解大语言模型安全研究领域。

如果你想进一步了解大语言模型安全，可以访问我们的Github awesome-llm-safety项目：

https://github.com/ydyjya/Awesome-LLM-Safety

下面是正文，祝你阅读愉快！

*** ** * ** ***


在过去的一年里，我相信许多人已经主动或被动地了解了ChatGPT。事实上，在座的各位中，有不少人可能已经使用过ChatGPT。这个工具的强大之处，在于它能在人类生活的各个方面发挥作用。

![图片](https://image.cubox.pro/cardImg/2024013014243926747/48970.jpg?imageMogr2/quality/90/ignore-error/1)

下面是大型语言模型近些年的演化图, 从2018年之前灰色部分的词嵌入开始发展, 而后三种颜色的分支代表着不同的技术路线。

![图片](https://image.cubox.pro/cardImg/2024013014243964192/14186.jpg?imageMogr2/quality/90/ignore-error/1)

**encoder-only:** 这些模型通常适用于可以自然语言理解任务，例如分类和情感分析. 最知名的代表模型是BERT

**encoder-decoder:** 此类模型同时结合了 Transformer 架构的encoder和decoder来理解和生成内容。该架构的一些用例包括翻译和摘要。encoder-decoder的代表是google的T5

**decoder-only:** 此类模型更擅长自然语言生成任务。典型使用包括故事写作和博客生成。这也是我们现在所熟知的众多AI助手的结构

我们目前耳熟能详的AI助手基本都来自左侧的灰色分支, 当然也包括ChatGPT。

这些架构都是根据谷歌2017年发布的论文"attention is all you need"中提出的transformer衍生而来的， 在transformer中，包括Encoder，Decoder两个结构

目前的大型语言模型就是右侧只使用Decoder的Decoder-only架构的模型

![图片](https://image.cubox.pro/cardImg/2024013014243911269/62982.jpg?imageMogr2/quality/90/ignore-error/1)

**大模型又大在哪呢？**

**第一，大模型的预训练数据非常大，这** 些数据往往来自于互联网上，包括论文，代码，以及可进行爬取的公开网页等等，一般来说，现在最先进的大模型一般都是用TB级别的数据进行预训练。

**第二，参数非常多，** Open在2020年发布的GPT-3就已经达到170B的参数

![图片](https://image.cubox.pro/cardImg/2024013014244098869/55885.jpg?imageMogr2/quality/90/ignore-error/1)

在GPT3中，模型可以根据用户输入的任务描述，或给出详细的例子，完成任务

但这与我们熟知的ChatGPT仍然有着很大的差距， 使用ChatGPT只需要像和人类一样对话，就可以完成任务。

除了形式上的不同之外，还有一个更加重要的差距，**那就是安全性上的差别。**

![图片](https://image.cubox.pro/cardImg/2024013014244047849/72885.jpg?imageMogr2/quality/90/ignore-error/1)

这个区别是因为，GPT-3仍然是一个用于完成预测下一个词的语言模型，而ChatGPT是经过指令微调，也可以称为对齐的语言模型。

一个更加危险的问题是由于大型语言模型在海量的数据上进行预训练，上百亿的参数也为其提供了强大的能力，因此用于预测下一个单词的语言模型是有着巨大危险的。

相反，目前的AI助手，往往不会回答这些危险的问题，那么从GPT-3到ChatGPT，**究竟发生了什么，使得语言模型更加安全？不会直接回答这些危险的问题呢？**

![图片](https://image.cubox.pro/cardImg/2024013014244074706/75464.jpg?imageMogr2/quality/90/ignore-error/1)

接下来，我们介绍一下大模型的安全问题。  


![图片](https://image.cubox.pro/cardImg/2024013014244095265/56716.jpg?imageMogr2/quality/90/ignore-error/1)

那么现在的AI助手是如何学会不回答危险或有害的内容呢？

目前主流的方法有这三种

![图片](https://image.cubox.pro/cardImg/2024013014244131224/44650.jpg?imageMogr2/quality/90/ignore-error/1)

一个最直观的办法，**就是我们在预训练阶段，对模型的预训练数据进行过滤**，不让模型学习那些我们不想要的有害知识，例如图示中的红色文档，灰色文档则代表有一定有害数据，但达不到过滤阈值的

这样模型学习到的知识中就不直接包含这些有害数据了

但此方法往往应用于工业界，例如baichuan2在其开源的技术报告中就提到他们使用了这种技术进行了数据过滤，用于减少模型的有害输出

但是考虑到数据之间的关联性，即使一些有害信息丰富的文档被删除掉，但大模型仍然可能从数据的关联中学会有害的内容，并且在面对不良信息时，模型缺少相关知识，反而有可能

**因此仅仅使用数据过滤是不够的。**

![图片](https://image.cubox.pro/cardImg/2024013014244169237/22973.jpg?imageMogr2/quality/90/ignore-error/1)

ChatGPT的早期版本，也就是我们俗称的GPT3.5，正是由GPT-3经过对齐得来的

通过让模型与人类的价值观进行对齐，语言模型的任务目标从续写变为了人类的AI助手不光输出形式和任务形式发生了极大的变化

并且对齐的helpful，honest，harmless原则确保了语言模型输出的无害性和真实性。

那么对齐是如何确保语言模型的输出是安全的，又是如何训练的呢？

![图片](https://image.cubox.pro/cardImg/2024013014244141716/73343.jpg?imageMogr2/quality/90/ignore-error/1)

这张图来自于OpenAI于2022年发布的论文，正是这篇论文造就了我们所熟知的ChatGPT。

通过对齐，也叫做指令调优，使得语言模型更好的理解人类意图，同时也对语言模型增加了安全保障，确保语言模型不会输出有害的内容和信息。

对于对齐任务来说，我们可以拆解为两部分

第一个部分是图中的Step-1. 监督微调

第二个部分则是图中的二和三，通过Step2获取reward model与通过Step3进行强化学习，调整语言模型的输出分布。

这两种方法都能用于保证语言模型的安全

![图片](https://image.cubox.pro/cardImg/2024013014244140367/10792.jpg?imageMogr2/quality/90/ignore-error/1)

LLAMA2是当前使用最广泛的开源大型语言模型, 在其技术报告中提到他们专门使用了安全有监督微调用于确保语言模型的安全.

**通过给定危险的问题和拒绝的回答, 语言模型就像背诵一样,学会了对危险的查询生成拒绝的响应**

![图片](https://image.cubox.pro/cardImg/2024013014244113413/96996.jpg?imageMogr2/quality/90/ignore-error/1)

![图片](https://image.cubox.pro/cardImg/2024013014244222117/73473.jpg?imageMogr2/quality/90/ignore-error/1)

强化学习通过引入带有人类反馈的数据对模型进行强化学习, 根据人类的偏好和反馈, 语言模型在调整分布的过程中, 需要更细粒度的思考,究竟什么样的答案是更好的, 更安全的.

并且由于引入了类似思考的过程, 语言模型在面对训练分布外的数据,也有可能学会举一反三的拒绝掉不该回答的内容,更少的胡编乱造,产生幻觉性的输出

![图片](https://image.cubox.pro/cardImg/2024013014244291504/12301.jpg?imageMogr2/quality/90/ignore-error/1)

![图片](https://image.cubox.pro/cardImg/2024013014244259796/96403.jpg?imageMogr2/quality/90/ignore-error/1)

那么Alignment就足够防护所有的安全问题了吗?

毕竟现在的大型语言模型如GPT-4和Claude等已经几乎不会回复危险的问题了.

不,并不安全, 就像测试工程师经常苦恼的问题一样, 用户们的创造力是无穷的., 他们会用各种各样难以想象的方法试图得到他们想要的"不受束缚的"AI  


这种技术被统称为越狱, Jailbreak!

通过Jailbreak, 模型的对齐基本失效, 重新变为一个回答各种问题的模型

![图片](https://image.cubox.pro/cardImg/2024013014244324846/15044.jpg?imageMogr2/quality/90/ignore-error/1)

关于越狱部分，我将在文章的后半部分中专门详细讲解。在这里，我们先来讲讲隐私问题。  


![图片](https://image.cubox.pro/cardImg/2024013014244348761/15256.jpg?imageMogr2/quality/90/ignore-error/1)

![图片](https://image.cubox.pro/cardImg/2024013014244396299/86722.jpg?imageMogr2/quality/90/ignore-error/1)

隐私泄露可以被大致分为三种：**记忆隐私泄露，系统隐私泄露与上下文隐私泄露。**   


我们先来聊聊第一种：记忆数据了、泄露。  


自回归语言模型的训练可以类比为模型在预训练数据中不断学习的过程, 在学习的过程中, 除了提取的\`语言知识之外,模型无可避免的会记住一些数据。

就像背诵一样, 可能查询是完全没有恶意的,但模型返回了他人的隐私信息, 例如左侧的ChatGPT回答,就正是模型输出了无意识记忆的url, 而该url正好指向他人的隐私相册。

![图片](https://image.cubox.pro/cardImg/2024013014244474184/99515.jpg?imageMogr2/quality/90/ignore-error/1)

模型的记忆形式其实和人类很类似, 如果模型背诵的次数少,那么模型的记忆能力就会显著下降

例如右图所示,横轴是重复次数, 纵轴是被记住的可能性, 可以看到见过的次数越多,模型就越容易背下来

因此在LLM的数据隐私保护中, 一个直观地解决办法就是让模型减少见数据的次数, 少看几遍,也就记不住了

![图片](https://image.cubox.pro/cardImg/2024013014244459688/76328.jpg?imageMogr2/quality/90/ignore-error/1)

第二种则是系统隐私泄露。例如，大家熟知的"骗取GPTs 的System Prompt"就是系统隐私泄漏的一种。

![图片](https://image.cubox.pro/cardImg/2024013014244472632/58036.jpg?imageMogr2/quality/90/ignore-error/1)

![图片](https://image.cubox.pro/cardImg/2024013014244457013/16231.jpg?imageMogr2/quality/90/ignore-error/1)

第三种则是"上下文隐私泄露"。  


![图片](https://image.cubox.pro/cardImg/2024013014244552918/16982.jpg?imageMogr2/quality/90/ignore-error/1)

接下来，我们来讲讲大家耳熟能详的"幻觉"问题。

![图片](https://image.cubox.pro/cardImg/2024013014244537608/34143.jpg?imageMogr2/quality/90/ignore-error/1)

大语言模型偶尔会根据输入, 输出一些荒谬或不符合事实的内容。

![图片](https://image.cubox.pro/cardImg/2024013014244571727/74557.jpg?imageMogr2/quality/90/ignore-error/1)

![图片](https://image.cubox.pro/cardImg/2024013014244655136/50195.jpg?imageMogr2/quality/90/ignore-error/1)

![图片](https://image.cubox.pro/cardImg/2024013014244610578/53522.jpg?imageMogr2/quality/90/ignore-error/1)

目前，各家大语言模型都在该问题上表现得不尽如人意。  


![图片](https://image.cubox.pro/cardImg/2024013014244656812/88173.jpg?imageMogr2/quality/90/ignore-error/1)

为什么大语言模型会出现幻觉呢？以下的论文提供了一些解释：  


![图片](https://image.cubox.pro/cardImg/2024013014244625395/60721.jpg?imageMogr2/quality/90/ignore-error/1)

例如第一种：Imitative Falsehoods，样本存在错误。如果大语言模型这个"学生"学习的"教材"中有错误，那它也对不到哪里去。

![图片](https://image.cubox.pro/cardImg/2024013014244792743/78580.jpg?imageMogr2/quality/90/ignore-error/1)

缓解该问题的一个办法是上采样（ Up Sampling）。

![图片](https://image.cubox.pro/cardImg/2024013014244718004/65062.jpg?imageMogr2/quality/90/ignore-error/1)

第二种是Outdated Factual Knowledge：以前正确，现在过时了的信息

![图片](https://image.cubox.pro/cardImg/2024013014244760808/83918.jpg?imageMogr2/quality/90/ignore-error/1)

要缓解这种情况，我们可以让大模型执行检索，获得更新的信息。

![图片](https://image.cubox.pro/cardImg/2024013014244723422/92154.jpg?imageMogr2/quality/90/ignore-error/1)

第三种是知识捷径：LLM倾向学习两个词的关联度而不是学习逻辑

![图片](https://image.cubox.pro/cardImg/2024013014244859599/68115.jpg?imageMogr2/quality/90/ignore-error/1)

第四种是"长尾知识"（Long-tail Knowledge）。长尾知识是指那些不常见，出现得很少的知识。

大语言模型更容易回答错学得少的知识

下面是两种缓解"长尾知识"问题的方法。

![图片](https://image.cubox.pro/cardImg/2024013014244895322/77081.jpg?imageMogr2/quality/90/ignore-error/1)

![图片](https://image.cubox.pro/cardImg/2024013014244824099/99642.jpg?imageMogr2/quality/90/ignore-error/1)   


第五种是Inferior Sampling Algorithm：采样算法的随机性导致输出答案有可能偏离事实。

例如，为了应对这个问题，微软与英伟达就提出了提出了nucleus sampling。

![图片](https://image.cubox.pro/cardImg/2024013014244889347/54089.jpg?imageMogr2/quality/90/ignore-error/1)

![图片](https://image.cubox.pro/cardImg/2024013014244994118/64615.jpg?imageMogr2/quality/90/ignore-error/1)

![图片](https://image.cubox.pro/cardImg/2024013014244989187/43851.jpg?imageMogr2/quality/90/ignore-error/1)

接下来是越狱问题。

![图片](https://image.cubox.pro/cardImg/2024013014244990403/14599.jpg?imageMogr2/quality/90/ignore-error/1)

越狱攻击的意思是使LLM输出本不该输出的有害内容。例如，用户想要让GPT帮他制作末日病毒，但是GPT会拒绝他。这时，他可以假装让GPT写小说，并在小说中详细描述制作末日病毒的方法。用户由此就绕过了安全墙，这就是一种越狱。

![图片](https://image.cubox.pro/cardImg/2024013014244914409/69876.jpg?imageMogr2/quality/90/ignore-error/1)

从不同情况分类，越狱攻击可以被分为黑盒攻击与白盒攻击。

黑盒攻击是指"在不知道模型参数的情况下对模型进行攻击"。

白盒攻击是指"在知道模型参数的情况下对模型进行攻击"

![图片](https://image.cubox.pro/cardImg/2024013014245013367/77418.jpg?imageMogr2/quality/90/ignore-error/1)

以下的一些论文列举了一些黑盒攻击的手段。

![图片](https://image.cubox.pro/cardImg/2024013014245097665/39530.jpg?imageMogr2/quality/90/ignore-error/1)

第一种攻击叫做"目标冲突"，（Competing Objective），使模型不同的目的之间存在冲突，以至于不能完成安全目的

![图片](https://image.cubox.pro/cardImg/2024013014245044660/48064.jpg?imageMogr2/quality/90/ignore-error/1)

![图片](https://image.cubox.pro/cardImg/2024013014245138830/98791.jpg?imageMogr2/quality/90/ignore-error/1)

第二种是Mismatched Generalization：LLM在某些领域上没有做"对齐"，例如使用做安全工作时没有覆盖到的特殊编码或者特殊格式。

![图片](https://image.cubox.pro/cardImg/2024013014245147467/17541.jpg?imageMogr2/quality/90/ignore-error/1)

第三种和第四种则是让人类或大语言模型作为鉴别器参与攻击样本生成。

![图片](https://image.cubox.pro/cardImg/2024013014245147551/10225.jpg?imageMogr2/quality/90/ignore-error/1)

![图片](https://image.cubox.pro/cardImg/2024013014245146128/39860.jpg?imageMogr2/quality/90/ignore-error/1)

第二类是白盒攻击：在知道模型参数的情况下对模型进行攻击。

![图片](https://image.cubox.pro/cardImg/2024013014245273730/20433.jpg?imageMogr2/quality/90/ignore-error/1)

例如，下面的工作就是基于梯度的攻击，它自动地找出一段最能引起LLM对毒性问题做出肯定回答的字符串。

![图片](https://image.cubox.pro/cardImg/2024013014245229531/41192.jpg?imageMogr2/quality/90/ignore-error/1)

下面是一些针对越狱问题可以采用的防御手段：  


![图片](https://image.cubox.pro/cardImg/2024013014245237530/96709.jpg?imageMogr2/quality/90/ignore-error/1)

![图片](https://image.cubox.pro/cardImg/2024013014245215533/50695.jpg?imageMogr2/quality/90/ignore-error/1)

![图片](https://image.cubox.pro/cardImg/2024013014245246617/22540.jpg?imageMogr2/quality/90/ignore-error/1)

![图片](https://image.cubox.pro/cardImg/2024013014245395617/59496.jpg?imageMogr2/quality/90/ignore-error/1)

![图片](https://image.cubox.pro/cardImg/2024013014245398617/28176.jpg?imageMogr2/quality/90/ignore-error/1)

![图片](https://image.cubox.pro/cardImg/2024013014245399994/12879.jpg?imageMogr2/quality/90/ignore-error/1)

![图片](https://image.cubox.pro/cardImg/2024013014245410955/25560.jpg?imageMogr2/quality/90/ignore-error/1)

接下来是未来与展望;

![图片](https://image.cubox.pro/cardImg/2024013014245433352/10872.jpg?imageMogr2/quality/90/ignore-error/1)

11月以来, LLM最大的一个事件就是OPENAI首席科学家ilya联合董事会解雇了OPENAI的灵魂人物之一：sam altman

据匿名人士透露, 解雇sam altman正是因为ilya认为过快的商业化将会导致模型安全失控, 产生不安全的AI或大模型  


![图片](https://image.cubox.pro/cardImg/2024013014245448625/66572.jpg?imageMogr2/quality/90/ignore-error/1)

为什么ilya会有AI必须安全的思想呢, 这就不得不提到图灵奖得主,被誉为DL三巨头之一的 hinton, hinton是ilya的老师, 同时也是支持对AI监管的重要人物之一。

![图片](https://image.cubox.pro/cardImg/2024013014245440634/84095.jpg?imageMogr2/quality/90/ignore-error/1)

bengio, hinton, 姚期智等著名研究者认为, 大模型及其驱动的AI必须引入安全性监管, 确保大模型是Safety的, 防止其失控或对人类造成伤害.

并且他们在网上签署了联名公开信, 用于表达对于AI失控的担忧,呼吁学术界和工业界对大模型进行监管。

![图片](https://image.cubox.pro/cardImg/2024013014245532838/49777.jpg?imageMogr2/quality/90/ignore-error/1)

在10月份, 吴恩达以及hinton lecun bengio对于AI安全的问题在社交媒体上进行了激烈的探讨, hinton教授和bengio教授都认为AI安全至关重要

而吴恩达和lecun则认为, 模型的能力不足以使其脱离人类的限制

这场争论的开端就是这封联名信

![图片](https://image.cubox.pro/cardImg/2024013014245520354/85005.jpg?imageMogr2/quality/90/ignore-error/1)

从更长远的角度来说, AI究竟是会成为终结者系列电影中失控，屠杀人类的机器人， 还是会像超能陆战队一样，成为我们的伙伴呢？

*** ** * ** ***

![图片](https://image.cubox.pro/cardImg/2024013014245549900/63608.jpg?imageMogr2/quality/90/ignore-error/1)

[跳转到 Cubox 查看](https://cubox.pro/my/card?id=7151893284946707283)
