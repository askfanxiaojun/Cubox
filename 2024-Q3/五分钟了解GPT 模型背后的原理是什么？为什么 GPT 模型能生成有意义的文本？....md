五分钟了解GPT 模型背后的原理是什么？为什么 GPT 模型能生成有意义的文本？为什么 GPT 模型不会做简单的数学题？为什么有人担心 GPT 模型可能会危害人类？
==================================================================================

[blog.csdn.net](https://blog.csdn.net/qq_32727095/article/details/130078348)成就一亿技术人!

#### {#t0}五分钟了解[GPT](https://so.csdn.net/so/search?q=GPT&spm=1001.2101.3001.7020) 模型背后的原理是什么？为什么 GPT 模型能生成有意义的文本？为什么 GPT 模型不会做简单的数学题？为什么有人担心 GPT 模型可能会危害人类？

* 0. 导读
* 1. 为什么 GPT 模型能生成有意义的文本？
* 2. 为什么 GPT 模型不会做简单的数学题？
* 3. 为什么有人担心 GPT 模型可能会危害人类？
* 4. 小结

{#t1}0. 导读
----------

由于 GPT 模型的相关内容非常丰富，所以我计划对它进行更加深入的学习和研究，并把它应用到自己的工作、生活和学习中，用来提高工作效能，改善生活质量，提升学习效果。

按照第一性原理，在开始实战演练之前，我认为有必要先了解一下 GPT 模型背后的原理，这样才能避免盲目地崇拜它，也能避免无知地轻视它，而以更加理性的态度来应用它。

之前看到过一篇介绍 ChatGPT 原理的文章：ChatGPT 在做什么... 以及它为何发挥作用？全文超过 3 万字，包含 100 多张图片，并于 2023 年 3 月 9 日出版成书。

![](https://cubox.pro/c/filters:no_upscale()?imageUrl=https%3A%2F%2Fimg-blog.csdnimg.cn%2F5c065eb33b35406aac1a06c977e38ee5.png)

去查了一下原文的作者，发现是一个非常厉害的牛人，他就是数学软件 [Mathematica](https://so.csdn.net/so/search?q=Mathematica&spm=1001.2101.3001.7020) 的创始人------史蒂芬·沃尔夫勒姆（Stephen Wolfram），他还是著名的复杂科学家，研究神经网络超过 40 年，并且发明了 Wolfram 语言。

![](https://cubox.pro/c/filters:no_upscale()?imageUrl=https%3A%2F%2Fimg-blog.csdnimg.cn%2Fc21bb829e52141ceaff338ca1091ac1b.png)

下面结合沃尔夫勒姆的文章、谷歌团队的论文、ChatGPT 的回答、以及万维钢的 AI 前沿课等内容，抛开一些技术的细节，结合自己的理解，尽量用比较通俗的语言，来解读 GPT 模型背后的原理。

![](https://cubox.pro/c/filters:no_upscale()?imageUrl=https%3A%2F%2Fimg-blog.csdnimg.cn%2F10de6f2bd3ac474e86b4f39c42aa309b.png)

{#t2}1. 为什么 GPT 模型能生成有意义的文本？
----------------------------

GPT 模型本质上是基于大量的语言数据，对文本进行「合理的延续」，它的核心是「大[语言模型](https://so.csdn.net/so/search?q=%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B&spm=1001.2101.3001.7020)」（`LLM`）。

简单来说，GPT 模型的原理有点类似于玩「单词接龙」的游戏。
> 比如，把CSDN「写湿」的文章作为「学习材料」，用来训练 GPT 模型，当给它输入「我」字时，它可能会接着生成一个「是」字；紧接着，它会把「我」和「是」组合成「我是」，按照单词出现的概率，接着可能会生成下一个「写」字，再把「我是」和「写」组合成「我是写」，不断重复这个过程，就能生成一段有意义的文本，例如「我是写湿」。

![](https://cubox.pro/c/filters:no_upscale()?imageUrl=https%3A%2F%2Fimg-blog.csdnimg.cn%2Fce54f1a205ea46659fb4288ca54eef9c.png%23pic_center)

我们把上面这个过程称为「**自回归生成** 」，它属于一种无监督的自然语言处理（NLP）模型。有点类似于智能输入法，它可以根据用户的输入，在已输入词语的基础上，自动预测接下来可能需要输入的词语，以帮助用户提升打字的速度。
> 但是，如果 GPT 模型总是挑选概率最高的词，通常就会得到非常「普通」的回答（有时甚至是千篇一律的答案）。
>
> 然而，当 GPT 模型随机挑选概率相对较低的词时，就有可能得到「更有趣」的回答（有时甚至会让人感觉很有创意）。
>
> 所以，GPT 模型的回答并不是每次都一样，这让人感觉它更加智能。

但事实上，它目前并没有自主的意识，早期版本的 GPT 模型，甚至就如同「鹦鹉学舌」，甚至不理解自己到底说了什么。

GPT 模型的底层原理，其实是在谷歌团队提出的 Transformer 模型的基础上，建立一个庞大的神经网络，其突出特点是大数据、大模型和大计算。

其实说白了，就是「**大力出奇迹，暴力计算**」。

在经过大量数据的预训练和大量的计算之后，GPT 模型表现出了惊艳的语言理解和生成能力，可以有选择性地记住前文的重点，形成思维链推理能力。
> 因此，GPT 模型能够「理解」人类的意图，进行多轮有效的沟通，实现智能问答交流，还能模仿知名作家的写作风格，甚至能够完成诗歌的创作，做到内容完整、重点清晰、有概括、有逻辑、有条理。

{#t3}2. 为什么 GPT 模型不会做简单的数学题？
----------------------------

尽管 GPT 模型有很强的语言能力，但它对数学问题却还不大擅长。

比如，我随便输入一些数字，让 ChatGPT 做一道简单的算术题：

123123 ∗ 2080 + 321321 ∗ 8020 = ？ 123123\*2080+321321\*8020 =？ 123123∗2080+321321∗8020=？

结果 ChatGPT 一本正经地给出一个错误的答案： 2832402360 2832402360 2832402360，但中间有几位是错误的，正确答案应该是 2833090260 2833090260 2833090260。

![](https://cubox.pro/c/filters:no_upscale()?imageUrl=https%3A%2F%2Fimg-blog.csdnimg.cn%2F80f316f74c2f4142ac64f5e77847783c.png%23pic_center)

为什么 GPT 有强大的推理能力，却连这么简单的计算题都算错了呢？

其根本原因在于，GPT 是一个大语言模型，它的思维很像是人类的大脑，而人类的大脑是不太擅长计算这种数学题的，假如让你来计算，估计也要用笔算，或借助计算器等工具。

所以，GPT 其实更像是人类的大脑，而不是像一般的计算机程序。

据估计，人类的大脑大约有 1000 亿个神经元，而 GPT-4 的模型参数远超 1000 亿个，正是因为数量庞大，才产生了「**涌现**」的效果，也就是当数量大到一定程度，会突然出现一些原本并不具备的能力。就像蚂蚁的数量足够多之后，突然具备了某种组织能力。

{#t4}3. 为什么有人担心 GPT 模型可能会危害人类？
------------------------------

虽然 GPT 模型目前还不善于解决一些数学问题，但其实只要给它增加适当的插件，当遇到它不擅长的领域时，就运用多元思维模型，调用其他模型来解决。

比如，与 Wolfram 相结合，就能轻松解决一些数学问题，这就好比给人类配上计算器，算术能力就能得到明显增强。

![](https://cubox.pro/c/filters:no_upscale()?imageUrl=https%3A%2F%2Fimg-blog.csdnimg.cn%2Fd0fc65b717974cbcbc8df01748dbad43.png)

因为 GPT 模型本身属于一种无监督的算法，所以它就像一个黑匣子，常常会出现让人难以预料的结果，却不知道具体的原因，因此不免让人担忧：它会不会做出一些危害人类的事情来呢？

从历史经验来看，科技是一把双刃剑，用好了可以造福人类，用不好可能给人类造成灭顶之灾。
> 1905 年，爱因斯坦提出的质能方程，揭示了质量与能量之间的关系------即使是微小的质量变化，也会产生巨大的能量。
>
> 原子弹的基本原理，就是利用了质能方程。爱因斯坦曾向美国发出警告，指出德国正在进行原子研究，一旦德国研制成功，将会对世界构成严重的威胁。
>
> 1945 年，美国研制成功之后，在日本投下了 2 颗原子弹，造成超过 20 万人死亡，爆炸后释放了大量的核辐射，对人类产生长期的负面影响，导致癌症等健康问题，对生态环境造成巨大的损失，对人类安全造成严重的威胁。

因此，有很多人一直在呼吁：禁止核武器的使用和研发，以避免发生大的灾难。

![](https://cubox.pro/c/filters:no_upscale()?imageUrl=https%3A%2F%2Fimg-blog.csdnimg.cn%2F4cef4f39f91148e9b8a0c9f1e4e8ddcc.png)

2023 年 3 月底，美国未来生命研究所发布了一封公开信，呼吁人类暂停研究比 GPT-4 更强大的人工智能系统，为期至少 6 个月，以免 GPT 变得过于强大，给人类带来未知的危险。

这个未来生命研究所的发起人，就是著名的人工智能研究者迈克斯·泰格马克，也就是《生命 3.0》这本书的作者。

至于 GPT 未来到底会给人类造成什么样的影响，是收益更多，还是危害更大，目前恐怕还没人确切地知道。

我个人觉得，目前 GPT 还没有强大到威胁人类生存的地步，但是运用「六顶思考帽」思维模型，站在不同的角度去思考问题，提前想到潜在的风险，做好相应的预防措施，这对我们来说不是坏事。

我们还应该学会运用批判性思维，尽管 GPT 模型可以帮助我们提炼知识、总结经验和指导方法，但我们仍然需要自己来做出判断和决策，避免出现明显的逻辑错误，并对最终的结果负责。

{#t5}4. 小结
----------

最后，讲一个与 GPT 模型相关的故事。
> 据说在 2021 年，美国有一个叫约书亚的人，他的爱人杰西卡因病去世，因此他感到伤心欲绝。在一次偶然的机会，他把自己和爱人的所有聊天记录都上传到 GPT-3 模型中。
>
> 此后，他有空就和 GPT-3 聊天，结果神奇的事情发生了，他感觉到电脑屏幕对面就是杰西卡本人，因为聊天的很多细节都太像她了。
>
> 在聊天的过程中，约书亚经常泪流满面，哭累了就睡，睡醒了就继续聊。结果竟然治愈了约书亚，他不再像之前一样深陷其中、不能自拔了，最后他说：AI 复活了我的妻子，但我决定跟她说再见了。

这个故事给了我很大的启发，我觉得应该保持记录的习惯，多写一些复盘总结，并妥善保存好一些记录、照片、语音等，说不定将来就可以借助 GPT 模型，与过去的自己聊天。

当你记录的数据越多，GPT 模型就越准确，跟它聊天的感觉就越真实，将来也许可以成为情感的一种寄托，帮你舒缓情绪，治愈心灵，实现用数据赋能成长。

据说，国外有人把自己的日记导入到 GPT 模型中，训练了一个「童年的自己」，并向她提问、跟她对话，帮助自己理清内心的思绪，切实解决了自己遇到的问题。

GPT 模型的原理其实比较简单，但只有当数据到达一定的量级，量变才会引起质变。就像心理学家提出的 10000 小时定律，要想在某个领域达到专业水平，至少需要 10000 小时的刻意练习。

最后我相信，如果 GPT 模型运用得当，它将能帮助我们更好地发挥自身的潜能和创造力。

[跳转到 Cubox 查看](https://cubox.pro/my/card?id=7213180038559042367)
