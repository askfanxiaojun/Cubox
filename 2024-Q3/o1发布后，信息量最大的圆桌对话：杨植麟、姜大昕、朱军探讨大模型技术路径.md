o1发布后，信息量最大的圆桌对话：杨植麟、姜大昕、朱军探讨大模型技术路径
====================================

[mp.weixin.qq.com](https://mp.weixin.qq.com/s/nqEe8rNnED3-EGZwowPJBg)Founder Park Founder Park

![](https://cubox.pro/c/filters:no_upscale()?imageUrl=https%3A%2F%2Fmmbiz.qpic.cn%2Fsz_mmbiz_gif%2FqpAK9iaV2O3sAVsSPfCN9UX44XiaoicbUJIrOGuaujdMNY6iaQewDZEX1GY3tcVk3QGeKJyUMMHBSMALvO8B7DZwsA%2F640%3Fwx_fmt%3Dgif%26from%3Dappmsg)


在 2024 云栖大会上，阶跃星辰创始人姜大昕、月之暗面Kimi创始人杨植麟、生数科技首席科学家朱军与极客公园创始人张鹏一起，探讨了各自眼中 AI 技术发展的现状，推演未来 18 个月，大模型行业会发生什么。

在这场圆桌里，他们重点聊了：

* 客观来说，AI 领域过去两年发生了什么？

* OpenAI o1 的发布对行业意味着什么？

* o1 背后的强化学习新范式对算力和数据提出了怎样的新要求？

* AI 应用层的创业，在今天该怎么做？

* 未来 18 个月，AI 技术和应用的发展路径是什么？

信息量很大，我们将现场实录整理如下。

![](https://cubox.pro/c/filters:no_upscale()?imageUrl=https%3A%2F%2Fmmbiz.qpic.cn%2Fsz_mmbiz_jpg%2FqpAK9iaV2O3vWXb2bosCQtT4JJx4gzKl1ItUshLZIfLhnQNVdKJXv1x8QutfmT8SSBf7PcXKvSfXUkSMjcAUOoA%2F640%3Fwx_fmt%3Djpeg%26from%3Dappmsg)

![](https://image.cubox.pro/cardImg/49ptyqcouzc9s3xbvbkji7igwiujtno1uzwe4pm8wftkt6j5kq?imageMogr2/quality/90/ignore-error/1)

**Founder Park**

来自极客公园，专注与科技创业者聊「真问题」。

303篇原创内容

<br />

公众号   

，

点击关注，每天更新深度 AI 行业洞察

**01**
------

**AI 发展的速度太快了**
---------------

**张鹏：OpenAI 发布到现在快两年了，这两年里引发了整个世界对 AI 的讨论。各位都是下场创业做大模型的创业者，你们的感受是怎样的？**

**我们是在「看游戏」，你们在「打游戏」，感受可能会很不一样。过去 18 个月，AI 技术的发展在减速吗？**

**姜大昕**：我觉得过去 18 个月是在加速的，而且速度还是非常快的。

过去 18 个月里发生的大大小小的 AI 事件，我们可以从两个维度去看，数量和质量。

数量上，基本上每个月都有新模型、新产品、新应用涌现出来。单说模型，OpenAI 2 月发的 Sora，过年期间把大家轰炸了一下，然后 5 月出了 GPT-4o，上周又出了 o1。OpenAI 的老对手 Anthropic，它有 Claude 3、3.5 系列，再加上 Google Gemini 系列、 Groq、Llama......

![](https://cubox.pro/c/filters:no_upscale()?imageUrl=https%3A%2F%2Fmmbiz.qpic.cn%2Fsz_mmbiz_jpg%2FqpAK9iaV2O3vWXb2bosCQtT4JJx4gzKl1RS4SHJ0yWGgTbWiaRTnphQj2EiavGwibrlBNFez8OZnP8djxhfJWodviaQ%2F640%3Fwx_fmt%3Djpeg)

去年我们的体感还是 GPT-4 一家独大、遥遥领先，今年就变成了群雄并起、你追我赶的局面。所以各家肯定是在提速。

从质量的角度来看，我觉得有三件事情给我的印象非常深刻。

第一是 GPT-4o，在多模融合领域上了一个新的台阶。之前有视觉理解模型 GPT-4v；有视觉生成模型 DALL-E、Sora；有声音模型 Whisper、Voice Engine。4o 把孤立的模型能力融合在了一起。

为什么融合非常重要？因为我们的物理世界本身就是多模态，**多模融合有助于我们更好地为物理世界建模，更好地去模拟世界。**

第二是特斯拉的 FSD v12，一个端到端的大模型，它把感知信号直接变成控制序列。我觉得自动驾驶非常有代表性，它是一个从数字世界走向物理世界的真实的应用场景。FSD v12 的成功意义不仅在于自驾本身，可以说这套方法论为将来智能设备如何与大模型结合，如何更好地探索物理世界指明了方向。

第三就是上周的 o1，它第一次证明了语言模型也可以有人脑的慢思考，也就是系统 2\* 的能力。我们一直认为 AGI 的演进路线分为模拟世界、探索世界、归纳世界。而系统 2 的能力正是归纳世界的前提条件。

注：系统 1、系统 2 来自《思考，快与慢》，系统 1 指快速的、无意识的快思考；系统 2 指有意识的慢思考。

过去几个月的时间，GPT-4o、FSD v12 和 o1 分别在这三个方向上都取得了非常大的突破，而且为将来的发展也指明了方向。所以我觉得无论是从数量还是质量来说都是可圈可点。

**张鹏：感觉你在你期待的领域里都看到了广泛的突破和进展。那植麟的体感是怎么样的？投身其中的人可能会跟我们外边「看游戏」的人不一样。**

**杨植麟**：我也觉得整体是处于加速发展阶段，AI 发展的核心可以从两个维度来看。

第一是纵向的维度，智商是一直在提升的，体现上还是去看文本模型能做到多好；第二是横向的发展，除了文本模型之外，像刚才提到的多模态，这些模态其实是在做横向的发展，它让模型具备更多技能，能够完成更多任务，然后同时跟纵向的智商发展相结合。

![](https://cubox.pro/c/filters:no_upscale()?imageUrl=https%3A%2F%2Fmmbiz.qpic.cn%2Fsz_mmbiz_jpg%2FqpAK9iaV2O3vWXb2bosCQtT4JJx4gzKl1Y4v7qhicgNyqxXPQiauRXiatENzv3UaWGtXhbrdOOeeOIicByzQiccrll3A%2F640%3Fwx_fmt%3Djpeg)

在这两个维度上我都看到了非常大的进展。在纵向维度上，数学竞赛的能力去年是完全不及格，而今年已经能得到 90 多分了。代码也是一样，现在能击败很多专业编程选手。也产生了很多新的应用机会，比如说现在流行的 Cursor，能通过自然语言直接去写代码，未来这样的软件也会越来越普及。

很多具体的技术指标，比如现在的语言模型能支持的上下文长度，去年大部分模型都只能支持 4-8K 的上下文。但是今天 4-8K 已经是非常低了，128K 是标配，很多已经可以支持 1M 或者甚至 10M 的上下文长度，它其实也是智商不断提升的重要基础。

最近的很多进展不光只是在做 scaling，很多进展来自于后训练的算法优化、数据的优化，这些优化其实周期会更短，更短的优化周期也会导致整体的 AI 发展节奏进一步加快。

横向上也产生了很多新的突破。Sora 可能是影响力最大的，它完成了视频生成，最近也有特别多新的产品和技术出来，比如现在已经可以通过一篇论文，直接生成一段真假难辨的 Podcast 双人对话。未来类似这样不同模态之间的转化、交互和生成会变得越来越成熟。所以我觉得整体是在加速的过程中。

**张鹏：感觉这些技术还在加速地扩展，虽然可能没有长出 Super App，但如果抛掉 Super App 的视角，去看技术，反而能看到它真正的进展，这可能是更理性客观的视角。朱军老师，你会怎么总结这 18 个月？你觉得 AGI 的技术经历了什么样的发展？**

**朱军**：其实 AGI 里大家最关注的还是大模型，大模型方面从去年到今年发生了很多重要的变化，我非常同意整个进展在加快。另外大模型解题的速度也变快了，它的 learning curve（学习曲线）在变得更陡。

大语言模型从 2018 年到现在发展过来走了 6 年的路，去年下半年大家开始讨论多模态，到今年年初，只过了半年时间，多模态大模型的时空一致性就已经让大家震惊了。这种加速最核心的原因在于，大家对路线的认知和准备达到了比较好的程度。

![](https://cubox.pro/c/filters:no_upscale()?imageUrl=https%3A%2F%2Fmmbiz.qpic.cn%2Fsz_mmbiz_jpg%2FqpAK9iaV2O3vWXb2bosCQtT4JJx4gzKl16yMic2ia3kC65R7MGpHHBZoMWxOjz0iaWPRJIrAzappZYD3wiaeJLX7YrQ%2F640%3Fwx_fmt%3Djpeg%26from%3Dappmsg)

还有物理条件，比如云设施、计算资源的准备也在加速。ChatGPT 刚出来时大家不知所措，很多人没准备好去接受它，花了很长的时间去学习和掌握。当我们接受和掌握它之后，再去解决新的问题，它的发展速度是越来越快的。

当然，能力辐射到实际的用户身上也有快慢之分，而且也分行业。可能在广泛的角度上大家没感知到能力的进步，但从技术来说，进步的曲线越来越陡。我对高阶的 AGI 发展是比较乐观的，而且发展速度会越来越快。

**02**
------

**o1 提升了 AI 上限，**
-----------------

**带来了新范式**
----------


**张鹏：外界有人说：「AGI 怎么发展变慢了」，三位的反应好像是：「你还想要怎样？」它的发展进程在这 18 个月里已经让我们每个人都目不暇接了。**

**OpenAI 的新模型 o1，在专业人群里产生了非常大的影响，现在还有很多讨论。先问问大昕，你怎么看 o1？很多人认为这是 AGI 发展阶段的一个重要进步，我们到底该怎么理解这个进步？**

**姜大昕**：我确实看到了一些非共识：有些人觉得 o1 意义很大，有些人觉得 o1 也不过如此。我试用 o1 的第一印象就是：它的推理能力确实非常惊艳。我们自己试了很多 query，觉得推理能力确实上了一个很大台阶。

它背后的意义究竟是什么？我能想到的有两点。

第一，**o1 第一次证明了其实 LLM 可以有人脑的慢思考，也就是系统 2 的能力。**以前 GPT 的训练范式是「预测下一个 token」。这就注定了它只有系统 1 的能力，而 o1 用了强化学习这样一个新的训练框架，所以带来了系统 2 的能力。系统 1 是直线型思维，虽然我们看到 GPT-4 可以把一个复杂问题拆解成很多步，然后分步解决，但它还是直线型的。系统 2 和系统 1 最大的区别就在于，系统 2 能够去探索不同的路径，可以自我反思、自我纠错，然后不断试错，直到找到正确的途径。

这次 o1 把以前的模仿学习和强化学习结合起来了，使模型同时有了人脑系统 1 和系统 2 的能力，我觉得从这个角度来看它的意义是非常大的。

第二，**带来了 scaling law 的新方向**。o1 试图回答的一个问题是：「强化学习究竟怎么泛化？」o1 不是第一个做强化学习的，DeepMind 一直在走强化学习路线，从 AlphaGo 到 AlphaFold 到 AlphaGeometry。DeepMind 在强化学习上非常厉害，但是以前这些强化学习都是为特定场景去设计的------AlphaGo 只能下围棋，AlphaFold 只能预测蛋白质的结构。o1 的重大意义是让强化学习的通用性和泛化性上了一个大台阶。

而且 o1 已经 scale 到了一个很大的规模，我认为它带来了一个 scaling 技术的新范式，不妨称之为 RL scaling。而且 o1 还不成熟，它还是一个开端。这点恰恰让我觉得非常兴奋，这就等于 OpenAI 跟我们说：「我找到了一条上限很高的道路，仔细思考它背后方法的话，你会发现这条路是能够走下去的」。

总的来说，o1 从能力上展示了 LLM 可以有系统 2 的能力；技术上带来了一个新的 scaling 范式，所以我觉得它的意义还是非常大的。

**张鹏：听起来，虽然说现在有非共识，感觉你是非常看好，非常认同的。朱军老师怎么看，o1 带来这一阶段的进展，你怎么评价它的意义？**

**朱军**：我的看法是，它代表着一个显著的质变。

我们对 AGI 大概做过一些分级，学术界和产业界有 L1-L5 的分级，L1 相当于聊天机器人，像 ChatGPT 等，之前大家做了很多对话。L2 叫推理者，可以做复杂问题深度思考的推理。L3 叫智能体，「数字世界」走向「物理世界」，要去改变，去交互。L4 是创新者，要去发现、创造一些新的东西，或者发现一些新的知识。L5 是组织者，它可以去协同，或者有某种组织方式更高效地运转，这是大家对于 AGI L1-L5 的分级，当然每一级也有 narrow 和 general 的区分，现在在某些任务上可以展示出来。

比如 o1 在 L2 的 narrow 场景下，在一些特定任务下已经实现了，可以达到人类很高阶的智能水平。我觉得从分级角度来看，它确实代表着整个行业的一个巨大的进步。

技术上，过去的强化学习或者其他一些技术，其实在研究里已经做出了很多东西，但能在大规模基座模型上 scale up，做出效果，从工程上或者从实现上来说，对行业来说是一个很大的触动。当然它也会触发或者激发出很多未来的探索，或者实际的研发，可能会走向从 narrow 到 general 的跃迁。这个速度我相信会很快，因为大家已经有很多准备了，我也期待这个领域里有更多人将 L2 做得更好，甚至实现更高阶的效果。

![](https://cubox.pro/c/filters:no_upscale()?imageUrl=https%3A%2F%2Fmmbiz.qpic.cn%2Fsz_mmbiz_jpg%2FqpAK9iaV2O3vWXb2bosCQtT4JJx4gzKl1RwuicM7aKYbgRy59585vwCnfuFX87xNX3oNCfvFfo4dMddvAnq94RPg%2F640%3Fwx_fmt%3Djpeg%26from%3Dappmsg)

**张鹏**：感觉你对这个定义已经很高了，你看到了 AGI L2 层面显著明确的路径和阶段性成果，而之前都在 L1 的层面。当然要到大家期望的拥抱改变物理世界，最终还要往前走，到了 L3，可能这件事就真的会完整的、系统性地发生。

**回到植麟这边，这次发布 o1 之后，Sam Altman 热情洋溢地说，我们认为这是一次新范式的革命。当然 Sam 很会演讲，很会表达。我想听听你怎么看，怎么理解他说的「这是一次新的范式变革」，你是否认同？**

**杨植麟**：我觉得它的意义确实很大，**主要意义在于它提升了 AI 的上限**。

AI 的上限是说，（能）去提升 5%、10% 的生产力，还是 10 倍的 GDP？我觉得这里最重要的问题就是，能不能通过强化学习进一步 scaling。所以我觉得这（o1) 是一个完全提升了 AI 上限的东西。

如果我们看 AI 历史上七八十年的发展，唯一有效的就是 scaling，加更多的算力。在 o1 出来之前，也有很多人在研究强化学习，但都没有一个非常确切的答案，强化学习如果和大语言模型，或者和 pre-training、post-training 整合在一起，它能不能持续提升？比如 GPT-4 这一代模型的提升，更多是确定性的提升，在一样的范式下把规模变得更大。

**但是我觉得 o1 的提升并不是一个完全确定性的提升。**

在之前，大家可能会担心数据墙的问题，现在互联网上大部分优质数据都已经被使用完了，也没有更多的数据可以挖掘，所以原来的范式可能会遇到问题。AI 有效了，又需要进一步 scaling，那这个 scaling 从哪里来？我觉得（o1）很大程度上解决了这个问题，或者说至少证明了初步可行。初步可行的情况下，可能会有越来越多人投入去做这个事情，最终要做到 10 倍 GDP 的最终效果，它完全有可能，我觉得是一个很重要的开端。

当然，我觉得对很多产业格局，或者对于创业公司的新机会来讲，也会发生一些变化。比如这里很关键的一个点是，训练和推理算力占比会发生很大的变化，这个变化不是说训练的算力会下降，训练的算力还会持续提升，但与此同时，推理的算力提升会更快，这个比例的变化本质上会产生很多新的机会，会有很多新的创业公司的机会。

一方面，达到一定算力门槛的公司，可以做很多算法的基础创新，甚至可以在基础模型上取得突破，我觉得这个很重要。而对于算力相对小一点的公司，也可以通过后训练的方式，在一些领域上做到最好的效果，也会产生更多的产品和技术机会，所以我觉得，整体也打开了创业相关的想象空间。

**张鹏**：所以这一次核心的所谓范式变化，带来的就是在 Scaling Law 上解决了我们接下来的 scale what，我们看到了新的路径，并且未来可拓展的创新路径空间和探索的东西变多了，而不像原来，是一个收缩甚至是遇阻的状况。

**03**
------

**推理能力泛化路径还** **不明确，**
----------------------

**是一个新的技术变量**
-------------


**张鹏：想问问朱军老师，今天在一个阶段性的、还比较明确的一些场景里，这种把 RL 加到体系里面成为一个新的范式之后，我们能看到明显地去泛化这个能力的路径吗？**

**朱军**：这个问题确实很值得思考，因为现在它先是在一些任务上能取得突破，我们再想着把它做到更广泛的任务上，或者有更广泛的能力提升上。从目前来看，o1 没有完全告诉我们技术路线是怎么做的。

**张鹏**：明显没有 ChatGPT 出来前那么 open。

**朱军**：对，但是从本身科研的积累解读，能看到它到底用了哪些技术。

这里有一个很重要的问题，叫过程监督的数据，它和之前的结果直接 output 的监督还不太一样，要对里面的每一步都去标注，比如思考的过程，获取这种数据首先可能就比较难，需要专业的人去做专业的高价值数据。

另外，在实际做的过程中，包括大家之前看 AlphaGo 迁移到其他领域面临同样的问题，在更泛化，或者**更开放的场景下，Reward Model 不好定义**。

比如说，现在有确定答案的定理证明或者是编程问题，Reward 是比较明确的，奖励函数是很容易定义的。但如果到自动驾驶、具身，或者艺术创作里面，比如生图、生视频，这里面的界定是比较模糊的，可能很多场景下是很难清晰地定义到底什么好，什么不好，可能很多问题不是「是」和「非」的问题，比如像生成式内容，对美学或者对其他的评价，每个人感受还不太一样。在这种情况下要去泛化的话，技术上就面临很多问题，我怎么定义 Reward Model，怎么收集数据，还包括怎么高效地实现，给它 scale up。

现在大家看到这条路了，相当于已经看到曙光，会引导大家朝着这个方向去努力。另外，结合现在比较强大的基座模型，可能比之前上一代 AlphaGo 迁移到其他领域里，我相信会更快，包括像一些开放领域里，我们有更好的模拟器，甚至包括一些 AGI 的生成方式来构建这个环境。这些加持在一起，我想这条路会走得更快一点，会比之前更容易取得效果和提升。

**张鹏：今天确实还没有看到一个公开、明确的，可以确定性把这个泛化完成的路径，但它存在探索的空间和足够的可能性。想追问一下植麟，这个状态对于像你们这样的创业公司，是好事还是坏事？**

**杨植麟**：我觉得这其实是一个很好的机会，因为等于说有了一个新的技术变量，是一个新的技术维度。

当然这个我们之前或多或少也有一些投入，但是现在可能它会变成一个主题，在这个主题下面，我们会有非常多新的机会。一方面是朱军老师提到的怎么去泛化的问题，另一方面是，在这个过程中还有一些基础的技术问题没有被完全解决，底层涉及到训练和推理，这两个东西要同时去 scaling，很多问题今天还没有被完全探索清楚，包括刚才提到过程监督的问题，中间的一些幻觉也会给它的效果带来很大的伤害，这些问题都很值得研究。

但如果能做好的话，可以把现在很多的能力提升一个台阶。所以对我们来讲，可能会有更多通过技术创新，形成一些突破的机会。

**张鹏**：有不确定其实是好事。有确定的方向和不确定的路径，对创业公司反而是好事，否则就没有创业公司的事了。

**04**
------

**新范式对算力的需求会更大**
----------------

**张鹏：回到大昕这边，过去我们说算法、算力、数据，这三个都是谈 AGI 时的关键三角，这次看起来在算法层面有些范式的变化，反过来对于算力、对于数据这方面，这个三角形会怎么产生连锁的反应，能不能帮我们推理一下？**

**姜大昕**：我觉得算法、算力、数据这个连锁的铁三角关系没有改变。RL 确实是算法上的改变。对算力造成的结果中，有一个是确定的，有一个大概率会确定，还有一个是目前不太会确定的。

确定的是，就像刚才两位谈到的，**在推理侧，它对计算的需求量肯定是成倍的提升**，这就是 OpenAI 在博客里提到的 Test-Time Scaling。它对推理芯片的能力要求肯定也是提高了，可想而知，OpenAI 在 o1 的背后可能是用了 H100 在做推理，一个问题往往要消耗十几秒、几十秒时间，所以我们要加快速度的话，对推理芯片的要求也会提高。

一个大概率会确定的事情是，**在训练 RL 的阶段，我们所需要的算力可能并不比预训练要少**，这可能是一个非共识。因为在 RL 的阶段我们做 self-play，这个数据量在理论上是可以没有上限的，我们也听说 OpenAI 在训练 Strawberry 模型时用了上万张 H100，训练了几个月，现在还是 o1 的 preview，训练还没有完成，所以训练的代价是非常高的。如果我们追求的是一个通用的，有泛化能力的推理模型，而不是为某个特定场景所设计的 RL 模型，可能训练所需要的计算量并不小。

还有一个是我不太确定的，在 self-play 时，我们用了主模型，它的参数量要不要再继续 scale，让它产生更好的推理路径。因为现在大家有一个普遍的观点是，GPT-4 到了万亿级参数之后，你再去 scaling 它的参数，它的收益边际是在下降的。但如果 RL 方法产生了放大器作用，它能加倍你的收益的话，是不是总的收益 ROI 又打正了？这是一个不太确定的事情，可以留在后面去验证。如果这个结论成立的话，算力的增长又回到了平方的维度，因为计算量=参数量×数据量，所以我的感觉是，RL 带来的不管是对于推理侧还是算力侧，它对算力的需求都是在增长的。

那么数据测，刚才提到，RL 阶段有两种数据，一个是少量人工合成的数据；二是海量机器生成的数据。数据量可以很大，但数据的质量非常关键，所以你怎么去构造生成数据的算法，以及 self-play 用的主模型能力就变得非常关键了。

**05**
------

**模型输出变慢了，**
------------

**怎么向用户交待？**
------------


**张鹏：今天三位都是创业者，有自己的团队。想问问植麟，Kimi 在今年引发了很多关注，产品发展得也很好。你觉得这一波 AI 的新变化，接下来会对 AI 相关的产品带来什么样的连锁反应？这个变化会如何发生？**

**杨植麟**：我觉得我们现在还是处于产业发展的早期，这个阶段技术驱动的产品会更多，很多时候产品需要去看当前的技术怎么发展，然后去把最大化的价值提取出来。

我们可以根据新的技术进展，反推一下，现在的产品应该做什么变化。

现在的技术发展有几个点，一个是会有很多探索新 PMF 的机会，这个 PMF 是两方面的平衡，一方面是大模型需要做系统 2 的思考，导致延时是增加的，但这个延时增加对用户来说是一个负向体验，因为所有用户都希望能尽快拿到结果。

第二，它确实能提供更好的输出，能拿到更好的结果，能完成一些更复杂的任务。所以，我觉得这个新 PMF 探索的过程，**其实是要在延时增加带来的用户体验下降和结果质量更高的用户价值上升之间找到一个平衡点**。

要让增量的价值大于体验的损失，我觉得这个很重要，在一些更高价值的场景，特别是生产力的场景，可能率先会有一些东西出来，因为如果是娱乐场景，用户很难忍受延时上的增加。

然后，我觉得产品形态上可能也会发生一些变化，因为引入了思考范式。现在这种即时的类似聊天的产品形态，一定程度上也会发生变化，以后的 AI 可能会思考 20 秒、 40 秒，或者去调用各种工具。它会去执行分钟级别、小时级别甚至天级别的任务，产品形态上会更接近一个人，更接近一个真实的 assistant 或者助理的概念，帮你去完成一个一个的任务。这里面产品形态的设计我觉得会发生很大的变化，新的想象空间蛮大。

**06**
------

**推理能力会向物理世界落地**
----------------

**张鹏：我们也看到在 AGI 领域有一些其他的变化，比如李飞飞在推空间智能，也看到在自动驾驶、机器人等具身智能方面的变化，想问问朱军老师，在 AI 相关条线里的一些技术进展，会对未来的产品，或者说技术最终落到产业里有什么明确的推动？**

**朱军**：大规模预训练技术代表着一整个范式的变化。不光是从语言到多模态，再到具身智能，或者李飞飞老师的空间智能，**其实重点还是怎么让智能体能够有交互，能够在交互的过程中去学习。**

从智能角度来看的话，这是必然的，因为决策交互实际上是智能里非常核心的能力的体现，人类每时每刻都在做决策。我们面对的是一个未知的开放环境，对智能来说，它的发展路径规划里，也是朝这个方向在走。

现在所有的进展，包括刚刚讨论很多的 o1、视频生成、3D，这些东西最后指向的有两个方向：

一个是给消费者看到的数字内容，看上去很好看、很自然，能够讲故事，能够让大家参与讲故事、能够交互。在数字内容上，这肯定是一个很重要的方向，

另外一个方向，指向实体、指向物理世界，这一定是生产力的提升。不光给我们做一些好看的东西，或者好玩的东西，最终还要和物理世界结合。这其中可能最好的一个结合点就是和机器人结合，现在已经有了好多例子，我们也看到很好的一些进展，比如用了预训练的范式让机器人能力具有通用性。

我们自己实验室做过一些例子，像四足机器人，过去让它跑起来需要用很多的人工调参。但现在，在一个仿真环境里面，或者用一些 AI 的方式来生成一些合成数据，让它在里面大规模地训练，训练出来的策略可以灌到机器人上，相当于换了一副大脑，可以让它的四肢更好地协同起来，同样一套策略可以做各种场地的适应。这还只是一个初步的例子，现在大家也在关注更复杂的控制决策，像空间智能、具身智能等。

刚才讲到智能体是 AGI 的 L3，现在到 L1、L2 的进展之后，后面肯定会提升到 L3，让机器人更好地做推理规划，更高效地和环境做交互，更好地完成我们的复杂任务。现在很多任务相对来说还是会进行分解，定义成一个简化的。未来，通过它内嵌的思维链或者过程的学习方式，能够完成复杂任务。到那个时候，智能的能力又有一个很巨大的提升。

**07**
------

**谈卡伤感情，**
----------

**没卡没感情**
---------


**张鹏：想问问大昕，之前你们要花很多的成本去做基础模型，以及多模态模型，参数也要做得很大，都让人觉得要捏把汗，因为要花很多的钱，还会经常遇到问题。在过去 18 个月，包括这次的 o1 出来，对你的心态有什么影响吗？未来作为创业公司是有了更大的创新空间，有了更让人兴奋的可能性吗？**

**姜大昕**：我觉得从两个角度看，一个就是创新的点，RL 确实和前面的范式不太一样，GPT 的范式是 predict next token，其实从 18 年 GPT-1 出来，一直到 GPT-4，除了加了 MOE 的混合专家模型以外，没有什么太多新的东西。但是 o1 我觉得还是一个初始阶段，刚才也谈到强化学习究竟怎么和大模型相结合，能够做到泛化。我觉得里面有非常多的问题值得去探索。

刚才植麟也谈到 Reward Model，包括在做搜索路径的时候，需不需要人工干预去帮它找到更好的路径等等，self-play 题目从哪里来、答案怎么找，这些都是一些新的未知的领域，要去探索。我相信在未来的一段时间里，肯定会有很多加速，一定是这样的一个趋势。对于我们创业公司来说，在创新方向上肯定有很多的机会。

但是另外一方面，我认为在推理侧也好，训练侧也好，需要的算力还是不小的，尤其当我们追求通用性能够泛化的推理模型的时候，所需要的算力并不小。其实我们内部也有调侃「谈卡伤感情，没卡没感情」，后面又加了一句「用卡费感情」，但如果我们所要追求的目标就是 AGI 的话，付出再多也还是要坚持下去。

**张鹏：过去觉得如果按照 Scaling Law 继续往下走，玩家会变得越来越少，因为对资源的比拼要求太高。现在，你觉得对于资源的门槛会有降低吗？还是说继续要拼算力上的资源？**

**姜大昕**：我觉得分成两种不同的创新，一种是基础模型，就是奔着 AGI 去，就是要做通用的泛化新能力强的，这个的投入还是很大，而且我们看到国外巨头都是一年千亿美金的规划。

但是另外一方面，我觉得做应用，还是有大量创新的空间。我们本来觉得 GPT-4 所展现出来的智能，加上智能体 Agent 的框架，已经能够解决很多的问题了，一个是数字世界的问题，一个是物理世界的问题。那今年 o1 出来以后，强化学习又泛化到了一个更高的阶段，上限变得更高了，这里面还是有大量的机会。

**08**
------

**做 AI 应用，**
------------

**找 ChatGPT 的弱点**
-----------------


**张鹏：那我再问问植麟，今天植麟能不能换个身份，假定你今天不是创业者，是个有技术背景，对 AI 很了解的投资人。你今天会看创业者的什么数据，作为你的投资决策？**

**杨植麟**：首先像 DAU 这些数据，肯定是重要的指标。然后，可能分成几个层面。

第一个层面，作为创业者，做一个产品首先要有价值，或者满足了用户的真实需求，这个跟 AI 也没有太大关系，产品本身需要满足这些属性，所以可能又有更多的前置指标，比如留存，这个还是最重要的。

第二个点跟 AI 更相关，不光要有价值，也要有增量的价值。相比于市面上已有的 AI 产品，或者说更通用的 AI 产品比如说 ChatGPT，能产生增量价值，产生**一个在 ChatGPT 里面做不了的事情，或者说做起来体验很差的事情，这个就会有很大的增量价值**。比如说最近很火的 Cursor 就是一个例子。

一般增量价值会来源于几个方面：一方面，可能交互完全不一样，或者是不同的入口，有可能背后对应了不同的资源。通过这种方式去产生增量价值，我觉得可能会是一个很重要的事情。

第三个维度，不光是有增量价值，而且它还要随着技术的发展，市场规模应该越来越大，而不是越来越小。

如果目前的产品有一定的 PMF，但还没有泛化到一个很主流的群体，有可能是技术不够强，这个时候再搭配上第二点，有增量价值的话，这个市场又越来越大，它可能就是一个好的 AI 创业的机会。

**张鹏**：听起来就是，数据是要看的，但是在看数据之前先看逻辑，就是产品存在的逻辑，如果它是成立的，数据又能证明这就是一个值得投的公司。

**09**
------

**朱军：**
-------

**18 个月，在 L4 取得进展**
-------------------


**张鹏：在下一个 18 个月里，期待看到什么样的进展？就是在 AGI 领域里第一你觉得会很兴奋，第二你觉得它是有可能的事情。**

**朱军**：我希望看到 L3 已经基本上实现。

AGI 的 L3，至少在智能体、世界模型的创建生成、虚实融合，在一些特定场景下的决策能力有巨大提升。其实它会利用我们今天讲到的推理、感知等。

**张鹏**：在特定场景里确实不是 copilot，而是 autopilot 了。

**朱军**：我觉得可能在某种意义上会达到这种能力，至少在一些特定场景下。

我们前一段时间对 L4 做了专门的分析，发现如果要去做科学发现或者创新的话，需要的那些能力目前散落在各个角落里，但是现在还没有一个系统把这些整合在一起。

所以，如果更激进一点，我甚至觉得未来 18 月，**可能在 L4 上也会有显著的进展。**这里主要说的是严肃科学，L4 还有创意表达的部分，其实在某种意义上已经达到了，比如像艺术创造、图生视频，一定程度上帮大家去放大想象，或者是让想象可以具象化。我对整个的发展还是比较乐观的，至少 L3，或者 L4 有一些苗子吧。

**张鹏：年底之前，你自己的事上有什么进展，能提前透露吗？**

**朱军**：今年年底的话，希望将我们的视频模型能够以更加高效、更可控的方式提供给大家。

解释一下，高效和可控主要是指，去表达一个故事，不是简单的让一段话或者一张图片动起来，我们希望它可以连续的去讲，而且不光是人的一致性，还包括像物体等各种的主体一致性，还包括交互性。

高效，一方面是解决对算力成本的考量，因为如果想去服务很多人，让大家去用的话，首先成本要降下来，不然就还是烧钱、一直赔钱。另外一个更重要的还是体验上。对使用者来说，想去表达自己的创意，需要多次和系统交互，一方面去验证，另外是获得启发，这个过程也需要模型能够比较高效，比如说终极目标是实时，让大家能快速尝试。

到这个阶段的话，我相信用户体验、用户量都会有一个巨大的提升，这是今年我们重点想去突破的。

**10**
------

**杨植麟：**
--------

**开放性的强化学习、自我进化**
-----------------


**张鹏：3 个月的目标和 18 个月的对未来的期待都是很明确的。植麟呢，可以说说这 18 个月，也可以讲讲未来 3 个月会有啥进展。**

**杨植麟**：我觉得接下来最重要的 milestone 是开放性的强化学习。比如说在产品上跟用户交互，在一个真实的环境里面去完成任务，然后自己进化。当然，我觉得 o1 已经一定程度上说明这个方向比之前有更强的确定性，这个会是一个重要的里程碑，可能也是 AGI 路上仅剩甚至唯一的一个重要问题了，我觉得这个会很关键。

**张鹏：这个关键问题，你期待未来 18 个月有突破和进展？**

**杨植麟**：对，应该是能看到很多进展。

**张鹏：那未来三个月有什么可以透露的吗。**

**杨植麟**：我们还是希望能在产品和技术上持续创新，至少在一两个重要领域能够做到世界最好，有新的进展会尽快跟大家分享。

**11**
------

**姜大昕：**
--------

**多模融合，通向世界模型**
---------------


**张鹏：大昕你怎么看？18 个月和未来 3 个月。**

**姜大昕**：第一，我也很期待强化学习能够进一步泛化。

另外一个方向我也很期待，就是**视觉领域的理解和生成一体化**。在文字领域，GPT 已经做到了理解生成一体化，但是在视觉领域非常难。目前为止，我们看到的视觉模型，理解和生成是分开的，即使像多模融合的 GPT-4o，其他模态都解决了，唯独不能生成视频，这是一个悬而未结的问题。

为什么很重要呢？如果我们解决了视频理解生成一体化，就可以彻底建立一个多模的世界模型，有了一个多模的世界模型以后，可以帮助我们去生成非常长的视频，解决 Sara 目前的技术缺陷。还有就是可以和具身智能相结合，作为机器人的大脑去帮助智能体更好去探索物理世界，这个我也是非常期待的。

**张鹏：年底之前，你这边有什么值得期待的进展吗？**

**姜大昕**：一方面期待模型或者技术的进步，另外一个是产品能够带给用户更多更好的体验。

阶跃有一款产品叫做跃问，用户可以在上面体验到我们最新的万亿参数模型，它不光是理科很强，而且文学创作能力也很强，经常给大家带来一些惊喜。

跃问上还有一个新的功能叫拍照问，用户经常拍张照片去问食物的卡路里，问宠物的心情，问一个文物的前世今生等等。包括 Meta 眼镜的发布，还有 Apple Intelligence，都突出了视觉交互的功能，我们在跃问上也有体现，而且我们会努力一步步把这个功能做得越来越好。

![](https://cubox.pro/c/filters:no_upscale()?imageUrl=https%3A%2F%2Fmmbiz.qpic.cn%2Fsz_mmbiz_png%2FqpAK9iaV2O3uZFIyK9yAG4nfYwqxWDcqRiabAZSMtMqSEicDc5qzCRs8Zmr9tYpqEoXyJQv9I3jX3l05Q22AAJqdw%2F640%3Fwx_fmt%3Dother)

*** ** * ** ***

[![](https://cubox.pro/c/filters:no_upscale()?imageUrl=https%3A%2F%2Fmmbiz.qpic.cn%2Fsz_mmbiz_png%2FqpAK9iaV2O3sr5DM1ZXfrwBYuMUEd2y2aH7VgPwhRY5NVve78Io8WiaPEX9JsG2deNfmcfFZ6uRgq4EG8JpntdAg%2F640%3Fwx_fmt%3Dpng%26from%3Dappmsg)](http://mp.weixin.qq.com/s?__biz=Mzg5NTc0MjgwMw==&mid=2247507499&idx=1&sn=06f2cf36a3412f6d82b5acd8b5a9d24d&chksm=c0093817f77eb101b1a182a116dac9ac0f513c17585a915fbcf639407cd1a12df9c77c81b2f8&scene=21#wechat_redirect)


*** ** * ** ***

**更多阅读**


[Claude工程师聊prompt：不要把模型当小孩子、不需要角色扮演、实话实说](http://mp.weixin.qq.com/s?__biz=Mzg5NTc0MjgwMw==&mid=2247508182&idx=1&sn=ee4704560be3098755152fb832df5460&chksm=c00946eaf77ecffc37485cc26795006d8c250da98ef43e0a164d5d28ed11a49d7316d59c722b&scene=21#wechat_redirect)   


[8月份AI应用月活盘点，离超级应用有多远？ChatGPT还有机会吗？](http://mp.weixin.qq.com/s?__biz=Mzg5NTc0MjgwMw==&mid=2247508147&idx=1&sn=8e7321b2c2635b4db220e5fbcdbbb062&chksm=c009468ff77ecf99515a06cf0798d33df300fa4cc029089df6556088b0e5e1059151a39f4693&scene=21#wechat_redirect)   


[OpenAI o1是AGI下半场的开始，强化学习将成为新的 Scaling Law](http://mp.weixin.qq.com/s?__biz=Mzg5NTc0MjgwMw==&mid=2247508141&idx=1&sn=d952ee3baf2beb2b6443af7303a1bde0&chksm=c0094691f77ecf8780b1b13ebf3a598a81640a26101bd0f60f58d904a436eaabd2262b0f557b&scene=21#wechat_redirect)   


[AI创业者李博杰：小公司也能做RL，过几个月可能出现o1开源平替](http://mp.weixin.qq.com/s?__biz=Mzg5NTc0MjgwMw==&mid=2247507881&idx=2&sn=d8d3a3abc4588138a5740b4ec8147790&chksm=c0093995f77eb083401230c46d05c982acb5514fc9f64294e5c047b668d2a12168fa21b91a49&scene=21#wechat_redirect)

![](https://image.cubox.pro/cardImg/49ptyqcouzc9s3xbvbkji7igwiujtno1uzwe4pm8wftkt6j5kq?imageMogr2/quality/90/ignore-error/1)

**Founder Park**

来自极客公园，专注与科技创业者聊「真问题」。

303篇原创内容

<br />

公众号   

，

转载原创文章请添加微信：founderparker

[跳转到 Cubox 查看](https://cubox.pro/my/card?id=7236416084163691951)
