涌现能力与Scaling law是什么？有何联系，一文揭晓
=============================

[www.sohu.com](https://www.sohu.com/a/774197572_583945)

![](https://cubox.pro/c/filters:no_upscale()?imageUrl=https%3A%2F%2Fq0.itc.cn%2Fimages01%2F20240425%2F88149086e1c2485f8f48cc910d805262.png)

据不完全统计：自大模型涌现能力以来，全球每年有数千亿美元的资金源源不断地投入到大模型性能的研究中。

但什么才是模型智能涌现的关键？如何持续提升模型性能？业内给出了很多说法，同时也留给了人们很多的疑惑。

为了方便了解，下面笔者将根据以往的观察和总结，和大家分享一二，希望对大家有所启发\~

**大模型智能涌现的要素**
--------------

当人们惊讶于人工智能在对话、问答、做题、创作等方面带来的惊喜，可能也好奇是什么，让机器能表现出像人类一般的智慧。

这种随着大模型突破一定规模，所带来的语言理解、内容生成、逻辑推理等类人智慧的出现以及模型性能显著提升的现象，通常就被称为"涌现能力"（英文全称为：Emergent Ability）。

涌现被视为通向AGI（强人工智能）的必经之路。为此，关于模型能力涌现背后的成因，科学家们做出了很多的探讨。

比如以往很多人就认为，**涌现能力是模型突破一定规模后的产物，所以模型参数量会是模型能力涌现的成因。**

而且，关于涌现能力的参数门槛还涌现出了多种说法。比如有人认为至少要达到500-600亿的参数才有可能产生涌现能力，也有人认为这个参数值在100亿-1000亿。

虽然，有不少业内人士认为：模型参数量决定涌现的观点，有受到2022年谷歌某篇论文中的不严谨的实验和主观推论的误导。

但当下主流的大语言模型，依然倾向于将千亿级参数量作为大模型智能涌现和性能提升的"标配"。即将推出的Llama 3，甚至参数量达到了400B+。

![](https://cubox.pro/c/filters:no_upscale()?imageUrl=https%3A%2F%2Fq6.itc.cn%2Fimages01%2F20240425%2F8909b2c8b10542e88c25f23723b2ed57.jpeg)数据摘自公开信息，仅供参考

另一边，观察到部分模型在下游任务上的性能和模型大小无关，而与总体的预训练损失有很大关联，也有人认为：**Pre-training Loss（预训练损失）才是涌现的关键**。

在这种思路下，无论模型大小，只要将Pre-training Loss控制在某个较低的阈值，就能实现涌现能力，改善模型在下游任务的性能。

这刷新了人们对模型涌现能力的认知。

但预训练并不是提高模型涌现能力的唯一途径，加上在训练量带来的模型能力中，**Pre-training Loss更多作为评估的早期指标。**

而且Loss值能随着算力、数据规模、参数量的提升呈现明显的线性下降，它背后的Scaling Law，很快成了模型智能涌现和性能提升的另一大关注点。

**Scaling law对模型性能的影响**
-----------------------

Scaling Law，顾名思义，就是Scale（规模化、尺度化）和Law（定律、法则）的组合，所以它又被译作规模定律、缩放定律、尺度定律等。

Scaling Law因提出"当模型参数（模型容量）、数据集规模（数据量）和计算量（训练时长）越大，就可以实现大模型性能的持续提升"这一概念而被人们所知。

不过就实现逻辑而言，**Scaling law本质上在于通过增大训练量，延长硅基大脑的学习时间，从而强化模型能力**。从第一性原理来看，这种思路和人类智能的产生其实是相通的。

因为看好Scaling law在这方面的潜力，2020年OpenAI率先采用Scaling law模式进行ChatGPT、Sore等爆款产品的训练，最终用行动向世人验证了Scaling law对提升和优化大模型性能的助益，也开创了一段"大力出奇迹"的商界传奇。

但机器智脑毕竟不像人脑，AI模型的进化需要规模、数据、算力、能耗等多方面的支撑。

而且随着每次模型参数、数据集和训练量的增加，所需的资金投入、能源消耗（如电能、水能等）等还会倍增。
> 据荷兰国家银行数据专家Alex de Vries估算：ChatGPT日耗电量超过50万千瓦时，超过美国家庭每天用电量的1.7万倍。
> 《2024年人工智能指数报告》显示：OpenAI的GPT-4等模型系统的训练成本约7800万美元，谷歌Gemini Ultra的计算成本约1.91亿美元。

加上在实际应用中，即便加大了模型参数、数据量和算力，依然可能存在训练不足，利用率不高等问题。

在这样的背景下，2022年，谷歌Deepmind团队提出了在计算成本达到最优的情况下，模型大小与训练数据 (token) 的数量应等比例缩放的方案。通过寻求三者之间的最优而非最大化，实现配置优化和模型性能的提升。

针对参数训练不足，利用率低等问题，司普科技等大模型应用服务商也提出了在保障足够学习时间（通过scaling law保障学习时长）的基础上，通过提高训练效率和质量，以提升模型性能的思路。
> 智能水平 = 学习效率\*学习时长

**Scaling law之下，加速垂类模型商业化**
---------------------------

通过Scaling law"大力出奇迹"的方式改善模型性能，能很好地满足通用模型的多元化业务需求，推动AGI（通用人工智能）在泛行业的早日落地。

不过，对于垂直领域来说，Scaling Law模式很容易面临算力、数据量等方面的瓶颈，即便实现，也很难在领域内发挥全部优势。

这时变"通用性大模型"为"辨识性小模型"，通过私域数据内循环、数据训练效率与质量的优化以及行业精调来提升模型性能，在眼下显得更为常见。

可能是发现小模型在这方面的潜力，身处大模型阵营的Llama3，在官方博客中公开提到：

"较大的模型可以用较少的训练计算来匹配这些较小模型的性能，但较小的模型通常是首选，因为它们在推理过程中效率更高。"

目前，贴合业务场景，以更少的算力和数据投入，更垂直的性能和更灵活的配置，来实现模型性能的提升，正成为垂类大模型的一大思路，也加速推动着AI小模型在垂直领域的商业化落地。

**写在最后**
--------

综上，模型规模、Pre-training Loss（预训练损失）、Scaling Law等都曾被认为是模型涌现能力的要素，尤其是Scaling Law，还被视为模型性能持续提升的关键法则。

目前，Scaling Law仍是主流玩家拓展模型性能边界的重要策略。

但出于成本、资源利用率、能耗等考虑，业内提出了在计算成本最优的情况下，模型大小与训练数据量等比例缩放，或提高机器学习效率以提升智能水平等想法。

此外，在垂直领域，通过与业务场景的深度结合，小模型已经能在通用大模型的基础上，缩小规模、数据、算力的同时，发挥出卓越的性能。

这极大地加速了垂类模型的商业化进程，也为提升模型性能提供了新的思路。

备注：首发司普科技，本文有参考脑极体、CSDN、知乎、GLM大模型等，司普科技CTO黄洪武先生对于本文亦有贡献。[返回搜狐，查看更多](https://www.sohu.com/?strategyid=00001&spm=smpc.content.content.1.1722478400711prTydHy "点击进入搜狐首页")

责任编辑：

[跳转到 Cubox 查看](https://cubox.pro/my/card?id=7218509682904665127)
