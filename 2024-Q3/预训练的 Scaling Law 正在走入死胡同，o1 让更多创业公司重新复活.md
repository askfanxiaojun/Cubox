预训练的 Scaling Law 正在走入死胡同，o1 让更多创业公司重新复活
=======================================

[mp.weixin.qq.com](https://mp.weixin.qq.com/s/olALbpciQ_omproZjLwnZw)今夜科技谈 极客公园


![](https://cubox.pro/c/filters:no_upscale()?imageUrl=https%3A%2F%2Fmmbiz.qpic.cn%2Fmmbiz_jpg%2F8cu01Kavc5bp2HcwLOGCvEQQWaSZiaK4cFZialLWbLqexEdHcS4iaVhvp7iaA7QWABcQoyavaBxibvyAtWLah1RPI2g%2F640%3Fwx_fmt%3Djpeg%26from%3Dappmsg)   


GPT-4o 读万卷书，「o1」行万里路。

**整理 \| 宛辰** **编辑**\| 靖宇****


[北京时间 9 月 13 日凌晨，OpenAI 在官网发布了其最新一代模型，](https://mp.weixin.qq.com/s?__biz=MTMwNDMwODQ0MQ==&mid=2653054485&idx=1&sn=4d0b5b5495ff57d36fe44d44260c0b88&scene=21#wechat_redirect)没有延续过去 GPT 系列的名称，新模型起名为 o1，当前可以获取 o1-Preview 和 o1-mini 这两个版本。  


当天，Sam Altman 在社交平台上兴奋地称，「『o1』系列代表新范式的开始」。

但这可能是第一次，外界比 OpenAI 的掌舵人 Sam Altman 本人，更加兴奋地期待 OpenAI 的新品发布。这份期待里，无关对赛道第一名的艳羡，更多是同呼吸、共命运的决定性瞬间。下一代模型是否有惊人的进展？能否为 AGI 的浪潮和梦想完成信仰充值？

今年，你可能也对 AI 这个字眼麻木了，去年有多狂热，今年就有多麻木。原因无他，在 AI 的落地应用上，看不到信心二字。截止目前，仍未出现颠覆性的 AI 应用；Inflection.ai、Adept.ai、Character.AI 等最头部的明星公司接连被大厂纳入麾下；科技巨头们在财报周被反复拷问 AI 的巨额资本支出何时看到回报......

这些情绪背后，都指向同一个问题，那个所谓的第一性原理「Scaling Law」可以通向 AGI 吗？以今年十万卡、百亿美金投入，换取模型性能线性增长、乃至对数级增长的门槛来看，这注定是一场玩不起的游戏。不少人开始质疑它的合理性，这波 AI 不会就这样了吧？

这是「o1」诞生的时代性。

在 OpenAI 交出答卷后，AI 创业者表示「又行了」。不同于预训练的 Scaling Law，一条在推理阶段注入强化学习的路径成为明确的技术新方向，徐徐展开。

极客公园「今夜科技谈」直播间也在第一时间邀请极客公园创始人 \& 总裁张鹏，和创新工场联合首席执行官/管理合伙人汪华、昆仑万维首席科学家\&2050 全球研究院院长颜水成，一起聊了聊 o1 所代表的新范式及创业者脚下的路。

![](https://cubox.pro/c/filters:no_upscale()?imageUrl=https%3A%2F%2Fmmbiz.qpic.cn%2Fmmbiz_png%2F8cu01Kavc5bp2HcwLOGCvEQQWaSZiaK4c1RTg9JwNBw0icENBXNUYI1N51O4GQCyTgQPaatXvATgVbotbHFUBNSw%2F640%3Fwx_fmt%3Dpng%26from%3Dappmsg)

以下是直播沉淀文字，由极客公园整理。


**01**


****「o1」释放了明确的技术信号，****

****但更期待下一个里程碑****


**张鹏：从去年传出「Q\*项目」到现在，** **OpenAI** **的强推理模型「o1 系列」终于发布了。实际用下来，「o1」的发布符合你们的预期效果吗？**

**颜水成** ：我用 o1 做的第一件事情是，把我女儿做的数学题输进去看结果，o1 的表现令人惊喜。它解题的逻辑顺序、总结的 CoT（Chain of Thoughts，思维链）信息，让人觉得很不一般。

如果是**用 GPT-4 或 GPT-4o，只是做下一个 token（词元）的预测，其实我们心里会打鼓、会怀疑：只是做下一个词元的预测，是不是就能实现复杂推理过程。**

**但 o1 相当于在回答问题之前，先引入用 CoT（思维链）表示的思考过程，把复杂问题先用 planning（规划）的方式将任务拆解，再根据规划的结果一步步细化，最后把所有结果做总结，才得到最终结果。**

一个模型的好与不好，关键在于它是不是直觉上能解决问题。GPT-4 和 GPT-4o 还是一种快思考，这种快思考不太适合解决复杂推理问题；但是 o1 是一种慢思考的过程，像人一样思考，更可能解决一个问题，尤其是跟数学、编程或者逻辑有关的问题。o1 所代表的技术路径未来会走得非常远，带来非常大的想象空间。

**汪华：**我觉得 o1 是一个非常好的工作，水到渠成，符合预期。符合预期是说这个时间点该有成果了，为更高的未来预期打开了通路，但并不 surprise，没有超出预期。

因为这个工作其实从去年就已经有一系列的线索，包括 OpenAI、DeepMind 出的一系列的论文像 Let』s Verify Step by Step (OpenAI, 2023)，以及其他像 Quiet-STaR 和 in-contest reinforce learning 中都有迹可循。

大家用强化学习、包括用合成数据去串 Reward Model（奖励模型）或 Critic Model（评判模型），或者后来**用各种各样结构化的推理来提高模型正确率。事实上，无论是 OpenAI、Meta，还是其他大厂，大家现在都已经在做类似的工作，这个方向其实是大家的一个共识。**

不光 OpenAI，很多其他模型在数学、编程、推理上都已经有了很大进步，就是因为或多或少用了一部分这方面的技术，但 OpenAI 发布的 o1 是集大成，并且工作做得非常好，而且里面应该有它独特的工程探索。

![](https://cubox.pro/c/filters:no_upscale()?imageUrl=https%3A%2F%2Fmmbiz.qpic.cn%2Fmmbiz_jpg%2F8cu01Kavc5bp2HcwLOGCvEQQWaSZiaK4cM1cGGAZic5zkzwicrjGpSKUlQDdcc2MGpQicah87p7h12rdZBNrSHtRlw%2F640%3Fwx_fmt%3Djpeg%26from%3Dappmsg)图片来源：OpenAI 官网

**张鹏：预期之内，但还不够惊喜。**

**汪华：**对，整个框架还是在预期范围之内，没有像 GPT-4 或者 GPT-3.5 发布一样带来很大的惊喜。

你会发现 **o1 针对推理等各方面性能的增强，还是在一些有明确对与错和封闭结果的领域。**比如 o1 展现的代码、学术解题，包括数据分析能力其实都属于有明确信号的领域。

哪怕是在明确领域，比如数学编程的问题，它在做得好的问题上表现非常好，但在一些问题上也做得不太好。也就是说，可能它在训练 Critic Model（评判模型）或者 Reward Model（奖励模型）的时候，对于下游任务的泛化，可能还是遵循物理规律。如果对下游任务覆盖得好，它就做得好；如果覆盖得不好、下游任务没见过这些数据，或者 reward model 没法很好地给予 reward 的时候，它泛化也不一定真的能泛化过去，所以从这个角度来讲，o1 没有特别的超出常识的部分。

我还测了一些更加通用推理的场景，在这些领域，o1 增强得还不太多，很多也没有带来增强的效果。

**实际上对 OpenAI 抱持更高的期待是，希望它下一步能做到，把推理泛化到通用领域。**

当然现在端出这么一个非常完善的工作，把这件事给做出来，OpenAI 这点还是非常厉害的。而且在跟 OpenAI 的同学聊天时，能感觉到他们在做更难的事情，朝着通用推理的方向在做，只是可能现在还不成熟，所以先放出来对于 signal（技术信号）更明显的阶段性成果，在代码、数学方面的工作。所以我也非常期待，什么时候 OpenAI 能把下一个里程碑也克服了。


**02**


****强化学习不新鲜，****

****「o1」在用强化学习上有创新****


**张鹏：o1 已经能在一些领域展现出复杂推理的能力，其中很重要的原因是，** **强化学习** **在 o1 系列模型里扮演了非常重要的作用。怎么理解强化学习在新一代模型里起的作用？**

**颜水成：****强化学习是一个存在时间蛮长的方向**，把这个技术用得最好的团队应该是谷歌 DeepMind，他们一开始就是从这个角度出发，去解决真实世界的实际问题。

**我个人觉得强化学习在 o1 里最核心的点，不在于使用强化学习，因为强化学习在 GPT 3.5 里就已经用了**PPO（一种强化学习算法），用一个奖励函数去指导 PPO，进而优化模型参数。

强化学习优化一个描述长期累计 rewards 的目标函数，而原先传统算法只是求解损失函数。相当于，在优化 policy action（策略动作）的时候，需要考虑未来所有奖励的总和。

具体来说，像在围棋博弈中，它会用 self-play（自我博弈）的形式去收集 action-status 序列，这个过程自动生成一个奖励值，而不是说去学一个奖励函数。它是直接自动产生出奖励，或者说人工可以定义奖励，用这些奖励就可以把策略学出来，然后逐步提升策略。它最大的特点是整个过程不需要人类干预，不是像 RLHF（根据人类反馈的强化学习），有很多的步骤需要人去反馈。

我觉得其实 **o1 跟原来的强化学习有一个最本质的差别。**有人认为，o1 的原理可能与斯坦福大学团队 (E Zelikman et al, 2024) 发表的 Quiet-STaR 研究成果最相关。Quiet- STaR 的一个特点是从 CoT（思维链）的角度出发，但是 **CoT 并不是一开始就存在**。

要做推理问题，原本有最初的文本存在，如果在文本里面再插入一些 CoT 的信息，它就能提升推理效果。

但当我们希望去解决通用的、复杂的推理问题时，大部分的情况下 CoT 是不存在的。那么在强化学习的 pipeline（流程管道）里面，如何把这些 CoT 的信息一步一步生成出来是非常困难的。

这就要问 o1 的模型架构是什么？是一个模型它既可以去做规划，又可以根据规划去生成 CoT，又可以做自我反思（self-reflection），又可以做验证，最后做一个总结，这些所有的事情。还是说其实是好几个模型，一个模型根据信息生成 CoT，另外一个模型做反馈，两个模型相互交互，逐步把结果生成。目前 o1 还不是特别清楚，两种可能都能做，单一模型可能会让整个过程更优雅。第二种可能实现起来会更容易一些。

**如何用合适的方式把 CoT（思维链）生成，我觉得这是 o1，跟其他的强化学习区别最大的地方。**这里的细节还不是很清楚，如果清楚的话，o1 的黑盒问题可能就解决了。

**张鹏：怎么把** **强化学习** **运用到这一代推理模型里？是一个单体的超级智能、还是一个集体决策，这些还没有被公开。**

**颜水成：** 上一代的强化学习，可能更像下围棋，通过别人已有的棋局，先学了一些东西以后再接着往前走。我觉得要做通用、复杂推理的话，它就会碰到很多从零开始（zero start），可能一开始根本没有 CoT 的数据，这种情况大概怎么去做学习，有待探索。

![](https://cubox.pro/c/filters:no_upscale()?imageUrl=https%3A%2F%2Fmmbiz.qpic.cn%2Fmmbiz_jpg%2F8cu01Kavc5bp2HcwLOGCvEQQWaSZiaK4cWdM2IJVwHLRibZArdw3wXicq7t6hEEmeAxvBoYhMwPjwIPFr6CdqW0ug%2F640%3Fwx_fmt%3Djpeg%26from%3Dappmsg)2015 年，DeepMind 推出了 AlphaGo，这是第一个击败围棋世界冠军的计算机程序，通过强化学习，其后继者 AlphaZero 和 MuZero 基于自我对弈与强化学习的方式，变得越来越通用，能够解决许多不同的游戏以及复杂的现实世界问题，从压缩视频到发现新的更高效的计算机算法。｜图片来源：DeepMind

**张鹏：为什么把** **强化学习** **放到模型里，成为接下来发展的共识？这个共识是怎么达成的？核心都是要解决什么样的问题？**

**汪华：**技术上有颜老师在。从商业角度，大家还是在讨论模型的智能上限这样一个问题。

举个例子，哪怕是一个员工的应用场景，小学生能干的工种，跟中学生、大学生能干的工种，差别还是非常大的。所以**模型的幻觉，或者说模型的复杂指令遵循能力、模型的长链路规划和推理能力，已经制约了模型的进一步商业化，哪怕我不是为了实现 AGI（通用人工智能）。**

所以大家早就已经有这个说法了，一开始就有「系统-1」「系统-2」的说法（快思考和慢思考）。基本上预训练相当于知识的压缩，它本身就跟人的直觉一样，没法进行复杂的推理，所以必然要找到一个方法来实现「系统-2」。

在实现「系统-2」的时候，用各种各样的结构化推理，包括用各种各样的强化学习，有一个正好的规划，更稳定的模型输出，更好的指令遵循，包括让模型不光是学会知识本身，包括按什么样的 pipeline（流程管道）去使用知识。比如人类在解决问题 A 时会用思维框架一，解决问题 B 时会用思维框架二。像这些问题怎么来做？大家手里的武器库，其实除了 LLM，就是强化学习。

而且我特别同意颜老师刚刚的说法，**具体实现上用了一个模型还是两个模型，只是一个工程问题，但 CoT 的数据从哪里来？包括怎么来实现一些真实世界的模拟和对抗，这个反而是大家一直在试图攻克的难点。**代码和数学之所以能被很快地解决，是因为它的信号非常明确，对就是对，错就是错，而且它的步骤合成，合成它的推理 CoT 数据其实是相对比较容易的，奖励或者 Critics（评判）也是相对比较明晰。

**颜水成：**就相当于说奖励能直接获得。

**汪华：**更难的就是代码和数学之外，**世界上那种复杂的、复合的，甚至开放结果的，没有明确的、绝对对错的，甚至没有唯一执行路径的这些问题怎么办。**我觉得把这个问题给解了，难度要比一个模型和两个模型其实要难得多。

**颜水成：**o1 这个框架里面我觉得应该还是有一个奖励函数存在的，不然就没办法推演到通用的复杂推理。


**03**


****「o1」发展下去，****

****更接近一个「超级智能体」****


**张鹏：o1 跟跟此前的 GPT 系列相比，是两个技术方向，可以这么理解吗？**

**颜水成：**对，**o****1 表现出来的行为不再是下一个 token 的预测了，而更像是一个超级智能体的样子**，未来可以处理多模态、可以处理工具，可以处理存储记忆，包括短期和长期的语义记忆。

![](https://cubox.pro/c/filters:no_upscale()?imageUrl=https%3A%2F%2Fmmbiz.qpic.cn%2Fmmbiz_jpg%2F8cu01Kavc5bp2HcwLOGCvEQQWaSZiaK4cucrqc9VHicicNEV4xp9PELRqPY23fqC3RJUpZgasXhnzslBjKjWYHsrg%2F640%3Fwx_fmt%3Djpeg%26from%3Dappmsg)《思考，快与慢》，诺贝尔经济学奖得主丹尼尔·卡尼曼经典之作，介绍了大脑的两种思维系统：系统 1 快速直觉、系统 2 缓慢理性｜图片来源：视觉中国

我个人是认为 o1 这个技术方向肯定是对的，从 GPT-4 到 o1 的话，其实就是从「系统-1」到「系统-2」的一个转变。今年 5 月我做过一个演讲，AGI 的终局可能是什么东西，当时提到了两个概念，一个概念叫做 Global Workspace（全局工作空间），一个叫超级智能体。

Global Workspace（全局工作空间）在心理学和神经科学领域里的一个理论，是说大脑里除了专用的子系统，比如视觉、语音，触觉等子系统之外，可能还存在一个区域叫做 Global Workspace。

如果「系统-2」，就是多步和多模型的形式一起来完成的话，现在 CoT（思维链）产生的结果，它非常像 Global Workspace 的工作原理。用一个注意力的模型，把文本的、未来多模态的、工具等信息都拉到这个空间，同时也把你的目标和存储的记忆（memory）都拿到这个空间里进行推理，尝试新的策略、再做验证、尝试新的可能性......不停的往前推理，演绎的结果就是最终得到分析的结果。推理时间越长，就相当于在 Global Workspace 里的推演过程越长，最终得到的结果也会越好。

对于复杂的任务无法用「系统-1」（快思考）一竿子到底，就用「系统-2」（慢思考）的 Global Workspace，把信息逐步分解、推演，同时又动态地去获取工具，动态地去获取存储记忆，最后做总结，得到最后的结果。

**所以我觉得 o1 发展下去，可能就是「系统-2」（慢思考）的 Global Workspace 的 AI 实现方式，如果用 AI 的语言来描述的话，其实它就像是一个超级智能体。也就是说，o1 发展下去，可能就是一个超级智能体。**


**04**


****LLM+RL 的模式，****

****是否可以通向泛化推理？****


**汪华：强化学习相关的共识其实很早就有，但大家一直也没解决好问题。当年强化学习也很火，还被视作 AGI 的一个通路，包括机器人领域也都是用强化学习，但当时就遇到了这个难题：对于非常明确的任务，奖励函数很好建、任务的模拟器也很好建；但一旦扩展到真实世界的泛化任务时，就没法泛化，或者没法建立能完整模拟真实世界各种各样、复杂奇怪的任务模拟器，也没法去建立对它很好的奖励函数。**

**您觉得按现在这条 LLM（大语言模型）加上 Reinforcement Learning（强化学习）的模式，不止是在有明确信号的领域比如代码、数学，如果要往泛化推理走的话，要怎么走？**

**颜水成：**一个最大的差别就是，原来的强化学习，它的泛化性能不好。每次可能是专门针对一个游戏、或者一组类似的游戏去学一个策略。但是**现在它要做通用的复杂推理，面对所有问题都要有能产生 CoT 的能力，这就会变成是一个巨大数据的问题和工程的问题。**

我非常认同汪华的观点，在数学、编程、科学这些问题上，可能比较容易去造一些新的 CoT 数据，但是有一些领域，想要无中生有地生成这些 CoT 数据，难度非常高，或者说还解决得不好。

要解决泛化的问题，数据就要足够多样，但**在通用场景的推理泛化问题上，这种 CoT 的数据到底怎么生成？**

或者也有可能根本就没有必要，因为那个问题可能已经解决得很好了，你再加 CoT 可能也没有意义，比如说在有一些问题上，可能感觉 o1 没有带来本质的效果提升，可能因为那种问题本来就已经解决得非常不错了。

**张鹏：强化学习在下一代的模型里要扮演更重要的作用，会带来什么影响？**

**汪华：**如果大规模采纳这个方案，算力会更短缺，推理会变得更重要。

因为之前说推理成本将来会降 100 倍，现在如果往强化学习的方向发展，推理成本就更需要降了，因为解决问题要消耗更多的推理 token。降低推理价格其实等效于推理速度提升，需要把推理所需的时间压缩下来，很多应用才会变得可用。

第二，模型大小也要变得非常精干，因为如果无限的扩张基模尺寸的话，推理速度会变得更慢、也更昂贵，从商业上来讲就更加不可行了，因为采纳结构化推理，可能要消耗 100 倍的 token 来解决同一个问题。

**张鹏：你怎么看 LLM+RL 的前景，推理泛化的路径是清晰的吗？**

**汪华：**o1 之后 AI 的未来怎么发展，其实我相对比较保守，什么事情都做两种打算。

**第一种是，我们在很长段时间内没有找到泛化的方法。但即使是这样，我个人认为依然是一个巨大的进步。**因为这虽然意味着很多开放的、复合的、非常复杂、模糊的问题上，我们没办法用这个方式来提升，但是**商业场景下有大量的问题，比如法律、金融领域，很多问题是封闭的、明确的。在这些问题上可以通过这条路径，去合成数据、去做奖励模型、判别模型（critic model），极大地提高垂直领域的性能，甚至把性能提升从 Copilot（辅助驾驶）提升到 Autopilot（自动驾驶）的地步**，这就是一个飞跃性的改变。

![](https://cubox.pro/c/filters:no_upscale()?imageUrl=https%3A%2F%2Fmmbiz.qpic.cn%2Fmmbiz_jpg%2F8cu01Kavc5bp2HcwLOGCvEQQWaSZiaK4cuwM1f2icibepdg3EJ1EpJDBOJSZARCmvs4afdWTS9WuA2FX6ZlAFrH7A%2F640%3Fwx_fmt%3Djpeg%26from%3Dappmsg)无人驾驶汽车｜图片来源：视觉中国

这个场景有点像回到 AI 1.0，但比 AI 1.0 好，因为会针对垂直领域会做出一个个垂直的模型或者一套体系，从商业角度上来说已经是个巨大进步了。现在大家天天忙着做 copilot，没法做 autopilot，就是因为模型不擅长做长推理，一做复杂问题就出错，产生幻觉等问题。

第二种，如果能实现通用模型的话，影响就比较大了。哪怕不一定带来 AGI，它的泛用性、泛化性差、解决问题依然比人差，正确率没有高的，但**只要高过普通人的平均水平，那也可以给世界上 70% \~ 80% 的事情带来自动化。**你要知道我们很多成年人也做不对奥数题，而且我们自己也有「幻觉」。


**05**


****建立真实世界模拟器：****

****能读万卷书，也能行万里路****


**张鹏：在今天这个节点看得见希望，但是可能一颗红心两种准备。哪怕不能够完全泛化，今天也能解决很多问题了，比如在专用领域里可以做到通用能力达标。**

**汪华：**对，**能不能实现推理泛化，我个人觉得关键在于能否构造一套泛化的「真实世界模拟器」。**构建这个真实世界模拟器，可能难点在于数据加上算法等一系列的因素。

**因为模型跟人互动，解决这些开放问题的时候，本质上是在跟真实世界互动，真实世界就是那个奖励函数或者判别函数（Critic Model），能不能建立一套新的方法论，能真实地模拟这个真实世界的反馈，而且能脱离人类反馈。**

之前的 SFT（精调），包括之前的强化学习本质上是基于人类的反馈（RLHF），这就像 AlphaGo 只是跟着人类棋谱学习，而不能左右互搏，效果肯定是有限的。

构建这个「真实世界模拟器」，可能难点在于数据加上算法等一系列的因素。这个模拟器一旦建立了，模型会产生无限的数据，就像 AlphaGo 互相下棋，它可以下 100 万盘、 1000 万盘、1 亿盘，而且它来判阵输赢，通过输赢的判断去模拟棋道真谛。

**张鹏：有点像它是要创造一个真正有效的世界，AI 在里面能** **「解万道题」，甚至** **「行万里路」，而不只像原来那样「读万卷书」，这个东西其实最终才能通向更广泛化的意义，而不受限于人类的反馈、等着人类师傅带，成本很高、泛化也很难。**

**汪华：**而且这里面其实在我看来分两个阶段。第一阶段就是 LLM（大语言模型）的阶段，预训练的阶段就是压缩知识，学习人类的知识，而 RL（强化学习）的阶段是练习和摸索思维方式。两个阶段培养两种能力，最后都压缩到这个 LLM 里面的 Latent Space（潜在空间，深度学习中一种数据的低维表示形式）里面。

![](https://cubox.pro/c/filters:no_upscale()?imageUrl=https%3A%2F%2Fmmbiz.qpic.cn%2Fmmbiz_png%2F8cu01Kavc5bp2HcwLOGCvEQQWaSZiaK4cO19XkR2GzFCqzhXaVRiaRp4eG1hvfbtrX2esiaQGiaZhlTExv52AWm8wg%2F640%3Fwx_fmt%3Dpng%26from%3Dappmsg)Sam Altman 在 X 平台称，OpenAI o1 是新范式的开始｜截图来源：X.com


**06**


****Sam Altman 所谓****

****「新范式的开始」，有几分可信****


**张鹏：作为这次 OpenAI 发的新模型，「o1」不再延续过去 GPT 系列的叫法，比如 GPT 四点几，而是开启了 o 系列新模型代号。同时 Sam Altman 称这是一次新范式的开始。我们怎么理解这个所谓的新范式？**

**颜水成：** 如果按照 Global Workspace 这套理论去对照「系统-2」（慢思考）和「系统-2」（快思考），o1 和 GPT-4 是有本质差别的，其中最大的差别在于，它会在解决问题的过程中产生 CoT（思维链）。

**一年前有人说 Ilya Sutskever (OpenAI 联合创始人) 在「草莓模型」（o1 之前的代号叫草莓）里发现了一些让人震惊的、可怕的事情。**今天可以大概推测，他当时到底在草莓模型里面发现了什么东西。**我觉得他发现的就是 AI 的学习过程，RL（强化学习）和 CoT（思维链）相结合，他发现系统具备自己挖掘 CoT（思维链）的能力。**

我个人认为，CoT 的过程，不是纯粹从已有的知识里去提取知识。CoT 的过程跟人类的思考过程一样，会展开不同的分析组合，包括验证、自我反思等。**CoT 过程结束之后，其实一个新的知识就产生了，**因为你其实会对以前的知识进行再加工，可以认为这是一个新知识。

当模型具备自动产生 CoT 的能力，意味着它有知识发现和知识增长的能力，新的知识可以重新完善（refine）模型，也会注入（inject）到模型自我的知识里，AI 就可以实现自我提升（self-improving）的能力。

从这个角度看，**o1 如果能够自动地去做挖掘 CoT，它真的就是一个新范式的开始。它不只是提取已有的知识，而是不断地产生新的知识，是一个知识增长的过程，是一个用算力去挖矿的过程，挖掘出新的知识**。知识就会越来越多，AI 就能做研究者能做到的很多事情。

**张鹏：** **要这么说的话，** **人类的科学发展进程也是人类不断产生 CoT 的过程，现在发现模型具备了 CoT 能力，自己能够获得更多知识，也能基于这个知识再优化自己，有了「自我进化」的能力。这可能是新范式代表的含义，当我们要需要模型更有效地解决问题，有赖于它自主产生 CoT 的能力，并能够自我进化。**

**颜水成：**所以有可能，OpenAI 把所有大家问的问题、信息全部都存下来，然后再拿这部分东西训练模型，就可以把模型的能力进一步提升。相当于全世界的人用自己的钱、用他的算力去进行了知识的扩展，然后扩展出来的 CoT 结果，又可以使模型变得越来越强。如果从这个角度来说的话，确实是一种新的范式的开始。

![](https://cubox.pro/c/filters:no_upscale()?imageUrl=https%3A%2F%2Fmmbiz.qpic.cn%2Fmmbiz_jpg%2F8cu01Kavc5bp2HcwLOGCvEQQWaSZiaK4ciaSae1GdZglSs7CsEa0vHkyphicIiaI3iaFuBLKqspZnJeYTcyfK9WXDOg%2F640%3Fwx_fmt%3Djpeg%26from%3Dappmsg)擅长布道和公关的「融资大师」Sam Altman｜图片来源：视觉中国

**张鹏：直播间里有观众说得很直接，说 Sam Altman 口中的「新范式」就等于「OpenAI 融资难了，需要有一些新的话术」。你觉得他说的新范式，是一个给大家提高预期、一个信仰充值的东西，还是说会再开启一个所谓的新范式？**

**汪华：**从投资人的角度来讲，现在中美投资人，已经初步过了「为了科学突破而感到激动」要投钱的时间点了，都在忙着看商业化，会看这个东西到底解决什么问题。虽然投资人可能比较俗气，但是过去一年多毕竟也是砸了那么多钱进去，千亿美金的算力、Infra 等都投进去了。

对创业公司或者大语言模型的发展来讲，我个人认为是一个新范式，而且是大家已经期待的新范式。过去的 Scaling Law 每次都要 100 倍的算力（扩张），指数级别的算力增长，然后只得到线性的模型性能提升。这会带来两个问题，第一，怎么再继续扩张（scaling）下去。第二，对于创业公司、研究机构，包括新的 idea 的出现，是一个绝对的扼杀，因为最后只有少数的帝王级企业，才有资格去做这件事。

但「o1」代表的范式，把很多东西拉回来了，世界可以更多元化了。不是说不要 Scaling Law 了，可能新范式下依然是模型越大效果越好。但有了「o1」所代表的新范式，Scaling Law 可以从更小的模型做，而可能这个模型算力提升 10 倍就能看到原先百倍的效果，而不是要指数级别的提升才能看到很多进步，包括对于各种各样的商业化也更友好了。

所以从商业角度来讲，我个人觉得**「新范式」是有潜力发生的，而且是必须的。按原来那条 Scaling Law 是一条死路，都不说再 Scaling Law 下去全世界的数据够不够用这个问题，在 Scaling Law 下，全世界还有多少人能做这件事都是问题。**


**07**


****「o1」打破了预训练的****

****Scaling Law 瓶颈，****

****商业上解锁了新的可能****


**颜水成：****所以其实最近有不少公司，也基本上觉得纯粹的基础模型的预训练意义已经不大了，因为基本上是 10 亿美金级了。**

**汪华：**而且你**就算训练得起，你用得起吗？**AI 如果真的要给整个世界带来广泛的进步，本身就需要范式改变，光靠推理成本的下降是撑不住的。

另外从学术的角度来说，我觉得这个范式有的地方变了，有的地方还是没变。现在 o1 模型里的很多问题，包括规划、推理，其实它在产生 CoT 的过程依然还是在做下一个 token 的预测。

![](https://cubox.pro/c/filters:no_upscale()?imageUrl=https%3A%2F%2Fmmbiz.qpic.cn%2Fmmbiz_jpg%2F8cu01Kavc5bp2HcwLOGCvEQQWaSZiaK4cnDkcnCMN9x3oKqsia9r6px4jxRwohiaWj0Fr2uBibS2nHcj2n1uic4Mw8Q%2F640%3Fwx_fmt%3Djpeg%26from%3Dappmsg)生成式 AI 的创业门槛，算力是绕不开的巨额成本｜图片来源：视觉中国

**颜水成：**因为有很多过程，比如有的在做规划，有的是一小节一小节在做 CoT，有的是在做自我反思（self reflection）。这个过程到底是怎么实现，现在还不是特别清楚。**如果就是说按照一个固定的流程，都按照下一个 token 的预测来做，那么 CoT 就是一个 new data 的问题了，但是我是觉得可能不只是一个 new data 的问题。**

**汪华：**对，因为没有细节不知道，但是我在测试模型的时候，还是发现它的推理步骤会有幻觉，中间会有奇怪的推理步骤，但错误的推理步骤却得到了正确的结果，正确的推理步骤下的推理，又飞到天上去了。

我个人觉得新范式是从学术上、科学角度来讲，范式是改变了，但说不定还需要改变更多。

我认同颜老师的观点，**知识是 data，推理过程和思维方式其实也是 data****。**下一个 token 的预测，这个方法也不一定是错的，也不是说一定要摆脱，但是**学习关于推理过程和思维方式的 data，是不是有更进一步的一些范式的改变。**

**张鹏：受限于之前的模型技术，一度觉得很多事都干不了了，现在 o1 之后，又感觉未来有很多事可干，作为一个技术研究者，你觉得有哪些下一步值得探索的方向？**

**颜水成：**以前用 GPT-4 或者 GPT-4o，虽然说能产生出不错的结果、能做不少事情，但是其实在直觉上会感觉，下一个 token 的预测，这个东西好像没有这个能力，或者应该不具备这种能力。所以这个条件下，我们会在 GPT-4o 的基础上，再搭一个 agent，用 agent 去调用大模型、调用现成工具的形式。

**虽然 Agent 有潜在可能性去解决这个问题，但是进展不是特别好，因为它还是没有一个比较完美的框架来解决问题，不像刚才提的 Global Workspace 的过程。后者是把信息全合在一起，在这个过程中去做演绎、去做推理、去做验证、去做自我反思。**

**但是现在有了 o1 就合理了，要得到最后结果，其中有一个思考过程**，这个思考过程其实并不是原来常规的大模型，就直接能生成出来。有了这一套范式之后，你给我任何一个问题，我直觉上应该是能用这种方式解决的，无论是复杂推理还是其他，所以会有很多事情可以做。

另外，有了这套范式，那种超级大的模型有可能变得不是那么重要，模型可以做得比较小，但它就能做成一个，模型参数并不是特别大的一个网络架构，但它在推理的时候，能够做得更加复杂。

这种情况下，就不会像以前的 Scaling Law 一样，到了只有 10 万张 H100，才能够真真正正的进入到第一梯队。你可能有几千张卡，就能在一些维度能做到非常好的效果。

**原来的 Scaling Law，可能在摧毁很多的创业公司，那么现在可能又会让一波的创业公司要重新的复活，去做各种各样的事情。**所以无论是从可行性，还是创业的角度，我觉得机会都比以前要更好，没有进入到一个死胡同。


**08**


****当「o1」通向 autopilot，****

****AI 应用该怎么做？****


**张鹏：从「o1」模型里看到新的可能性，会对接下来的创业、产品、解决真实世界的问题，带来什么样新的挑战或者机遇？比如一个问题是，「o1」推理的进程肯定比原来拉长了，原来像 GPT-4o 以快为美。**

**汪华：**我觉得这不会成为问题。**因为「系统-1」和「系统-2」是共存的关系，并不是说 o1 出来了，GPT-4o 就没有用了。人类在大部分时间其实也是处于「系统-1」（快思考）而不是「系统-2」（慢思考）。**

具体还是要看应用类型。比如像 AI 搜索、Character.ai、写作辅助这些应用，大部分场景其实用原来的模型、速度够快就行。产品上也好解决，可以通过意图识别的分类模型，把不同任务分给不同模型。

长期看，如果「o1」更加完满了之后，GPT-4o 所代表的「系统一」和「o1」代表的「系统二」实际上是在两个不同的流水线上。

举个例子，为什么要追求推理速度快？因为我们**现在大部分应用是 Copilot。Copilot 的应用当然要快，人就在旁边等着呢。但是如果「o1」未来做得足够好，能带来更高准确度、能解决复杂问题、能很好地实现 agent，它执行的任务可能是 autopilot（自动驾驶）级别的任务的话，你管它多久完成呢。**我给同事分派一个任务，也是这个礼拜布置任务，下个礼拜看结果，不会要他立即交。所以如果是 autopilot 的场景，重要的实际上是模型、是这个模式做出来的性能，而不是低时延，更何况推理速度正在进一步提升。

第二，这是一个自适应的问题，如果将来「o1」模型训练得更完满，它的强化学习做得足够充分时，它会根据问题的复杂度和类型，有合适的延迟（latency）和推理时间、和 token 的消耗的。

**张鹏：之前 GPT 系列在 Copilot 场景继续发挥优势，反应更快、交互自然，但同时 o1 带来了通向 autopilot 的可能性，以前觉得很难做到的场景，随着强化学习、模型能力的继续优化，有更大概率实现。**

![](https://cubox.pro/c/filters:no_upscale()?imageUrl=https%3A%2F%2Fmmbiz.qpic.cn%2Fmmbiz_jpg%2F8cu01Kavc5bp2HcwLOGCvEQQWaSZiaK4cHMokz6YX38ZD4lhslYKOiaFwSBicYCJUtzQgxXXmQjbBEAB78EI2iatFg%2F640%3Fwx_fmt%3Djpeg%26from%3Dappmsg)图片来源：视觉中国

**汪华：**对，再具体一点说，**「o1」首先能解锁的就是大量企业级应用。**

现在哪怕在 SaaS 生态和 AI 进展更成熟的美国，很多企业级应用增长得很快，但目前摘的也都是低垂的果实，应用类型依然跟中国差不多，比如员工的写作辅助、智能搜索，企业知识库、销售支持客服等智能助手类应用。更关键的生产性任务和更复杂的任务，不是企业不想用 AI 来完成，是之前的模型做不到。

**第二，也会给 C 端交互类应用带来影响。**

在 C 端的交互革命里面，80% \~ 90% 可能都是原来的快速的模型，可能只有 10% 的任务需要调 CoT 模型。所以这个是会有很明确的分野。比如，Meta 的雷朋眼镜如果将来加了多模态，其实大部分的任务也都不是深思熟虑的，而是我看到什么东西它直接给我辅助，执行我的命令。

**ToB、ToC，生产力任务、娱乐任务、交互任务其实都是会有 copilot 和 autopilot 明确的分野的，而且会协同。**

举个例子，比如说让 C 端应用帮我订张机票，在我和它的交互对话、它向我展示漂亮景点的过程，后台已经帮我比价、调用各种资源做旅行计划了，后台可能就在用新一代推理模型。有时延也没关系，前台多模态的模型跟我聊天、糊弄我、延长我的等待时间，后面的模型在那里勤勤恳恳做 CoT，调用 agents 做推理演绎。收集你的信息，还能给你情绪反馈、提供情绪价值。

**张鹏：产品设计的空间，可创新的东西打开了，这其实是让人真正兴奋的。**


**09**


****「o1」模型可以提升机器人大脑，****

****但具身智能还有自己的卡点****


**张鹏：o1 的模型对于机器人的能力未来会不会有很大提升？比如像这种 CoT 的能力，未来会对具身智能产生什么样的影响？**

**颜水成：**我觉得会，因为具身智能需要有比较强的推理能力，一次推理，或者一次 CoT 出来的结果可能并不能满足条件，所以它能自我反思或者自我验证非常重要。

例如行走的机器人去完成某些任务，如果它有「系统-2」的过程，输出会更加准确、更加可靠。同时在一些场景，其实让他先想一想，再去做交互，用户也是能接受的。

未来当「o1」拥有多模态能力，它用在具身智能场景会变得更好。

![](https://cubox.pro/c/filters:no_upscale()?imageUrl=https%3A%2F%2Fmmbiz.qpic.cn%2Fmmbiz_jpg%2F8cu01Kavc5bp2HcwLOGCvEQQWaSZiaK4cOGTCoicL9ib3ib3pkHDgTibgPE5qb1l9ZHd9Ej5wPfBsPEK4HogKPCdM3A%2F640%3Fwx_fmt%3Djpeg%26from%3Dappmsg)图片来源：视觉中国

**汪华：**具身智能，现在有三件事都是它的瓶颈。

第一是硬件，硬件本身，包括传感器，依然有很多的瓶颈。

第二是可泛化的运动控制。现在机器人都是基于物理计算，或者是基于单项任务的模拟仿真、强化学习。而人的动作是非常复杂的，可以抓、拧、掏、抠，我们现在其实没有一个在运动控制上的 GPT。

第三，也是现在「o1」能解决的，大脑的问题，运动控制相当于小脑的问题，机器人「大脑」现在也能做，但是「o1」会极大地提高大脑对于运动规划的准确性、可控性、可靠性。

**这三个问题要都解决，具身智能才能实现。如果只突破「o1」，没有可泛化的运动控制，依然会受很多限制，因为机器人能进行的动作会非常有限。**

另外，我个人觉得这两件事说不定都有同样的瓶颈被卡着，可泛化的运动控制大家也试图用强化学习、模拟学习（Imitation Learning）来做，也缺少大量的数据，就像缺少 CoT 数据一样，缺少大量的真实世界各种各样的运动控制数据，只是：一个是要解决可泛化的运动控制，一个是要解决可泛化的推理。

因为没有一个人在手上带着传感器、脑袋上顶着摄像头，也没有几千万人天天做这些动作，贡献一个互联网级别的一个数据集，所以大家在用模拟器、用强化学习在做。但说不定在一件事情上找到了一些解决方法，另外一件事可以用类似的方法来解决同一个问题。

**颜水成：**感觉还是不一样，这种数据产生的流程和「o1」产生 CoT 的流程还是有很大差别，可能要当做一个垂域的问题去思考。

**汪华：**我特别**期待强化学习本身的方法论发生一次超进化，把现在强化学习，对于奖励模型泛化的限制取消。**

**颜水成：**因为强化学习本身的算法就比 GPT-4 的优化更难一些。比如强化学习的曲线的损失（loss），基本上一直在剧烈的震荡，但是像 GPT 模型，或者 AI 1.0 时代的判别式模型，曲线基本上非常稳定，所以强化学习做起来的难度或者说要求的领域 know-how 更多。

中国本身做这块的人就蛮少，现在慢慢好一些，但是相比国外做的时间和积累还是要少一些。

**张鹏：为什么中国的强化学习这条线会弱一些？上一代 AI，其实就看到了强化学习这个路线。**

**汪华**：强化学习其实各种各样的 paper 都在外面，中国聪明的人也很多，之前之所以做得不好，不是学术上做不出来，而是工程上和累积上，我们投入太低的问题。客观地讲，会有点功利化。**之前强化学习（RL）在各个领域里的效果都不是特别明显。**

**即使是在大模型时代，OpenAI 做了 PPO（一种强化学习算法、由 OpenAI 在 2017 年提出），做了 RL（强化学习），但实际上对于大部分国内的大模型公司来讲，做好 SFT，做好 DPO，其实效果已经跟 RL 非常接近了，提供的增益也不大。**

**而 RL 做起来很难，非常耗工程，对于算力消耗也是非常明确的，所以在这种对于收益不明确的地方，国内大家的投入还是相对比较保守和谨慎。**

保守和谨慎就导致资源的投入，没有足够多的卡，没有足够多的实验，没有足够多的算力让大家去浪费，那这方面的人才就没法积累经验。因为有些东西不写在 paper 里，而是你在训练时一次次训崩的 knowhow。

中国在顶尖科研上的确存在系统性问题------别人探出路来了之后，我们会很有信心去投入资源去趟，但当初不明确的时候，我们不愿意投入。


**10**


**如果 Scaling Law 玩不起，**

**「o1」又是谁要下注的比赛**


**张鹏：** 「**o1」其实让大家看到一个明确的方向，这是不是意味着大家要在这个方向上更深入地探索？**

**颜水成：**我个人稍微悲观一点，主要原因是有一些细节不像 Sora 出来的时候，从它的技术文档上你就能看得很清楚，它的路线是什么东西。

第二，还是类比 Sora，当时 Transformer，以及后面的 DIT（一种文生视频架构）、扩散模型，是在开源的生态上往前走，创业公司只要去思考数据和工程的问题就可以。

但是这次强化学习，客观上来说，我觉得中国公司里，在大规模场景下，自己有代码库（code base）跑通的就很少，而且没有足够开源社区的支持。吸引人才其实也没有真正有一手经验的人。

所以这两个因素，一是没有大规模 RL 场景和好的 code base 做支撑，二是很多 know-how 的细节不清楚，可能会让追赶的速度比较慢，会比我们追上 GPT-4 所需要的时间更长一些，我觉得哪怕是在美国，优秀的公司要追上的话，也可能是以年为单位。

![](https://cubox.pro/c/filters:no_upscale()?imageUrl=https%3A%2F%2Fmmbiz.qpic.cn%2Fmmbiz_jpg%2F8cu01Kavc5bp2HcwLOGCvEQQWaSZiaK4cPQywo2VbNQ35RCws3e8SuplOTb9s7icLkbcqBMllcel1iaodDwoFHzyQ%2F640%3Fwx_fmt%3Djpeg%26from%3Dappmsg)图片来源：视觉中国

**张鹏：你正好提醒我，从 ChatGPT 出来，到国内出现类似 ChatGPT 的应用，大概经历了四五个月的时间，追 GPT-4 可能大概是半年左右，Sora 可能也是经过了大概半年，大概的追赶周期是半年。但追上 o1 这样的能力，可能是要以年为计的难度。汪华怎么看？**

**汪华：**我倒没有那么悲观。

**之前那种往上 scaling（扩展）模型尺寸的方法，国内真的追不起，哪怕几家拿到很多投资的创业公司，追到一定程度也就追不动了。哪怕是大厂，我觉得追到一定程度也不见得往下追了，因为国内的目前经济和资本环境也没有那么好。**

而且实际上，GPT-4o 其实不好追。**虽然 GPT-4o 的模型尺寸比 GPT-4（1.8 万参数）要小很多，但多模态的数据和训练是非常消耗资源的，很吃算力。所以训练 GPT-4o 只会比 GPT 4 更贵。**我倒不觉得国内是因为工程原因和学术原因做不出 GPT-4o。

那「o1」会有什么样的一个特点呢？就是它其实「吃」（大量消耗）很多的研究，也「吃」很多的实验，也「吃」很多的探索和 idea 的东西，包括数据的一些构造的技巧等，但**「o1」其实不太吃算力。并且它可是可以通过比较小的模型，去实验和模拟的。**

我个人觉得，**中国公司玩得起，而且 o1 出来了之后，开源社区也玩得起**，开源社区不太玩得起 GPT-4o。我觉得，不光是中国公司，开源社区和学术界也会试图在小尺寸的模型上，用各种各样的方法去实现类似的效果，包括一些开源框架。所以**中国公司也并不是只是孤单地说我对抗全世界，相当于是中国公司和开源社区一起追赶 OpenAI 的这件事。**

**张鹏：听起来中国的大模型的创业公司真的辛苦，过去一段时间已经连续铺开好几条阵线，很多东西还在 pipeline 里打磨中，但现在「o1」出来之后要去再去做，资源可能会如何分配？**

**汪华：**客观地讲，**不会所有的公司都去追的。有些大模公司会坚持方向，有些大模型公司会转型成产品公司，有些大模型公司可能会选择某个方向做突破口，但首先大厂应该都会去追。**

**张鹏：也许像 DeepSeek，这种比较「神」的公司也有可能。**

**汪华：**大厂都会去追。创业模型公司里有一部分会去追。

而且，大家在实验的角度应该都会追，因为你要说做出一个特别大的产品模型，那不会做，但在相当于 mini 级别的尺寸里面去夯实强化学习能力，去做这方面的实验，是必然要做的事，只是不一定大家都能做到生产级别。

**颜水成：** 其实有一点，比如说像 GPT 3.5 的时候，PPO 就基本上有很大的收益。其实开源社区也在想办法去复现一些东西，但是并没有谁开源出一个真正意义的 code base（代码库）能被中国公司直接使用。所以我觉得在 RL 上，门槛还是会比想象的要大一点。

**汪华：**我觉得有两方面原因。

第一，我个人觉得 PPO 的确是工程门槛要高很多。跑 PPO，同时多个模型跑，对算力的要求也消耗也很大，学术界也跑不太动。

还有一部分原因，开源社区当时很大的精力都放在「青春平替版」，发明了 DPO 等一系列东西去做开源平替。开源平替基本上 online PPO，Offline PPO 的确也做到了基本上 90% 的效果。

**颜水成：**这里面奖励函数非常关键，当年 GPT 3.5 的时候，其实他们是拿 GPT-4 的模型去训练奖励模型，才能保证 PPO 做得比较好。所以如果说模型在强化学习这里，如果要用奖励模型，这个东西本身也是一个瓶颈，不是小模型出来的东西就可以用。

**汪华：**是的，但开源社区并不仅只有创业公司。

**张鹏：有 Meta、阿里，看起来开源领域还是有一些巨头的，如果他们有坚定的目标要给群众发枪，我觉得也 make sense。**

**汪华：**可能的确不会那么快，但是技术扩散是必然的。随着更多的公司，包括国内的大厂，海外的开源社区，学术界去花更多的精力去做 RL（强化学习），其实过去几年在大模型之前，RL 已经非常冷了，无论是 PhD 还是工业界、学术界，其实选择做 RL 方向的人已经很少了，这也是一部分的原因。

如果大家突然觉得这个事靠谱的话，很快大家都用算力，各方面人才就会逐渐地多起来。包括技术扩散也会慢慢地发生，但的确这个门槛要高得多。

**颜水成：** 我原先做 RL 研究的时候，当时一个最大的问题，就是最前沿的算法，code base 都是基于 DeepMind 的 TPU 代码，基于 GPU 的实现当时比较缺乏。现在稍微好一些，很多东西基于 GPU 的东西已经多起来了。

**汪华：****我甚至觉得 Nvidia、微软都会试图去做这件事，然后甚至是开源他们的框架运营或者投认去做这方面的框架，因为对他们来说最重要的是把算力卖出去。**


**11**


****「o1」之后，创业者的选择****


**张鹏：现在创业者经常在谈的一个话题是：技术发展太快，怎么能够随着技术水涨船高，而不是被水漫金山？换句话说，技术涨对我有利，而不要技术一涨我就变得没价值。「o1」出来之后，你会怎么回答这个问题？**

**颜水成：**昆仑万维做大模型的方式还是以产品先行，目前大概有五六个产品，比如说像 AI 搜索、AI 音乐、AI 陪伴、AI 短剧创作和 AI 游戏，有这些具体的产品在前面做牵引，带动我们做基础模型的研发。有一些模型是通用的，有一些模型其实是垂域的，比如说音乐大模型就是垂域的。

我个人觉得，这一波「o1」出来之后，通用模型在原有的模型基础上，增加 RL + CoT 的方式，应该能把性能提升得更好，这肯定要做。

另外，可能更聚焦一些场景，比如精度优先会变得非常重要。举个例子，我们有一个产品是做 AI for research，在天工 APP 里面。有了 CoT 技术，它就能够帮助研究者去思考，在他的研究方向上指明哪一些课题可以探索，而是不是像以前只是给论文做总结、修改语法错误。

最近有人做了一个工作叫「AI scientist」，有了「o1」的这种范式，这种功能就有可能提炼出来了。因为以前是直接一次性生成的，结果是否具有创新性和可行性，不知道，它不能够做任何的分析，现在，在「o1」范式下，有可能把这件事情能做得更好。

![](https://cubox.pro/c/filters:no_upscale()?imageUrl=https%3A%2F%2Fmmbiz.qpic.cn%2Fmmbiz_jpg%2F8cu01Kavc5bp2HcwLOGCvEQQWaSZiaK4cicPUu6lwUTZcGpFCVovgUaUVWrFNbF5yODAmH5mzT7qUmgx36tPJptA%2F640%3Fwx_fmt%3Djpeg%26from%3Dappmsg)图片来源：视觉中国

**汪华：**因为 o1 也刚刚出来，我也还在测试它的能力。对做模型的同学来说，看到「o1」心里可能会打鼓，但对做应用的同学来说，出了「o1」之后，只可能有增益，不可能有损失，因为又多了一个东西可以用。

另外，我觉得不被「水漫金山」的话，核心还是一点：**我们是做 AI 应用的创业者，应用本身、场景本身是主语，AI 只是定语，「水漫金山」往往是把 AI 当主语，没有花很多时间深耕场景、需求和自身的禀赋、优势。**

做应用还是场景优先，同时随时观测技术进展，可能没法像 Google 一样自己去开发技术栈，但实际上很多的事情是有迹可循的。哪怕是「o1」的发布，其实之前在 DeepMind 的研究里这条路径是有迹可循的。

在跟技术前沿保持沟通的同时，做重大的工程决策要特别谨慎，因为这跟当年做移动互联网应用不一样，移动互联网时代做应用时，所有的技术栈都是成熟的，关键是能不能找到一个场景的问题。只要找到场景，拉一个产品经理、拉一个工程师，几个大学生也能做出一个爆款应用出来。

坚持快速 PMF 的原则，尽量使用市场上现有的模型来快速完成 PMF，而不是用复杂的工程。如果一个简单的模型要加复杂工程才能做 PMF 的话，那还不如一开始用最贵的、最好的模型去做 PMF，因为 PMF 消耗不了多少 token。在一开始阶段能用 prompt 解决的，就不要用 SFT，能用 SFT 解决的就不要用后训练。不到万不得已，千万不要用针对模型的缺陷去做一个非常复杂的工程 pipeline（流程管道）的补丁去弥补当前的模型缺陷。

**张鹏：要穿就穿新衣服，尽量少打补丁，如果真要打补丁，也不要打** **复杂的** **补丁，** **要** **打简洁的补丁，这可能在早期阶段就变得很重要** **。** **而且很重要一点，其实是你要解决的问题** **才是** **你的竞争力** **。只是** **围着技术非要找个场景落地，可能就本末倒置** **，思路一定要回到要解决的场景和问题上，这样「水涨船高」的可能性就更大。**


\*头图来源：视觉中国

本文为极客公园原创文章，转载请联系极客君微信 geekparkGO

**极客一问**

**o1 系列模型符合你的预期吗** **？**

![](https://cubox.pro/c/filters:no_upscale()?imageUrl=https%3A%2F%2Fmmbiz.qpic.cn%2Fmmbiz_png%2F8cu01Kavc5YTxYGib55rtMHhP1YJ44FLtVGp8Keyg6D2X3AUhgNicT1ibKKh0fE1eiaGqkSXnTlW0ib96ib3HDAIrnVA%2F640%3Fwx_fmt%3Dpng)

![](https://cubox.pro/c/filters:no_upscale()?imageUrl=https%3A%2F%2Fmmbiz.qpic.cn%2Fmmbiz_png%2F8cu01Kavc5bXib8IGpjbghqj305kKeYibwx0gmZO3iaFibnGncpOnsDNKDciaIH6xNBnpPpk7o5de1RKLzgq70eibBTw%2F640%3Fwx_fmt%3Dpng)


![](https://cubox.pro/c/filters:no_upscale()?imageUrl=https%3A%2F%2Fmmbiz.qpic.cn%2Fmmbiz_png%2F8cu01Kavc5an0KBXb9IbCwiajJefiaywlMX2G9daxebRIz0bpONcZbhCkA7mNIG39fwRUOEzpoBIPvAXIuA82B9Q%2F640%3Fwx_fmt%3Dpng)

**热点视频**


周受资：下一代人要精通数字技术，家长要和孩子一起做决定。

<br />

极客公园   

，赞   
139   


**点赞关注** **极客公园视频号****，**

**观看更多精彩视频**   


**![](https://cubox.pro/c/filters:no_upscale()?imageUrl=https%3A%2F%2Fmmbiz.qpic.cn%2Fmmbiz_png%2F8cu01Kavc5an0KBXb9IbCwiajJefiaywlMX2G9daxebRIz0bpONcZbhCkA7mNIG39fwRUOEzpoBIPvAXIuA82B9Q%2F640%3Fwx_fmt%3Dpng)
**更多阅读** [](http://mp.weixin.qq.com/s?__biz=MTMwNDMwODQ0MQ==&mid=2652990763&idx=1&sn=7e9897ec55186b7e8d529eb83ba34b9a&chksm=7e54109d4923998bc288abc888c969d0a58a2a4146db0f1a121aaafdff6bec95bb763e26c429&scene=21#wechat_redirect)**

[![](https://cubox.pro/c/filters:no_upscale()?imageUrl=https%3A%2F%2Fmmbiz.qpic.cn%2Fmmbiz_png%2F8cu01Kavc5ZDuaJ7snlsfydSwzXCKuACpzHdGh7qb8IzPX16HVUejXwazRS7VeIdribUia9icpYGFf4YgZ8ibhaLiaw%2F640%3Fwx_fmt%3Dpng%26from%3Dappmsg)](http://mp.weixin.qq.com/s?__biz=MTMwNDMwODQ0MQ==&mid=2653055041&idx=1&sn=6edd0e3e8643ba09b5f0e0de456fdb95&chksm=7e5715f749209ce1ffb0150cf884567260ec7670f175b953ec6509a21e456be510492d3a99d1&scene=21#wechat_redirect)

[![](https://cubox.pro/c/filters:no_upscale()?imageUrl=https%3A%2F%2Fmmbiz.qpic.cn%2Fmmbiz_png%2F8cu01Kavc5ZDuaJ7snlsfydSwzXCKuACdohW8SIzJ8Nu4q2icYXsb01CGWGhTBOEvOjWibjqxhwPm5VXXePbIicKA%2F640%3Fwx_fmt%3Dpng%26from%3Dappmsg)](http://mp.weixin.qq.com/s?__biz=MTMwNDMwODQ0MQ==&mid=2653054971&idx=1&sn=19d0ad56b7f2a25306ed310d4039326c&chksm=7e571a4d4920935b013ddd3851939c4f79f21799c3ea3c0369dcb379775125a22e2aea934b98&scene=21#wechat_redirect)

![](https://cubox.pro/c/filters:no_upscale()?imageUrl=https%3A%2F%2Fmmbiz.qpic.cn%2Fmmbiz_gif%2F8cu01Kavc5ZENt3gIiatQKstoLiatpXoWBUwkB6tO2b9y2Hoj5HpcnXc5zRJEX6MhbyXJ3q0gjTrrBIUF7boJGDA%2F640%3Fwx_fmt%3Dgif)

![](https://cubox.pro/c/filters:no_upscale()?imageUrl=https%3A%2F%2Fmmbiz.qpic.cn%2Fmmbiz_gif%2F8cu01Kavc5YR1a8dIHV2UrCdNIhialnevdQkialrf9oMibXZhuHeD0nPUHuFlYzYB4WYzwnTbhSyAvj9ibZb7ibewPw%2F640%3Fwx_fmt%3Dgif)

[跳转到 Cubox 查看](https://cubox.pro/my/card?id=7237779018307603850)
